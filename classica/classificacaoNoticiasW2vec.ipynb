{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classificacaoNoticiasW2vec",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HedersonSantos/Noticias/blob/main/classica/classificacaoNoticiasW2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6ntCB_ZoYIR"
      },
      "source": [
        "#https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 (medio)\n",
        "#https://www.kdnuggets.com/2018/11/multi-class-text-classification-model-comparison-selection.html/2 (fraco)\n",
        "#https://realpython.com/python-keras-text-classification/ (fraco)\n",
        "#https://sabber.medium.com/classifying-yelp-review-comments-using-lstm-and-word-embeddings-part-1-eb2275e4066b (fraco)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from multiprocessing import Pool, Process\n",
        "import multiprocessing as mp\n",
        "from google.colab import files, drive\n",
        "from pathlib import Path\n",
        "import io, os\n",
        "import pickle\n",
        "import pyarrow.parquet as pq\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "\n",
        "\n",
        "import nltk, re\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import gensim\n",
        "import gensim.downloader as gensim_api\n",
        "\n",
        "\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import cross_val_score, KFold, RandomizedSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "from sklearn import feature_selection\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, \\\n",
        "                            recall_score, confusion_matrix, \\\n",
        "                            plot_confusion_matrix, classification_report, \\\n",
        "                            balanced_accuracy_score, cohen_kappa_score, matthews_corrcoef, \\\n",
        "                            auc, roc_curve, precision_recall_curve\n",
        "\n",
        "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATJaqnNUqr_L"
      },
      "source": [
        "# Obtendo dataset de noticias tratadas em preProcessamento_noticias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnmIRw0WowrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f531c3da-1346-412d-e02c-bcaade9d2e8c"
      },
      "source": [
        "#!rm news.*\n",
        "#!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/news.zip\n",
        "#!unzip news.zip\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/Colab\\ Notebooks/\n",
        "!ls -lh \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/Colab Notebooks\n",
            "total 974K\n",
            "drwx------ 3 root root 4.0K Aug 31 12:22  amostra_news_integrada\n",
            "-rw------- 1 root root  57K Jul 13 19:50  artigosWikibr.ipynb\n",
            "drwx------ 2 root root 4.0K Aug 30 22:59  bertimbau\n",
            "-rw------- 1 root root  37K Aug  1 23:19 'BERTimbau_com_amostras (1).ipynb'\n",
            "-rw------- 1 root root 158K Sep  2 16:41  BERTimbau_com_amostras.ipynb\n",
            "drwx------ 2 root root 4.0K Aug  7 13:00  bertimbau_resp\n",
            "drwx------ 2 root root 4.0K Sep  8 22:28  classica_resp\n",
            "-rw------- 1 root root 125K Sep  6 10:58  classificacaoNoticiasW2vec\n",
            "-rw------- 1 root root 170K Aug  7 17:47 'Cópia de BERTimbau_Testes (1).ipynb'\n",
            "-rw------- 1 root root  98K Aug  7 15:29 'Cópia de BERTimbau_Testes (2).ipynb'\n",
            "-rw------- 1 root root  72K Aug  1 12:19 'Cópia de BERTimbau_Testes (3).ipynb'\n",
            "-rw------- 1 root root  46K Aug  1 11:45 'Cópia de BERTimbau_Testes (4).ipynb'\n",
            "-rw------- 1 root root 168K Aug 23 20:02 'Cópia de BERTimbau_Testes.ipynb'\n",
            "drwx------ 2 root root 4.0K Aug 16 22:40  figuras\n",
            "-rw------- 1 root root 9.7K Aug 16 23:41  ResultadosExperimentos.ipynb\n",
            "-rw------- 1 root root  306 Jul 12 18:08  Untitled\n",
            "-rw------- 1 root root 9.5K Sep  2 18:02  Untitled0.ipynb\n",
            "-rw------- 1 root root 1.2K Jul 12 17:26  Untitled1.ipynb\n",
            "-rw------- 1 root root 2.3K Sep  3 11:25  Untitled2.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsW44sgpaDj-",
        "outputId": "ebca1d16-95f1-4b4c-d032-a2b001834e5c"
      },
      "source": [
        "%cd /gdrive/My\\ Drive/Colab\\ Notebooks/\n",
        "!ls ./amostra_news_integrada/amostra_5\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/My Drive/Colab Notebooks\n",
            "test.csv  train.csv  valid.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CQJzsnkYJz_",
        "outputId": "55cc7f85-0855-4458-c31c-c7638c24bcaf"
      },
      "source": [
        "mp.cpu_count()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWX1z5Ntq-Ec"
      },
      "source": [
        "# Funções para processamento de Linguagem Natural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmvLl_wTrFR9"
      },
      "source": [
        "def download_pt_stopWords():\n",
        "\n",
        "  '''download das stopwords '''\n",
        "  nltk.download('stopwords') #stopwords\n",
        "  nltk.download('rslp')  #stemming\n",
        "        \n",
        "def removeStopWords( texto, excluirWords:list=None):\n",
        "  '''remove as stopwords do texto. Novas stopwords podem ser adicionadas através da lista excluirWords'''\n",
        "  naoQueridas = nltk.corpus.stopwords.words('portuguese')\n",
        "  if not excluirWords==None:\n",
        "    naoQueridas.extend(excluirWords)\n",
        "  naoQueridas = list(set(naoQueridas))\n",
        "  palavras = [i for i in texto.split() if not i.lower() in naoQueridas]\n",
        "  return (\" \".join(palavras))\n",
        "\n",
        "def aplicaStemming( texto):\n",
        "  ''' obtém o radical das palavras do vocabulário'''\n",
        "  stemmer = nltk.stem.RSLPStemmer()\n",
        "  palavras = []\n",
        "  for w in texto.split():\n",
        "      palavras.append(stemmer.stem(w))\n",
        "  return (\" \".join(palavras))\n",
        "\n",
        "def removeCaracteresNaoDesejados(texto):\n",
        "  textoLimpo = re.sub(r\"http\\S+\", \"\", texto)\n",
        "  textoLimpo = re.sub(r\"www\\..+\\..+\", \"\", texto)\n",
        "  textoLimpo = re.sub(r\"[^a-zA-ZáÁéÉíÍóÓúÚãÃàÀôâÂêÊôÔçÇ ]\", \"\", texto)\n",
        "  return textoLimpo\n",
        "\n",
        "def retornaVetorizacao(X,pct_min=1, pct_max=1, n_grams=(1,1), excluirSW:list=None, vetorPalavras=None, n_top=None):\n",
        "  ''' monta a matriz sparsa com o índice de vocabulário em cada texto. \n",
        "    Retorna a matriz sparsa e o vocabulário '''\n",
        "  if vetorPalavras==None:\n",
        "    count_vect = CountVectorizer(min_df=pct_min, max_df=pct_max, lowercase=True,stop_words=excluirSW, ngram_range=n_grams, max_features=n_top) \n",
        "  else:\n",
        "    count_vect = CountVectorizer(min_df=pct_min, max_df=pct_max, lowercase=True,stop_words=excluirSW, ngram_range=n_grams, vocabulary=vetorPalavras, max_features=n_top)\n",
        "  matriz_sparsa = count_vect.fit_transform(X)\n",
        "  vocabulario = count_vect.fit(X).vocabulary_\n",
        "  return [matriz_sparsa,vocabulario]\n",
        "\n",
        "def retornaMatriztfIdf(V):\n",
        "    ''' em cada documento, calcula o tf-idf de cada palavra\n",
        "        term frequency - inverse document frequency'''\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "    matriz_tfidf = tfidf_transformer.fit_transform(V)\n",
        "    return matriz_tfidf\n",
        "\n",
        "def reduzDimensionalidadecomPCA( X, nro_dimensao,UT=None):\n",
        "    data_pca= PCA(nro_dimensao)\n",
        "    if UT==None:\n",
        "        UT = data_pca.fit(X)\n",
        "    X_pca =  UT.fit_transform(X)\n",
        "      \n",
        "    return [UT,X_pca]\n",
        "\n",
        "def padronizaValores(X):\n",
        "  X_norm = StandardScaler(with_mean=False).fit_transform(X)\n",
        "  return X_norm\n",
        "\n",
        "def retornaPalavras(listaTexto:list):\n",
        "  words=\"\"\n",
        "  for i in listaTexto: \n",
        "    i = str(i) \n",
        "    separate = i.split() \n",
        "    for j in range(len(separate)): \n",
        "        separate[j] = separate[j].lower() \n",
        "  words += \" \".join(separate)+\" \"\n",
        "  return words\n",
        "def montaWordCloud(words, n_palavras:int, sw=None):\n",
        "  wc = WordCloud(width = 400, height = 200, \n",
        "                background_color ='black', max_words=n_palavras,\n",
        "                min_font_size = 10, stopwords = sw).generate(words)\n",
        "  return wc\n",
        "\n",
        "def imprimiWordCloud(listaTexto:list, n_palavras:int, cluster:str=None, sw=None):\n",
        "  words = retornaPalavras(listaTexto)\n",
        "  wc = montaWordCloud(words, n_palavras, sw)\n",
        "  plt.figure(figsize = (8, 8), facecolor = None) \n",
        "  plt.imshow(wc) \n",
        "  plt.axis(\"off\") \n",
        "  plt.tight_layout(pad = 0) \n",
        "  if not cluster == None:\n",
        "    plt.title('PALAVRAS PARA O CLUSTER ' + cluster)\n",
        "  plt.show()\n",
        "\n",
        "def preProcessaTextos(dfDados):\n",
        "  download_pt_stopWords()\n",
        "  dfDados['text'] = dfDados['text'].astype('unicode')\n",
        "  dfDados['text'] = dfDados['text'].map(removeCaracteresNaoDesejados)\n",
        "  dfDados['text'] = dfDados['text'].map(removeStopWords)\n",
        "  dfDados['text'] = dfDados['text'].map(aplicaStemming)\n",
        "  return dfDados\n",
        "  \n",
        "\n",
        "def PreProcessamentoParalelo(df, n_jobs):\n",
        "  df_split = np.array_split(df,n_jobs)\n",
        "  pool = Pool(n_jobs)\n",
        "  resultado = pool.map(preProcessaTextos, df_split)\n",
        "  df = pd.concat(resultado, sort=False)\n",
        "  return df\n",
        "\n",
        "def processaVetorizacao(dfDados,min_fr=0.01, max_fr=0.7, ngrams=(1,1),n_top=None, localDestinoVocabulario=None, vocabulario=None):\n",
        "  arquivoVetores = 'count_vector.pkl'\n",
        "  if vocabulario != None:\n",
        "    vetorPalavras=vocabulario\n",
        "  elif localDestinoVocabulario==None or Path(localDestinoVocabulario+\"/\"+arquivoVetores).is_file()==False:\n",
        "    vetorPalavras=None\n",
        "  else:\n",
        "    vetorPalavras = carregaEstrutura(localDestinoVocabulario, arquivoVetores)\n",
        "    if not vetorPalavras:\n",
        "      vetorPalavras=None\n",
        "  X = dfDados['text'].values\n",
        "  vetores = retornaVetorizacao(X = X,pct_min = min_fr,pct_max = max_fr,n_grams = ngrams,vetorPalavras = vetorPalavras,n_top = n_top)\n",
        "  if localDestinoVocabulario!=None:\n",
        "    salvaEstrutura(vetores[1],localDestinoVocabulario,arquivoVetores)\n",
        "  V = vetores[0]\n",
        "  return vetores\n",
        "\n",
        "def calcula_especificidade(matriz_confusao):\n",
        "  FP = matriz_confusao.sum(axis=0) - np.diag(matriz_confusao) \n",
        "  FN = matriz_confusao.sum(axis=1) - np.diag(matriz_confusao)\n",
        "  VP = np.diag(matriz_confusao)\n",
        "  VN = matriz_confusao.sum() - (FP + FN + VP)\n",
        "  FP = FP.astype(float)\n",
        "  FN = FN.astype(float)\n",
        "  VP = VP.astype(float)\n",
        "  VN = VN.astype(float)\n",
        "  TVN = np.sum(VN)/(np.sum(VN) + np.sum(FP))\n",
        "  return TVN\n",
        "\n",
        "def calcula_especificidade_porClasse(matriz_confusao):\n",
        "  FP = matriz_confusao.sum(axis=0) - np.diag(matriz_confusao) \n",
        "  FN = matriz_confusao.sum(axis=1) - np.diag(matriz_confusao)\n",
        "  VP = np.diag(matriz_confusao)\n",
        "  VN = matriz_confusao.sum() - (FP + FN + VP)\n",
        "  FP = FP.astype(float)\n",
        "  FN = FN.astype(float)\n",
        "  VP = VP.astype(float)\n",
        "  VN = VN.astype(float)\n",
        "  TVN = (VN)/(VN + FP)\n",
        "  return TVN\n",
        "\n",
        "def calcula_GMean_multiclass(revocacao):\n",
        "   revoc = np.array(revocacao)\n",
        "   GMean = revoc.prod()**(1.0/len(revoc))\n",
        "   return GMean \n",
        "\n",
        "def elabora_relatorio_metricas(report, matriz_confusao):\n",
        "  espec = calcula_especificidade_porClasse(matriz_confusao) # calcula o valor da especificidade para cada classe\n",
        "  dfrep = pd.DataFrame(report).transpose() #transforma o conteúdo do classification_report em um dataframe pandas\n",
        "  dfrep_a = dfrep[:-3].copy() # separa as métricas de cada classe do valor da acurácia geral do modelo\n",
        "  dfrep_a['specificity'] = espec # inclui no dataframe o valor da especificidade\n",
        "  dfrep_b  = dfrep[dfrep.index=='accuracy'].copy() #obtem do dataframe somente o valor da acurácia\n",
        "  dfrep_b['specificity']=calcula_especificidade(matriz_confusao) # inclui o valor da especificidade geral de todas as classes\n",
        "  metricas = ['precision', 'recall', 'specificity', 'f1-score', 'support'] #organiza as métricas na ordem desejada\n",
        "  df = pd.concat([dfrep_a[metricas],dfrep_b[metricas]],sort=False) #concatena todos os valores em um único dataframe\n",
        "  df['support'] = df['support'].astype('int')\n",
        "  return df\n",
        "\n",
        "def imprimeMetricas(y_pred, y_true, caminho_destino):\n",
        "  relatorio = \"\"\n",
        "  print('Classification Report:')\n",
        "  n_classe = np.max(y_true)+1 #obtem o número de classes\n",
        "  report = classification_report(y_true, y_pred, labels=np.arange(0,n_classe), digits=4, output_dict=True) #gera o relatório de métricas\n",
        "  cm = confusion_matrix(y_true, y_pred, labels=np.arange(0,n_classe)) #gera a matriz de confusao\n",
        "  report = elabora_relatorio_metricas(report, cm) #inclui no relatorio a especificidade\n",
        "  \n",
        "  print(report)\n",
        "\n",
        "  acuracia_score = accuracy_score(y_true, y_pred)\n",
        "  cohen_kappa = cohen_kappa_score(y_true, y_pred,labels=np.arange(0,n_classe))\n",
        "  ccmatheus = matthews_corrcoef(y_true, y_pred)\n",
        "  Gmean = calcula_GMean_multiclass(report['recall'])\n",
        "  acuracia_balanceada = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "  print(\"Acurácia:\",acuracia_score)\n",
        "  print('Acurácia balanceada:',acuracia_balanceada)\n",
        "  print('GMean:', Gmean)\n",
        "  print('Cohen Kappa Score:', cohen_kappa)\n",
        "  print('Coef. Correlacao Matheus:', ccmatheus)\n",
        "  \n",
        "  y = caminho_destino.split('/')\n",
        "  arquivo = y[-1:][0].split('.')[0]\n",
        "  report.to_csv('/'.join(y[:-1]) + '/' + arquivo + \".csv\", index=None)\n",
        "  outrasMetricas  = \"Acurácia:\" + str(acuracia_score)\n",
        "  outrasMetricas  += '\\n Acurácia balanceada:' + str(acuracia_balanceada)\n",
        "  outrasMetricas  += '\\n GMean:' + str(Gmean)\n",
        "  outrasMetricas  += '\\n Cohen Kappa Score:' + str(cohen_kappa)\n",
        "  outrasMetricas  += '\\n Coef. Correlacao Matheus:' + str(ccmatheus)\n",
        "  \n",
        "  \n",
        "  salvaEstrutura(outrasMetricas, '/'.join(y[:-1]), y[-1:][0])\n",
        "  \n",
        "\n",
        "def imprimeROC_PrecisonRecall_curvas(y_teste,predicted_prob):\n",
        "  classes = np.unique(y_teste)\n",
        "  y_test_array = pd.get_dummies(y_teste, drop_first=False).values\n",
        "  fig, ax = plt.subplots(figsize=(12,8),nrows=1, ncols=2)\n",
        "  ## Plot roc\n",
        "  for i in range(len(classes)):\n",
        "      fpr, tpr, thresholds = roc_curve(y_test_array[:,i],  \n",
        "                            predicted_prob[:,i])\n",
        "      ax[0].plot(fpr, tpr, lw=3, \n",
        "                label='{0} (area={1:0.2f})'.format(classes[i], \n",
        "                                auc(fpr, tpr))\n",
        "                )\n",
        "  ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
        "  ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
        "            xlabel='False Positive Rate', \n",
        "            ylabel=\"True Positive Rate (Recall)\", \n",
        "            title=\"Receiver operating characteristic\")\n",
        "  ax[0].legend(loc=\"lower right\")\n",
        "  ax[0].grid(True)\n",
        "\n",
        "  ## Plot precision-recall curve\n",
        "  for i in range(len(classes)):\n",
        "      precision, recall, thresholds = precision_recall_curve(\n",
        "                  y_test_array[:,i], predicted_prob[:,i])\n",
        "      ax[1].plot(recall, precision, lw=3, \n",
        "                label='{0} (area={1:0.2f})'.format(classes[i], \n",
        "                                    auc(recall, precision))\n",
        "                )\n",
        "  ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
        "            ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
        "  ax[1].legend(loc=\"best\")\n",
        "  ax[1].grid(True)\n",
        "  plt.show()\n",
        "\n",
        "def salvaEstrutura(estrutura, local, arquivo):\n",
        "  print('***SALVANDO***')\n",
        "  print(local + \"/\" + arquivo)\n",
        "  pickle.dump(estrutura,open(local + \"/\" + arquivo,'wb'))\n",
        "\n",
        "def carregaEstrutura(local, arquivo):\n",
        "  estrutura = pickle.load(open(local + \"/\" + arquivo,'rb'))\n",
        "  return estrutura\n",
        "\n",
        "def obtemListasn_grams(corpus):\n",
        "  ''' transformar o texto das noticias em uma lista de n-grams. Usaremos uni-gramas, bi-gramas e tri-gramas '''\n",
        "  ## cria uma lista de uni-grams\n",
        "  lst_corpus = []\n",
        "  for string in corpus:\n",
        "    lst_words = string.split() #cria uma lista com as palavras da noticia\n",
        "    lst_grams = [\" \".join(lst_words[i:i+1]) \n",
        "                for i in range(0, len(lst_words), 1)] #gera uma lista de combinação palavra a palavra (1-ngram)\n",
        "    lst_corpus.append(lst_grams)\n",
        "\n",
        "  ## detect bigrams and trigrams\n",
        "  bigrams_detector = gensim.models.phrases.Phrases(lst_corpus, \n",
        "                  delimiter=\" \".encode(), min_count=5, threshold=10)\n",
        "  bigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)\n",
        "  trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n",
        "              delimiter=\" \".encode(), min_count=5, threshold=10)\n",
        "  trigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)\n",
        "  return [lst_corpus, bigrams_detector, trigrams_detector]\n",
        "\n",
        "def tokenizar(lst_corpus, max_seq_length):\n",
        "  #Transformar o corpus pré-processado (lista de n-gramas: lst_corpous) em uma lista de sequência usando tensorflow/keras\n",
        "  #tokenizar o texto - monta um dicionário cujas chaves são as palavras do texto e o value é um identificador (sequencial) da palavra.\n",
        "  tokenizer = kprocessing.text.Tokenizer() \n",
        "  tokenizer.fit_on_texts (lst_corpus) \n",
        "  dic_vocabulary = tokenizer.word_index\n",
        "\n",
        "  #criar sequência - para cada noticia, obtem o id de cada palavra no vocabulário\n",
        "  lst_text2seq = tokenizer.texts_to_sequences (lst_corpus)\n",
        "\n",
        "  #sequencia preenchimento - transforma a lista em um array numpy e limita o tamanho do vetor de cada noticia. \n",
        "  #                          Usaremos limite de 128. Sequencias menores que estas são preenchidas para ficar com 128.\n",
        "  #                          Usaremos tanto o preenchimento (padding) quanto o corte (truncating) após as 128 palavras. \n",
        "  X = kprocessing.sequence.pad_sequences(lst_text2seq, \n",
        "                                                maxlen = max_seq_length, padding = \"post\", truncating = \"post\")\n",
        "  return [X, lst_text2seq, dic_vocabulary]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Sln_Vau32S"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Isto está formatado como código\n",
        "```\n",
        "\n",
        "# Prepara dataset para classificação com algoritmos clássicos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ2xC0jmIIIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "914f8d0c-629e-4ad4-bc80-b004662a215b"
      },
      "source": [
        "#dfDados = pd.read_csv('news_integradas.csv')\n",
        "dfTreino = pd.read_csv('./amostra_news_integrada/amostra_5/train.csv')\n",
        "dfvalidacao = pd.read_csv('./amostra_news_integrada/amostra_5/valid.csv')\n",
        "dfteste = pd.read_csv('./amostra_news_integrada/amostra_5/test.csv')\n",
        "dfTreino = pd.concat([dfvalidacao, dfTreino], sort=False)\n",
        "print(dfTreino.shape, dfteste.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17320, 2) (1925, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "1C8HtgdwILvc",
        "outputId": "87459800-f2aa-4219-f04c-80a3845988ca"
      },
      "source": [
        "df = dfTreino['category_nro'].value_counts()\n",
        "print(dfTreino['category_nro'].unique().size)\n",
        "df.plot.bar(figsize=(14,10))\n",
        "#'politica', 'economia', 'esporte', 'mundo', 'ilustrada', 'midia', 'tecnologia', 'educação', 'saude', 'ciencia'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f943eb9d390>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2StJlfb_vCMx",
        "outputId": "978bbe4b-bbab-4a3e-9af2-e889b408e9aa"
      },
      "source": [
        "%%time\n",
        "#para w2vec está sem stemizacao\n",
        "dfPreProc = PreProcessamentoParalelo(dfTreino,mp.cpu_count())\n",
        "#dfPreProc = preProcessaTextos(dfTreino)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "CPU times: user 1.72 s, sys: 697 ms, total: 2.42 s\n",
            "Wall time: 4min 24s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k3HMohJ3Dv9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "44261dbe-aaf3-47fd-8ebe-e0fea3cc469e"
      },
      "source": [
        "print(dfPreProc.shape)\n",
        "dfPreProc.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17320, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category_nro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jog will smith campeã sup bowl grand final lig futebol americ est unid new orle saint assassin noit dest sáb new orle smith mulh racquel bale após acid trânsit jog resist fer morr local deix trê filh racquel ating doi tir pern encaminh hospit est saúd divulg smith jog saint assin contrat new england patriot cheg jog tim nfl</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>após admit equívoc deix méd institut feder result en exam nacion ensin médi inep institut nacion estud pesquis educac divulg dad escol hav fic public orig acord inep divulg result nov escol oferec ensin médi integr educ profiss dua mai méd en ambas priv lic art ofíci paul colégi viscond port segur val sp campu vitór institut feder espírit sant hav obt lug tod escol públic país prov obje fic pos ranking atual segund inep escol not divulg dia outubr result modific nov cálcul pass inclu alun ensin médi integr educ profiss dess instituiç vej aqu planilh result ferrament busc sit minist educ aind cont dad atual acord past plataform atual nest sextaf</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>unicamp univers estad campin vai econom cerc r milh mê congel salári profes acim tet paul cúpul univers porém continu receb val acim dess limit constitu determin serv públic estad pod ganh govern gerald alckmin psdb receb atual r mil brut exceç procur cuj tet mai r mil segu regr judici unicamp docent ativ aposent salári mai tet pass ter venc limit após decis tribun justiç segundaf conform antecip folh desembarg câm direit públic derrub limin decis provisór proib univers aplic redu cúpul univers porém dev continu receb alckmin porqu nov profes carg import unicamp têm doi númer matrícul salári difer cad segund unicamp tet aplic cad matrícul dess mod apen salári cúpul administr limit rei josé tad jorg exempl junh salári brut r mil outr r mil nov decis soment prim venc congel unicamp justif salári duplic sob argument cúpul cumpr papel docent dirig temp diz aind possibil acumul venc lev consider som remuneraç extrapol tet aind discuss definitiv julg tce tribun cont est financi basic recurs govern estad unicamp pass dificuldad financ orç cerc r bilh compromet folh pag ideal máx edit artefolhapres adunicamp dire adunicamp associ docent afirm ser contr dupl remuner cúpul reun associ seman pass diss rei prórei ganh gratific represent aument venc sobr congel salári docent entidad diz vai recorr decis tribun justiç stj superi tribun justiç stf supr tribun feder prim secret adunicamp paul oliv corret carr acadêm ser amarr carg polít fic sujeit condiç moment hum polít afirm profes mai salári cheg top carr após ano dedic docent univers paul defend assemble legisl cri tet únic funcion paul assim ocorr est argument hav fug cad vez mai profes pesquis qualific batalh cort salári unicamp vir batalh tribun ano pass univers pass limit pag abril cumpr decis tce adunicamp entr justiç obtev limin mes suspend med ocas argument limit pag desrespeit isonom serv poi procur univers inclu tet isonom docent univers estad feder cuj tet salar mai sustent profes têm direit adquir legisl imped reduç salári fever dest ano justiç emit prim decis mérit cas autoriz cort segund univers entend confirm agor segund inst result unicamp seguir aplic decis tce congel tet salar docent divulg aplic tet salar alv polêm outr univers estad paul fim ano pass usp unesp unicamp entend pod entr cômput vantag salar adquir serv ant emend constitu deix regr cl instituiç pass ent cort serv val exced salári govern exclu vantag adquir ant daquel ano supr tribun feder porém decid outubr tod vantag dev ser consider cômput tet salar contrari lógic univers salári númer exat serv unicamp salári mai govern conhec mê pass univers vinh recus mostr dad julh public sit list númer matrícul salári mil serv ativ aposent inici ocorr após determin justiç bas açã mov folh abert dad assim acontec cas usp</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ano grand fabric ócul real virtual coloc aparelh merc ano câm grau vão cheg mão usu profiss opç disposi captur costur imag tod direç espalhams estand corr ce mai feir tecnolog mund abr port nest quartaf nikon exempl anunci nest terçaf keymission tamanh gopr capt imag tod direç resoluç k prim fabric tradic câm apost merc amad imag grau pesso quer adot tecnolog necess profiss fotograf víde ant hav apen opç empr ricoh cheg ócul real virtual oferec experi imers tip conteúd fabric pass apost model tend análog àquel inici prim model gopr coloc fotograf açã ant exig equip car complex mão usári comum hoj youtub facebook oferec possibil public víde grau fabric câm quer agor captur imag dess tip tão simpl quant tir fot tradic gopr apost ecossistem câm pequenin poder resist vem mont ano desd prim model her carrochef empr spheric solutiom conjunt equip softw feit produz conteúd grau suport encaix gopr vár direç difer aplic costur tud imag final ricoh ce câm thet receb boa crít ediç feir outr opç fabric menos conhec procur espaç sol merc cad vez competi pavilh centr exib cham atenç and corr port hast bol verd top câm esfér panon pioneir segment começ ser desenvolv aind aind companh nok ozo câm real virtual nok fly imagin empres pet adderton fã esport radic par ser gopr merc grau jornal viaj convit samsung</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>levant conselh feder medicin cfm neurolog sistem únic saúd su tod país cerc médic afirm cont serviç públic mecan tri identific imediat paci sofr avc acid vascul cerebr segund acess tomograf minut hiderald cabeç câm técn neurolog neurocirurg cfm diz tomograf tri import diferenci avc isquêm interrupç flux sanguíne quadr comum avc hemorrág romp vas sanguíne identific precis aplic trat apropri depend temp realiz atend h após avc quant temp dem men chanc indivídu recuper paci fic sequel diz cerc médic pesquis afirm cont medic trombolí promov desobstruç artér trat imediat paci dess popul médic diz possível faz básic diz cabeç contud segund daniel ciamp neurolog hospit clín profes usp problem vão além cheg rápid hospit dificuldad pres tod mund cois identific tip derram popul geral sab alguém tend avc outr problem diz ciamp sinal avc falt respost fraqu repentin lad corp boc tort dificuldad fal ciamp diz trein clín geral emergenc reconhec sab trat avc import tend vis imposs hav neurolog plant tod lug hospitaisescol falt remédi sei hospit pequen vai ter trombolí médic trein faz trombólis rupt tromb caus avc segund dad organiz mund saúd avc provoc milh mort mund cond princip respons incapacit pesso obes diabet colesterol alt hipertens estr sedentar mulh enxaquec uso pílul anticoncepc pod ser fat risc segund cabeç dad obt pesquis encaminh minist saúd minist afirm enfrent problem infraestrut aument frot ambulânc ampli recurs destin compr medic past trabalh reforç atenç básic nível assist imprescind atend paci sofr avc inclusiv atend car estrut apont conselh feder medicin cfm post saúd diz not</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text  category_nro\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      jog will smith campeã sup bowl grand final lig futebol americ est unid new orle saint assassin noit dest sáb new orle smith mulh racquel bale após acid trânsit jog resist fer morr local deix trê filh racquel ating doi tir pern encaminh hospit est saúd divulg smith jog saint assin contrat new england patriot cheg jog tim nfl             2\n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               após admit equívoc deix méd institut feder result en exam nacion ensin médi inep institut nacion estud pesquis educac divulg dad escol hav fic public orig acord inep divulg result nov escol oferec ensin médi integr educ profiss dua mai méd en ambas priv lic art ofíci paul colégi viscond port segur val sp campu vitór institut feder espírit sant hav obt lug tod escol públic país prov obje fic pos ranking atual segund inep escol not divulg dia outubr result modific nov cálcul pass inclu alun ensin médi integr educ profiss dess instituiç vej aqu planilh result ferrament busc sit minist educ aind cont dad atual acord past plataform atual nest sextaf             7\n",
              "2  unicamp univers estad campin vai econom cerc r milh mê congel salári profes acim tet paul cúpul univers porém continu receb val acim dess limit constitu determin serv públic estad pod ganh govern gerald alckmin psdb receb atual r mil brut exceç procur cuj tet mai r mil segu regr judici unicamp docent ativ aposent salári mai tet pass ter venc limit após decis tribun justiç segundaf conform antecip folh desembarg câm direit públic derrub limin decis provisór proib univers aplic redu cúpul univers porém dev continu receb alckmin porqu nov profes carg import unicamp têm doi númer matrícul salári difer cad segund unicamp tet aplic cad matrícul dess mod apen salári cúpul administr limit rei josé tad jorg exempl junh salári brut r mil outr r mil nov decis soment prim venc congel unicamp justif salári duplic sob argument cúpul cumpr papel docent dirig temp diz aind possibil acumul venc lev consider som remuneraç extrapol tet aind discuss definitiv julg tce tribun cont est financi basic recurs govern estad unicamp pass dificuldad financ orç cerc r bilh compromet folh pag ideal máx edit artefolhapres adunicamp dire adunicamp associ docent afirm ser contr dupl remuner cúpul reun associ seman pass diss rei prórei ganh gratific represent aument venc sobr congel salári docent entidad diz vai recorr decis tribun justiç stj superi tribun justiç stf supr tribun feder prim secret adunicamp paul oliv corret carr acadêm ser amarr carg polít fic sujeit condiç moment hum polít afirm profes mai salári cheg top carr após ano dedic docent univers paul defend assemble legisl cri tet únic funcion paul assim ocorr est argument hav fug cad vez mai profes pesquis qualific batalh cort salári unicamp vir batalh tribun ano pass univers pass limit pag abril cumpr decis tce adunicamp entr justiç obtev limin mes suspend med ocas argument limit pag desrespeit isonom serv poi procur univers inclu tet isonom docent univers estad feder cuj tet salar mai sustent profes têm direit adquir legisl imped reduç salári fever dest ano justiç emit prim decis mérit cas autoriz cort segund univers entend confirm agor segund inst result unicamp seguir aplic decis tce congel tet salar docent divulg aplic tet salar alv polêm outr univers estad paul fim ano pass usp unesp unicamp entend pod entr cômput vantag salar adquir serv ant emend constitu deix regr cl instituiç pass ent cort serv val exced salári govern exclu vantag adquir ant daquel ano supr tribun feder porém decid outubr tod vantag dev ser consider cômput tet salar contrari lógic univers salári númer exat serv unicamp salári mai govern conhec mê pass univers vinh recus mostr dad julh public sit list númer matrícul salári mil serv ativ aposent inici ocorr após determin justiç bas açã mov folh abert dad assim acontec cas usp             7\n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ano grand fabric ócul real virtual coloc aparelh merc ano câm grau vão cheg mão usu profiss opç disposi captur costur imag tod direç espalhams estand corr ce mai feir tecnolog mund abr port nest quartaf nikon exempl anunci nest terçaf keymission tamanh gopr capt imag tod direç resoluç k prim fabric tradic câm apost merc amad imag grau pesso quer adot tecnolog necess profiss fotograf víde ant hav apen opç empr ricoh cheg ócul real virtual oferec experi imers tip conteúd fabric pass apost model tend análog àquel inici prim model gopr coloc fotograf açã ant exig equip car complex mão usári comum hoj youtub facebook oferec possibil public víde grau fabric câm quer agor captur imag dess tip tão simpl quant tir fot tradic gopr apost ecossistem câm pequenin poder resist vem mont ano desd prim model her carrochef empr spheric solutiom conjunt equip softw feit produz conteúd grau suport encaix gopr vár direç difer aplic costur tud imag final ricoh ce câm thet receb boa crít ediç feir outr opç fabric menos conhec procur espaç sol merc cad vez competi pavilh centr exib cham atenç and corr port hast bol verd top câm esfér panon pioneir segment começ ser desenvolv aind aind companh nok ozo câm real virtual nok fly imagin empres pet adderton fã esport radic par ser gopr merc grau jornal viaj convit samsung             6\n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     levant conselh feder medicin cfm neurolog sistem únic saúd su tod país cerc médic afirm cont serviç públic mecan tri identific imediat paci sofr avc acid vascul cerebr segund acess tomograf minut hiderald cabeç câm técn neurolog neurocirurg cfm diz tomograf tri import diferenci avc isquêm interrupç flux sanguíne quadr comum avc hemorrág romp vas sanguíne identific precis aplic trat apropri depend temp realiz atend h após avc quant temp dem men chanc indivídu recuper paci fic sequel diz cerc médic pesquis afirm cont medic trombolí promov desobstruç artér trat imediat paci dess popul médic diz possível faz básic diz cabeç contud segund daniel ciamp neurolog hospit clín profes usp problem vão além cheg rápid hospit dificuldad pres tod mund cois identific tip derram popul geral sab alguém tend avc outr problem diz ciamp sinal avc falt respost fraqu repentin lad corp boc tort dificuldad fal ciamp diz trein clín geral emergenc reconhec sab trat avc import tend vis imposs hav neurolog plant tod lug hospitaisescol falt remédi sei hospit pequen vai ter trombolí médic trein faz trombólis rupt tromb caus avc segund dad organiz mund saúd avc provoc milh mort mund cond princip respons incapacit pesso obes diabet colesterol alt hipertens estr sedentar mulh enxaquec uso pílul anticoncepc pod ser fat risc segund cabeç dad obt pesquis encaminh minist saúd minist afirm enfrent problem infraestrut aument frot ambulânc ampli recurs destin compr medic past trabalh reforç atenç básic nível assist imprescind atend paci sofr avc inclusiv atend car estrut apont conselh feder medicin cfm post saúd diz not             8"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxhXdXAcG4YR",
        "outputId": "0379466f-dbc5-4b10-c7b5-dd3fbeb725ec"
      },
      "source": [
        "dfPreProc_Teste = PreProcessamentoParalelo(dfteste,mp.cpu_count())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUCZM0Jaq9Zc"
      },
      "source": [
        "CRIANDO WORD2VEC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke4TVkvuIGyu"
      },
      "source": [
        "''' transformar o texto das noticias em uma lista de n-grams. Usaremos uni-gramas, bi-gramas e tri-gramas\n",
        "  algoritmo obtido do artigo https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794\n",
        "'''\n",
        "lst_corpus, bigrams_detector, trigrams_detector =  obtemListasn_grams(dfPreProc['text'])\n",
        "lst_corpus_teste, bigrams_teste, trigrams_teste =  obtemListasn_grams(dfPreProc_Teste['text'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_e8GgLC9WsW",
        "outputId": "01220693-20d6-497e-ab5a-ac3a91015005"
      },
      "source": [
        "tamanho = [len(i) for i in lst_corpus]\n",
        "print('menor:', min(tamanho),'media:', np.round(np.mean(tamanho),1), 'mediana:',np.median(tamanho), 'maximo:',max(tamanho) )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "menor: 1 media: 253.1 mediana: 216.0 maximo: 5235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWQavIebD3-9",
        "outputId": "32e22755-5f3e-443e-d034-ff7e366088be"
      },
      "source": [
        "[[i,t] for i,t in enumerate(tamanho)][0:10]\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 59],\n",
              " [1, 113],\n",
              " [2, 465],\n",
              " [3, 229],\n",
              " [4, 259],\n",
              " [5, 412],\n",
              " [6, 460],\n",
              " [7, 107],\n",
              " [8, 25],\n",
              " [9, 123]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKBi2E2pq4bV",
        "outputId": "3384b924-213c-4995-d707-bdb6202bb7bd"
      },
      "source": [
        "print(list(bigrams_detector.phrasegrams.keys())[0:10])\n",
        "print(list(trigrams_detector.phrasegrams.keys())[0:10])\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(b'will', b'smith'), (b'sup', b'bowl'), (b'futebol', b'americ'), (b'est', b'unid'), (b'new', b'orle'), (b'noit', b'dest'), (b'dest', b's\\xc3\\xa1b'), (b'ap\\xc3\\xb3s', b'acid'), (b'acid', b'tr\\xc3\\xa2nsit'), (b'encaminh', b'hospit')]\n",
            "[(b'will', b'smith'), (b'sup', b'bowl'), (b'futebol', b'americ'), (b'est', b'unid'), (b'new', b'orle'), (b'noit', b'dest'), (b'noit', b'dest s\\xc3\\xa1b'), (b'noit dest', b's\\xc3\\xa1b'), (b'ap\\xc3\\xb3s', b'acid'), (b'encaminh', b'hospit')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOUogTkBq4eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "39ea12ce-b5e6-4d9c-a77a-66cd79d14228"
      },
      "source": [
        "## fit w2v\n",
        "'''nlp = gensim.models.word2vec.Word2Vec(sentences=lst_corpus, size=300,   \n",
        "            window=8, min_count=1, sg=1, iter=30)\n",
        "nlp.save('./classica_resp/nlp_w2vec.sav')'''"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"nlp = gensim.models.word2vec.Word2Vec(sentences=lst_corpus, size=300,   \\n            window=8, min_count=1, sg=1, iter=30)\\nnlp.save('./classica_resp/nlp_w2vec.sav')\""
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exgh8IU0q4h7"
      },
      "source": [
        "# obter o vetor da palavra primavera\n",
        "word = 'primavera'\n",
        "nlp.wv.__getitem__(word).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgiehQ0t74hJ"
      },
      "source": [
        "nlp = gensim.models.word2vec.Word2Vec.load('./classica_resp/nlp_w2vec.sav')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeR3sMdf788j"
      },
      "source": [
        "# Rede Neural\n",
        "\n",
        "* Primeiro, transforme o corpus em sequências preenchidas de ids de palavras para obter uma matriz de recursos.\n",
        "* Em seguida, crie uma matriz de incorporação de forma que o vetor da palavra com id N esteja localizado na enésima linha.\n",
        "* Finalmente, construa uma rede neural com uma camada de incorporação que pesa cada palavra nas sequências com o vetor correspondente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmHbNmXwJ5v4"
      },
      "source": [
        "#tokenizar: Transformar o corpus pré-processado (lista de n-gramas: lst_corpous) em uma lista de sequência \n",
        "max_length=128\n",
        "X_train, lst_text2seq, dic_vocabulary = tokenizar(lst_corpus, max_length)\n",
        "X_teste = tokenizar(lst_corpus_teste, max_length)[0]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8CBoNCBGf2T",
        "outputId": "5d3a46c8-32a3-4020-e2be-0459a38c1bab"
      },
      "source": [
        "print(X_train.shape, X_teste.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17320, 128) (1925, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SWKJpSP0nOW"
      },
      "source": [
        "y_train  = dfPreProc['category_nro']\n",
        "y_teste = dfPreProc_Teste['category_nro']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEl_e4o074mK",
        "outputId": "7118304b-9af3-420e-a51a-e7ecf8e3869c"
      },
      "source": [
        "len(dic_vocabulary.keys())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79779"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8akq0vNBEjN",
        "outputId": "4579bd5f-72c2-4093-a408-ca6430fef7a6"
      },
      "source": [
        "lst_text2seq[0][0:15]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[40, 1728, 3806, 1407, 1914, 6456, 54, 122, 297, 584, 178, 13, 181, 1061, 8172]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnmpCfIyCOoK",
        "outputId": "27dad535-4736-4127-963d-ade83aa12c46"
      },
      "source": [
        "print(X_train[0], '\\n \\n', X_train[1])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   40  1728  3806  1407  1914  6456    54   122   297   584   178    13\n",
            "   181  1061  8172  6386  2931   436    67   378  1061  8172  3806   144\n",
            " 32412  3507    42  1288  2436    40   880  1107   509   163    88    83\n",
            "   143 32412   543    48   453  2293  1719   526    13   101   166  3806\n",
            "    40  6386   754   201  1061  6057  5448    45    40   284  6247     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0] \n",
            " \n",
            " [   42  1307  5173    88   215   373    55   139   524   476    93    99\n",
            "   332  2349   373    93    28    50  1571   166    78    51    63    34\n",
            "   148   535    59  2349   166   139    11    51   354    99   332   401\n",
            "    89   446   116    31   215   524  2254   608 11195   740  3608    30\n",
            "   884 13781   369   737   102   803  2089   663   373    55  2176   295\n",
            "    63   703   315    17    51    41    24   103   414    34   758  1308\n",
            "   167     7  2349    51   179   166    14   922   139  2255    11  1717\n",
            "    15   279    87    99   332   401    89   446    94   666   451   337\n",
            "  4942   139  1059   258   286   145    89    21    37    78   167    59\n",
            "   958   882   167    16   539     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss15PqDUGOgI"
      },
      "source": [
        "'''criar uma matriz embedding para usar como peso na rede neural do classificador.\n",
        "   cada palavra no dicionário das noticias recebá o vetor do word2vec. '''\n",
        "\n",
        "## inicia uma matrix zerada de shape tamanho do vocabulário x tamanho do vetor\n",
        "embeddings = np.zeros((len(dic_vocabulary)+1, 300))\n",
        "for word,idx in dic_vocabulary.items():\n",
        "    ## atualiza a linha da matriz com o vetor do word2vec nlp.\n",
        "    try:\n",
        "        embeddings[idx] =  nlp.wv.__getitem__(word)\n",
        "    ## se a palavra não estiver em nlp, o vetor de pesos desta palavra ficará zerado.\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZRers6-G1GM",
        "outputId": "d34cc383-5cd0-40a9-b775-1cf756f58833"
      },
      "source": [
        "embeddings.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(79780, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCNYpbVjGO1X",
        "outputId": "52fd709d-1184-4837-b1e9-15a8927148f3"
      },
      "source": [
        "''' criar a rede neural para o classificador que terá as seguintes camadas:\n",
        "  * embedding - obtem as sequências (noticias tokenizadas em X_train) como entrada e o \n",
        "    vetor de palavras que compõem cada noticia (embeddings) como pesos.\n",
        "  * simple Attention layer - proposto por Bahdanau, Cho e Bengio, 2014 e também em quase todos os modelos apresentados no estudo bibliográfico - \n",
        "    permite entender qual parte do texto é realmente relevante. Captura os pesos de cada instância e ajuda a construir um explicador, sendo que \n",
        "    não é necessário para as previsões, apenas para a explicabilidade.\n",
        "  * Duas camdas de LSTM Bidirecional para modelar a ordem das palaras em uma sequencia em ambas as direções.\n",
        "  * Duas camadas densas finais que irão prever a probabilidade de cada categoria de notícias.\n",
        "'''\n",
        "\n",
        "## code attention layer\n",
        "def attention_layer(inputs, neurons):\n",
        "    x = layers.Permute((2,1))(inputs) #permuta a forma (shape) de entrada usando o padrão (2,1)\n",
        "    x = layers.Dense(neurons, activation=\"softmax\")(x) #executa a função de ativação softmax na matrix de entrada x. Neurons é a qtd de neurônios.\n",
        "                                                       #output = activation(dot(input, kernel) + bias) input=x, kernel=neurons (pesos)\n",
        "    x = layers.Permute((2,1), name=\"attention\")(x) \n",
        "    x = layers.multiply([inputs, x]) \n",
        "    return x\n",
        "\n",
        "## input\n",
        "x_in = layers.Input(shape=(max_length,))\n",
        "print('x_in:', x_in.shape)\n",
        "## embedding\n",
        "x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
        "                     output_dim=embeddings.shape[1], \n",
        "                     weights=[embeddings],\n",
        "                     input_length=max_length, trainable=False)(x_in)\n",
        "print('x embedding', x.shape)\n",
        "## apply attention\n",
        "x = attention_layer(x, neurons=max_length)\n",
        "print('x attention_layer', x.shape)\n",
        "## 2 layers of bidirectional lstm\n",
        "x = layers.Bidirectional(layers.LSTM(units=max_length, dropout=0.2, \n",
        "                         return_sequences=True))(x)\n",
        "print('x LSTM', x.shape)\n",
        "x = layers.Bidirectional(layers.LSTM(units=max_length, dropout=0.2))(x)\n",
        "print('x LSTM', x.shape)\n",
        "## final dense layers\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "print('x Dense', x.shape)\n",
        "y_out = layers.Dense(10, activation='softmax')(x)\n",
        "print('y_out', y_out.shape)\n",
        "## compile\n",
        "model = models.Model(x_in, y_out)\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "   \n",
        "  \n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_in: (None, 128)\n",
            "x embedding (None, 128, 300)\n",
            "x attention_layer (None, 128, 300)\n",
            "x LSTM (None, 128, 256)\n",
            "x LSTM (None, 256)\n",
            "x Dense (None, 64)\n",
            "y_out (None, 10)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 128, 300)     23934000    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "permute (Permute)               (None, 300, 128)     0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 300, 128)     16512       permute[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention (Permute)             (None, 128, 300)     0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 128, 300)     0           embedding[0][0]                  \n",
            "                                                                 attention[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 128, 256)     439296      multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 256)          394240      bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           16448       bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           650         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 24,801,146\n",
            "Trainable params: 867,146\n",
            "Non-trainable params: 23,934,000\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLZIGvt6zw-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "7325dba7-5ad7-4adc-cd1f-56a7191a6f8b"
      },
      "source": [
        "'''treinando o modelo '''\n",
        "## encode y\n",
        "dic_y_mapping = {n:label for n,label in \n",
        "                 enumerate(np.unique(y_train))}\n",
        "inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
        "y_train = np.array([inverse_dic[y] for y in y_train])\n",
        "## train\n",
        "training = model.fit(x=X_train, y=y_train, batch_size=256, \n",
        "                     epochs=10, shuffle=True, verbose=0, \n",
        "                     validation_split=0.1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-ae519b2af2fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m training = model.fit(x=X_train, y=y_train, batch_size=8, \n\u001b[1;32m      9\u001b[0m                      \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                      validation_split=0.1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knu4TZwq24ne"
      },
      "source": [
        "'''loss '''\n",
        "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
        "ax[0].set(title=\"Training\")\n",
        "ax11 = ax[0].twinx()\n",
        "ax[0].plot(training.history['loss'], color='black')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Loss', color='black')\n",
        "for metric in metrics:\n",
        "    ax11.plot(training.history[metric], label=metric)\n",
        "ax11.set_ylabel(\"Score\", color='steelblue')\n",
        "ax11.legend()\n",
        "ax[1].set(title=\"Validation\")\n",
        "ax22 = ax[1].twinx()\n",
        "ax[1].plot(training.history['val_loss'], color='black')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Loss', color='black')\n",
        "for metric in metrics:\n",
        "     ax22.plot(training.history['val_'+metric], label=metric)\n",
        "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7XAy9G102n8"
      },
      "source": [
        "y_pred = model.predict(X_teste)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IBi5NUtfe_Z"
      },
      "source": [
        "y_pred_cat = [np.argmax(i) for i in y_pred]\n",
        "y_pred_prob = [np.max(i) for i in y_pred]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ533RsIGhJx"
      },
      "source": [
        "imprimeMetricas(y_pred_cat, y_teste,'./classica_resp/lstm_v1.scr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCqXir0ngWkc"
      },
      "source": [
        "imprimeROC_PrecisonRecall_curvas(y_teste,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "728qr3F2GhM-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk32mwzfGhQP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55ZyIiT0GhTQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10xz4F2oGhWf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHx4qc9mmcqK"
      },
      "source": [
        "KNN"
      ]
    }
  ]
}