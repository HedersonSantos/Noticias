{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTimbau_Testes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HedersonSantos/Noticias/blob/main/BERT/BERTimbau_com_amostras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKMj5WG6lxGP",
        "outputId": "5146eac4-f896-4f88-e6e4-d5d813cf479d"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.16)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIlZbdsGo_vR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3c3e79-7b00-48de-b70f-2e02a921b599"
      },
      "source": [
        "\n",
        "!pip install torch==1.8.1 torchvision==0.9.1 torchaudio==0.8.\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.8.1\n",
            "  Using cached torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "Collecting torchvision==0.9.1\n",
            "  Using cached torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4 MB)\n",
            "Collecting torchaudio==0.8.\n",
            "  Using cached torchaudio-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1) (7.1.2)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install torch==1.8.1, torchaudio==0.8.0 and torchvision==0.9.1 because these package versions have conflicting dependencies.\u001b[0m\n",
            "\n",
            "The conflict is caused by:\n",
            "    The user requested torch==1.8.1\n",
            "    torchvision 0.9.1 depends on torch==1.8.1\n",
            "    torchaudio 0.8.0 depends on torch==1.8.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky7aWx6DyYU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "449d0a94-724f-4d98-8d95-e622d4d1a316"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep  1 15:27:28 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P0    30W /  70W |   4536MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hts6ewoYvUYn"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from google.colab import files, drive\n",
        "import io, os\n",
        "\n",
        "# Preliminaries\n",
        "\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "# Models\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "from transformers import AutoTokenizer  # Or BertTokenizer\n",
        "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
        "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
        "\n",
        "# Training\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, \\\n",
        "                            recall_score, confusion_matrix, \\\n",
        "                            plot_confusion_matrix, classification_report, \\\n",
        "                            balanced_accuracy_score, cohen_kappa_score, matthews_corrcoef\n",
        "import seaborn as sns"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPOdGbGitPHS"
      },
      "source": [
        "### Carregando tokens e vocabulário do BERTimbau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLcgDJZSpmfD",
        "outputId": "0c75503e-635c-4b2e-cc97-6b0cd685c1d9"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFtrA_dtunW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792286aa-c4a0-40fe-9e5b-58a6853d8d7c"
      },
      "source": [
        "#!rm *.csv\n",
        "#!!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/train.csv\n",
        "#!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/valid.csv\n",
        "#!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/test.csv\n",
        "#!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/miscelanea.csv\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/Colab\\ Notebooks\n",
        "path = '/gdrive/My Drive/Colab Notebooks'\n",
        "KEY='4/1AX4XfWjFfXOpQF_Lhhwz8Sp5DTauJAEXNDQjx66Khyme5ASsCvDLtcqsBX0'"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgHctA-zJTMu"
      },
      "source": [
        "#model.save_pretrained('./amostra_news_integrada/bertimbau_base')\n",
        "#tokenizer.save_pretrained('./amostra_news_integrada/bertimbau_base')\n",
        "# .save_vocabulary('./amostra_news_integrada/bertimbau_base')\n",
        "#tokenizer = AutoModel.from_pretrained('./amostra_news_integrada/bertimbau_base')\n",
        "#model = AutoModel.from_pretrained('./amostra_news_integrada/bertimbau_base')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZOL2o_ltyzz"
      },
      "source": [
        "### Preparando DataSet\n",
        "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFFldLWvV6Mg"
      },
      "source": [
        "#%cd amostra_news_integrada\n",
        "#!unzip amostra_news_integrada-20210815T184936Z-001.zip\n",
        "#!ls -lh\n",
        "#!ls  amostra_news_integrada/amostra_2\n",
        "#!mkdir bertimbau_all_categ\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfj3koMmWsZ9"
      },
      "source": [
        "#!mkdir ./bertimbau_all_categ/amostra_4\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV8T4nMYXqdR",
        "outputId": "51d4655f-e16f-4d00-ad81-977dd9752c34"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sjrTvfBLGCL"
      },
      "source": [
        "source_folder = './amostra_news_integrada/amostra_' #'/content'\n",
        "destination_folder = './bertimbau_resp' #'/content'\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90WJxOxcxgAQ",
        "outputId": "943a77a4-c966-4813-d337-6deab3b6956c"
      },
      "source": [
        "!ls ./amostra_news_integrada/amostra_5\n",
        "!head ./amostra_news_integrada/amostra_5/train.csv\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test.csv  train.csv  valid.csv\n",
            "text,category_nro\n",
            "\"Há pouco mais de um ano, Vinicius Lanza não encontrou reação que não fosse o choro depois da prova que resultou em decepção. Então com 19 anos, isolou-se no vestiário e mergulhou numa desolação profunda enquanto lágrimas corriam o rosto.  Por míseros cinco centésimos, estava fora dos Jogos do Rio, dentro do próprio Estádio Aquático Olímpico, que poucos meses depois sediaria o megaevento esportivo. A exemplo da vida, a natação é repleta de viradas, e a partir desta terça (2) Lanza terá chance de promover a sua.  Trezentos e setenta e sete dias após a frustração na seletiva olímpica, ele chega ao Troféu Maria Lenk, primeiro grande evento da natação nacional no ciclo, com o melhor tempo de entrada nos 100 m borboleta -que tem eliminatória e final nesta terça.    O torneio, no Parque Aquático Maria Lenk, dentro do Parque Olímpico, é classificatório para o Mundial de Budapeste, em julho. Ele vai marcar uma passagem de bastão entre gerações.Thiago Pereira, por exemplo, aposentou-se em abril, enquanto o país vê surgir nomes como o de Lanza e outros.  \"\"Não ter ido para os Jogos foi bom para manter o meu fogo, para brigar por vaga na final ou por medalha em Tóquio\"\", diz o mineiro, hoje com 20 anos e que representa o Minas Tênis Clube.  Medalha de prata no Campeonato Mundial júnior de 2015, em Cingapura, nos 100 m borboleta, Lanza adotou os EUA como base no início de 2016. Estabeleceu-se em Bloomington, no Estado de Indiana, onde passou a nadar pela universidade local -e a estudar veterinária também.  Na instituição, passou a ser orientado por Ray Looze, técnico que integrou a seleção norte-americana na Rio-2016 e que é guru de Lilly King, Cody Miller e Blake Pieroni, todos medalhistas nos Jogos.  Nesta temporada, o nadador brasileiro foi eleito capitão do time universitário. \"\"O Ray me disse 'Vini, também queria que você estivesse na Olimpíada, mas agora é olhar pra frente'\"\", contou o nadador.  A vaga em Budapeste deve corroborar o que o técnico lhe disse. \"\"Perder a Olimpíada por cinco centésimos ainda machuca, mas me fez manter o foco. Minha maior mudança foi de atitude\"\", resumiu. Luiz Altamir, 20, viveu experiência diferente em 2016.  Conseguiu a classificação para os Jogos do Rio, nas provas de 400 m livre e no revezamento 4 x 200 m livre.  Se não passou da primeira fase em ambas, o nadador reconhece que o aprendizado pode ser interessante no rumo à Olimpíada de Tóquio.  \"\"O objetivo agora neste ciclo é tentar ficar entre os melhores do mundo, e para isso preciso treinar bastante.\"\"  Natural de Roraima, ele se transferiu do Flamengo para o Pinheiros nesta temporada.  No clube paulistano, divide raia com Gabriel Santos, 20, que foi aos Jogos do Rio e nadou o revezamento 4 x 100 m livre, e Pedro Cardona, 21, que perdeu a Olimpíada por pouco -ficou em terceiro nos 100 m peito, com boa marca.  De 2016 para 2017, o Pinheiros readquiriu Cesar Cielo, o nadador mais vitorioso da história do país, mas mudou sua filosofia. Dispensou medalhões olímpicos como Bruno Fratus e João de Lucca e apostou na nova safra.  \"\"Percebemos que era hora de mudar mentalidade\"\", afirmou o técnico principal do clube, Alberto Pinto da Silva.  Outros bons valores que estarão no Troféu Maria Lenk são Brandonn Almeida, 20, que nadou os 400 m medley e os 1.500 m livre na Rio-2016 e deve se mudar para os EUA depois da competição.  O fundista Guilherme Costa, que em abril bateu o recorde sul-americano dos 1.500 m livre, também espera estabelecer-se como um dos expoentes da geração.  \"\"Eu estou muito confiante para o Maria Lenk. Se conseguir melhorar meu tempo, vou para o Mundial\"\", disse.  CRISE  Se há uma coisa que abala a confiança das promessas, esta é a situação turbulenta por que passa a CBDA (Confederação Brasileira de Desportos Aquáticos). Acusados de corrupção, quatro de seus dirigentes foram presos.  Em meio a isso, a entidade perdeu patrocínio do Bradesco e viu despencar o aporte feito pelos Correios.  \"\"[O investimento] vai diminuir bastante. Isso vai ser uma dificuldade muito grande para a nova geração, que busca novas competições lá fora\"\", afirmou Luiz Altamir.  O nadador disse que espera que a nova gestão tenha uma \"\"transparência melhor\"\".  \"\"A melhor maneira de ter credibilidade de novo é mudar tudo. Pensamento, estratégia, busca por novos patrocinadores. Renovar é a palavra certa\"\", complementou Altamir.  Para Vinícius Lanza, acrise que se abateu sobre a confederação \"\"é muito triste\"\", e vai afetar sua geração. Sobre os dirigentes presos, afirmou que \"\"é a Justiça quem decide\"\".  -  Os eventos do Troféu Maria Lenk  TERÇA-FEIRA (2) Eliminatórias às 9h, finais às 17h30: 100 m borboleta (Feminino e Masculino) 400 m livre (F e M) 100 m peito (F e M) 4x50 m livre (F e M)  QUARTA-FEIRA (3) Eliminatórias às 9h, finais às 17h30: 100 m costas (F e M) 200 m medley (F e M) 1.500 m livre (F) 800 m livre (M)  QUINTA-FEIRA (4) Eliminatórias às 9h, finais às 17h30: 100 m livre (F e M) 50 m peito (FeM) 50 m costas (F e M) 200 m borboleta (F e M)  SEXTA-FEIRA (5) Eliminatórias às 9h, finais às 19h: 400 m medley (F e M) 200 m livre (F e M) 200 m costas (F e M) 50 m borboleta (F e M) 4 x 100 m livre (F e M)  SÁBADO (6) Eliminatórias às 9h, finais às 19h: 200 m peito (F e M) 50 m livre (F e M) 800 m livre (F) 1.500 m livre (M) 4x100 m medley (F e M)\",2\n",
            "\"A situação financeira de São Paulo, apesar das restrições nas despesas e do ligeiro aumento da arrecadação registrado recentemente, é delicada.  O secretário da Fazenda, Luis Arrobas Martins, diz que a arrecadação do ICM (Imposto sobre Circulação de Mercadorias), que no primeiro semestre deveria ser de NCr$ 970 milhões (R$ 9,36 bilhões), ficou em NCr$ 815 milhões (R$ 7,86 bilhões), ou seja, uma queda de 16%.  Além de se distanciar da previsão orçamentária, o governo do Estado não reduziu, como desejava, os orçamentos das Secretarias da Educação e da Segurança Pública.\",6\n",
            "\"Dezenas de milhares de pessoas participaram nesta quarta-feira (9) na França de um dia de protestos convocados por sindicalistas e estudantes contra a reforma na lei trabalhista no país, impulsionada pelo governo.  O ato é uma resposta à intenção do presidente François Hollande de permitir às empresas mudar a jornada de trabalho de seus funcionários além das 35 horas semanais determinadas por lei sem remuneração adicional.    Pelo novo modelo, os trabalhadores poderão trabalhar até 12 horas diárias ou 48 horas semanais sem seguir os pactos com os sindicatos. Em troca, os funcionários passarão a receber mais folgas extras.  Também fazem parte da reforma medidas para reduzir as regras para demissões, trabalho remoto e noturno. A medida é defendida pelo governo como uma forma de reduzir a taxa de desemprego na França.  As manifestações reuniram trabalhadores, desempregados e estudantes, que responderam ao chamado dos sindicatos e do movimento estudantil e protestaram em mais de 200 cidades francesas.  Em Paris, o número de manifestantes chegou a 80 mil. O protesto desta quarta coincidiu com a greve dos funcionários das ferrovias suburbanas e de longa distância francesas, que provocou atrasos nos trens em todo o país.  CRÍTICAS  A reforma trabalhista irritou mais os movimentos sociais especialmente porque foi sugerida pelo Partido Socialista, o mesmo de François Hollande, que há 15 anos havia conseguido aprovar a jornada de trabalho de 35 horas.  Em 2008, o antecessor de Hollande, o conservador Nicolas Sarkozy, havia defendido o aumento da jornada, mas não levou a proposta adiante devido à pressão dos sindicatos franceses.  A ministra do Trabalho, Miriam el-Khomri, defendeu a lei como um avanço e disse que os sindicatos foram ouvidos. O primeiro-ministro, Manuel Valls, afirmou que a proposta não será retirada, embora o diálogo continue.  A intenção do governo era enviar a medida ao Parlamento nesta quarta, mas a resistência dos movimentos sindicais e dentro do Partido Socialista atrasaram a tramitação.\",3\n",
            "\"O Google decidiu voltar atrás em sua decisão de proibir a pornografia em sua plataforma de blogs Blogger, depois de receber muitas críticas de blogueiros, alguns dos quais donos de páginas há mais de uma década. O conteúdo explícito ainda poderá ser postado por meio da ferramenta, desde que o blog seja marcado como adulto.  Jessica Pelegio, gerente de suporte a produtos sociais da companhia, diz que empresa recebeu muitas mensagens sobre a proibição, especialmente sobre a obrigatoriedade de os blogueiros apagarem as imagens eróticas de modo retroativo, desde o início da página, como forma de não sofrerem penalidades –os sites seriam removidos ou marcados como \"\"privados\"\" (disponíveis apenas para os donos e pessoas com as quais eles compartilham o conteúdo, sem aparecer nas buscas).  Também houve reclamações sobre \"\"o impacto negativo em indivíduos que postam conteúdo sexual explícito para expressar suas identidades\"\", disse Pelegio. Por isso, a companhia, que poderia ver uma saída em massa de usuários para plataformas como o Tumblr, decidiu recuar.  Em 2013, o Google já havia alterado os termos de serviço do Blogger para impedir que páginas que exibiam conteúdo pornográfico gerassem dinheiro para os donos: ficou proibido exibir anúncios nessas páginas. O objetivo era impedir a criação de blogs que eram praticamente repositórios de conteúdo explícito e geravam tráfego para outros sites pornôs.\",6\n",
            "\"Convergente com a discussão acerca do bloqueio do Whatsapp, o texto do relatório final aprovado pela CPI dos Crimes Cibernéticos, nesta quarta-feira (4), tenta blindar o aplicativo de mensagens instantâneas.  De última hora, foi adicionado um termo que veda o bloqueio a aplicativos de mensagens pessoais em um dos dois projetos de lei que alteram o Marco Civil da Internet.  Ao todo, foram encaminhados seis projetos de lei pela CPI que versam sobre a ampliação da definição de invasão de computadores e o aumento do rigor da lei, a destinação de 10% do Fistel (Fundo de Fiscalização das Telecomunicações) para órgãos da Polícia Judiciária, e também sobre as mudanças no Marco Civil.  O relatório foi aprovado por 17 a 6. Agora, os projetos de lei presentes no relatório serão encaminhados às comissões da Câmara.  O projeto de lei que prevê o bloqueio de aplicações era um dos pontos de maior discussão Comissão. Originalmente, ele permitia que, por medida judicial, as operadoras de internet bloqueassem o acesso um aplicativo ou um site utilizado para práticas criminosas.    No texto final, ficou delimitado que as aplicações que podem ser bloqueadas são apenas aquelas hospedadas fora do Brasil, sem representação no país, que fossem dedicadas à prática ilícita e que a pena mínima para o crime que estivesse sendo cometido fosse de dois.  Segundo os relatores do projetos, ele complementa o Marco Civil da Internet, que prevê apenas a retirada do ar das aplicações de internet. No entanto, não especifica como se dá esse processo em sites ou aplicativos sem hospedagem ou representação no país.  O texto aprovado pela CPI cria a possibilidade de que juízes determinem que as operadoras façam o bloqueio.  Na sessão de terça (3), os deputados criticaram o texto e afirmaram que a redação banalizaria o bloqueio do aplicativo. Apenas após essas críticas que o parágrafo blindando o Whatsapp foi adicionado.  O debate esquentou na CPI após o aplicativo ser bloqueado, na segunda-feira (2), por uma medida do juiz Marcel Montalvão, da comarca de Lagarto (SE). Na terça, no entanto, o desembargador Ricardo Múcio Santana de Abreu Lima, do Tribunal de Justiça de Sergipe, revogou o bloqueio.  O texto, como foi aprovado, atinge sites piratas que disponibilizam listas de BitTorrents –arquivos de mídia que permitem a usuários compartilhar vídeos e músicas ilegais– e transmissão de mídia. Além disso, os relatores esperam que isso possa impedir o acesso a sites de contrabando e de tráfico de drogas.  No outro projeto de lei que altera o Marco Civil, a CPI tenta permitir a remoção ou o bloqueio de conteúdo por meio de ordens judiciais. Os casos previstos nesse projeto também se baseia na pena mínima de 2 anos para o crime praticado.  Segundo o deputado Alessandro Molon (Rede-RJ), um dos que votou contra o relatório, as mudanças no texto dos projetos de lei não são suficientes para garantir a liberdade de usuários na rede. A principal crítica do relator do Marco Civil da Internet é que há previsões para a retirada de conteúdos por meio de simples decisões judiciais.  \"\"Isso permite o o chilling effect [efeito de resfriamento]. Retira-se o conteúdo até que o judiciário se decida sobre o assunto e, enquanto o conteúdo não volta, mata-se o assunto\"\", diz Molon.  O sub-relator Sandro Alex (PPS-PR) afirma que Molon faz uma interpretação errada da matéria. O deputado explica que foi retirada do texto a possibilidade de bloqueio ou remoção crimes com pena menor que dois anos, retirando a previsão de crimes contra a honra.  \"\"Retiramos para não acharem que estamos tentando blindar os políticos. Além disso, isso não é a aprovação de uma lei. Mas apenas o encaminhamento de uma discussão para as mais diversas comissões\"\", afirma Alex.\",6\n",
            "\"Veja abaixo lista com mais de 140 cursos em 273 instituições com vagas abertas para o segundo semestre de 2016.  A busca pode ser feita por Estado, pelo nome da faculdade ou do curso –entre bacharelados, licenciaturas e tecnológicos–, ou pelo tipo de seleção.  *  Mais de 270 instituições abrem vagas no 2º semestre; veja cursos\",7\n",
            "\"A revolução digital mudou a alma de uma invenção do século 19: a fotografia. É o que veem executivos que trabalham com imagens em diferentes plataformas da internet.  \"\"A essência da fotografia está sendo alterada. Ela foi criada como um meio de capturar a memória. Agora usamos a fotografia como um meio de comunicação\"\", afirma Bernardo Hernández, principal executivo do Flickr, plataforma onde são compartilhadas mais de 1 milhão de imagens por dia. \"\"Aprender a tirar uma fotografia ficou quase tão importante quanto escrever ou falar.\"\"  A concordar com ele está Dan Rubin, fundador da publicação \"\"Photographic Journal\"\". Ele exemplifica essa mudança apontando um dos aplicativos que mais ganharam popularidade nos últimos tempos: \"\"No Snapchat, a comunicação está toda concentrada em imagem\"\".  Samuli Hanninen, especialista no assunto dentro da Microsoft, enxerga a mesma transição: \"\"As imagens antes eram mais de registro. E começamos a ver que ela se tornou parte da comunicação\"\".  O que impulsionou isso, argumenta Hernández, do Flickr, é que a foto se tornou um meio de gratificação imediata. \"\"É difícil conseguir brilhar no Twitter, mas é fácil tirar uma foto e fazer com que elas se destaque.\"\"  ENTRAVES  Mas há obstáculos pelo caminho da evolução da foto. Um deles é técnico.  Como explica John van Derlofske, cientista da 3M, apesar de toda a evolução, as lentes dos celulares atuais ainda têm limites importantes, e alguns deles guardam relação com a capacidade da bateria, um dos mais aspectos em que a indústria mais tem dificuldade em evoluir.  \"\"Quando as pessoas tiram selfies, têm que fazer várias até conseguir a selfie certa. Esse é um exemplo em que o problema da qualidade não está resolvido\"\", diz Hanninen, da Microsoft.  Outros dos obstáculos diz respeito à privacidade –e não só em relação a outras pessoas e aos governos de cada país, mas também no uso das imagens por empresas de tecnologia.  \"\"Estamos acostumados com computadores entendendo textos. Mas computadores agora podem entender fotos\"\", afirma Hernández, do Flickr. \"\"Se veem algo marrom e verde em certa disposição, podem entender que aquilo é uma arvore.\"\"  Da mesma forma, a tecnologia tem conseguido mudar a percepção das fotos. Uma tecnologia mostrada pela Intel no Mobile World Congress, em Barcelona, faz com que seja possível medir a distância entre objetos dentro de uma imagem.    Nessa evolução, a fotografia leva uma vantagem sobre o vídeo. \"\"As pessoas compartilham muito mais foto do que vídeo. Tem a ver com o esforço e tempo requeridos\"\", diz Sokratis Papafloratos, da Toghetera, aplicativo de imagens.  Para Hernández, do Flickr, a explosão da comunicação de imagens ainda está no começo. Ele afirma que volume de fotos enviadas será multiplicado dezenas de vezes nos próximos anos. \"\"O maior fotógrafo da história ainda está por aparecer.\"\"\",6\n",
            "\"Apesar de os líderes das Farc (Forças Armadas Revolucionárias da Colômbia) e o governo colombiano já terem chegado a um texto final do acordo que poderá encerrar os mais de 50 anos de guerra entre as duas partes, ainda faltam alguns passos para que a paz se torne realidade.  Um será a assinatura formal do tratado, prevista para dia 26. O outro, o plebiscito em que o povo colombiano dirá se aprova ou não o acordo, em 2 de outubro.  Antes disso, porém, entre 17 e 23 de setembro, as Farc realizarão uma conferência interna, em San Vicente del Caguán. O objetivo é fazer com que o comando da guerrilha informe seus soldados sobre os detalhes do acordo.  Além disso, pretendem desenhar as linhas do que será o partido político que surgirá ao deixarem a luta armada.  Também será tratado um tema espinhoso: o que fazer com os dissidentes, guerrilheiros que se negam a aceitar o acordo e já buscam outras guerrilhas, como o ELN (Exército de Libertação Nacional).  A Folha conversou, por e-mail, com Carlos Antonio Lozada, um dos membros do Secretariado das Farc.  *  Folha - O que ocorre numa conferência das Farc?  Carlos Antonio Lozada - Trata-se da instância máxima da guerrilha e tem poder para reajustar planos político-militares, escolher líderes do Estado Maior Central [órgão de direção das Farc] e modificar estatutos, regime disciplinar e normas de comando.  Que integrantes podem participar? Apenas o alto comando ou também o baixo escalão?  À conferência assistem os 31 integrantes do Estado Maior Central, além dos delegados de todas as frentes das Farc, escolhidos em assembleias locais. Esses delegados podem ser comandantes ou guerrilheiros de base.  É possível que saiam pedidos de mudança ao texto do acordo ou se trata de um encontro apenas para explicar à militância o que foi acordado?  O tema central será o acordo de paz. E, nesse sentido, a assinatura dele dependerá da aprovação do Estado Maior. Também trataremos de traçar as linhas da proposta política que apresentaremos ao país nessa nova fase, em que as Farc entrarão na política de forma aberta [o acordo prevê que ex-guerrilheiros recebam 10 postos no Congresso nas próximas duas legislaturas].  Vocês já estão se organizando para o deslocamento até as \"\"zonas de segurança\"\" [onde ficarão os guerrilheiros enquanto se montam os tribunais especiais]?  Todas as estruturas das Farc estão dedicadas ao estudo do documento. Só iremos às \"\"zonas de segurança\"\" depois de o acordo ser aprovado no plebiscito pela maioria dos colombianos [a oposição reivindica a remoção antes da votação, para não haver pressão sobre os eleitores].  Já planejam como vão atuar nas \"\"zonas de segurança\"\"?  Teremos três objetivos básicos: garantir que se verifique o cessar-fogo, organizar o início da entrega das armas e o processo de preparação para a reincorporação política, econômica e social das Farc à vida civil.  Haverá orientação aos guerrilheiros que manifestem vontade de atuar na política?  Mais do que um partido, as Farc aspiramos entrar no cenário da política com uma proposta ampla, que aglutine distintos setores sociais e políticos, interessados nas mudanças de que a sociedade colombiana precisa, para que a paz se faça realidade. Estamos empenhados em estruturar as propostas que vamos apresentar para o país.  Será discutido também o tema dos dissidentes?  Há dois meses, um grupo de comandantes de uma das frentes manifestou desacordo com o que foi acordado em Havana. O Estado Maior tomou a decisão de afastá-los. Ao mesmo tempo, pedimos aos guerrilheiros que integravam seus blocos que refletissem sobre a decisão de seus comandantes. Esse fenômeno ocorreu apenas uma vez. Seguramente na conferência analisaremos essa situação.\",3\n",
            "\"Estar acima do peso diminui cerca de um ano da expectativa de vida de um indivíduo, um preço que sobe para cerca de dez anos nos casos de obesidade severa, de acordo com um estudo amplo publicado nesta quinta (14) na revista médica \"\"The Lancet\"\".  A pesquisa refuta estudos anteriores que concluíram que ter alguns quilos extras não traz riscos para a saúde. Em vez disso, o novo estudo revelou evidências de que o risco de morrer antes de seu aniversário de 70 anos aumenta, \"\"de forma gradual e acentuada\"\", conforme a cintura se expande.  \"\"Esse estudo mostra, definitivamente, que o excesso de peso e a obesidade estão associados a um risco de morte prematura\"\", disse a líder da pesquisa, Emanuele Di Angelantonio, da Universidade de Cambridge, no Reino Unido. O risco de doença cardíaca coronária, acidente vascular cerebral, doenças respiratórias e câncer \"\"aumentaram todos\"\", disse a pesquisadora.  Usando dados de quase 4 milhões de adultos de quatro continentes, o estudo descobriu que as pessoas com excesso de peso perdem em média cerca de um ano das suas expectativas de vida, e as pessoas \"\"moderadamente obesas\"\" perdem cerca de três anos. \"\"Pessoas severamente obesas perdem cerca de dez anos da expectativa de vida\"\", afirmou Di Angelantonio.  Uma equipe internacional de pesquisadores selecionou dados a partir de mais de 10,6 milhões de participantes de 239 grandes estudos realizados entre 1970 e 2015 em 32 países na América do Norte, Europa, Austrália, Nova Zelândia e no leste e no sul da Ásia. O trabalho foi considerado o maior conjunto de dados sobre excesso de peso e mortalidade já reunido.  PESQUISA  Para descartar o impacto de outros riscos de mortalidade, a equipe excluiu fumantes e ex-fumantes, portadores de doenças crônicas e pessoas que morreram nos primeiros cinco anos das pesquisas –e ficaram com uma amostra de 3,9 milhões de adultos.  A equipe dividiu a amostra em categorias de acordo com seu Índice de Massa Corporal (IMC) –peso dividido pelo quadrado da altura– e comparou os números e as causas de morte em cada grupo. De acordo com o padrão da OMS (Organização Mundial de Saúde), um IMC de 18,5 a 24,9 é considerado normal, de 25 a 29,9 excesso de peso, de 30 a 34,9 obesidade moderada, de 35 a 39,9 obesidade severa, e acima de 40 obesidade mórbida.  Os pesquisadores descobriram que o risco de morrer antes dos 70 anos aumentou de 19% em homens com peso normal para 29,5% em homens moderadamente obesos. Entre as mulheres, esse risco aumentou de 11% para 14,6%. \"\"Isso corresponde a um aumento absoluto de 10,5%, para os homens, e 3,6%, para as mulheres\"\", disse um comunicado da revista \"\"The Lancet\"\".  Se todas as pessoas com sobrepeso e obesidade tivessem níveis normais de IMC, isso evitaria uma em cada cinco mortes prematuras na América do Norte, uma em cada seis na Austrália e na Nova Zelândia, uma em cada sete na Europa e uma em cada 20 no leste da Ásia, concluiu o estudo.  Em 2014, de acordo com a OMS, mais de 1,9 milhão de adultos em todo o mundo estavam acima do peso. Desses, mais de 600 milhões eram obesos. O excesso de peso é associado a doenças cardíacas, derrame e a alguns tipos de câncer.\",8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyoK7ac8t4HF"
      },
      "source": [
        "#https://towardsdatascience.com/multi-class-text-classification-with-deep-learning-using-bert-b59ca2f5c613\n",
        "#Model hyper-parameter\n",
        "MAX_SEQ_LEN = 512 #limita os artigos em 128 tokens. BERTimbau base é limitado em 512 tokens por texto.\n",
        "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
        "bs = 2\n",
        "lr = 5e-6\n",
        "\n",
        "\n",
        "\n",
        "# Fields - use_vocab=False  e tokenizer.encode permite que utilizemos os tokens do BERTimbau.\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
        "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
        "fields = [('text', text_field),('label', label_field)]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqRpEtW1OuqC"
      },
      "source": [
        "def tokenizaAmostra(source_folder, fields=fields):\n",
        "  # TabularDataset\n",
        "  train, valid, test = TabularDataset.splits(path=source_folder, train='train.csv', validation='valid.csv',\n",
        "                                            test='test.csv', format='CSV', fields=fields, skip_header=True)\n",
        "  # Iterators\n",
        "\n",
        "  train_iter = BucketIterator(train, batch_size=bs, sort_key=lambda x: len(x.text),\n",
        "                              device=device, train=True, sort=True, sort_within_batch=True)\n",
        "  valid_iter = BucketIterator(valid, batch_size=bs, sort_key=lambda x: len(x.text),\n",
        "                              device=device, train=True, sort=True, sort_within_batch=True)\n",
        "  test_iter = Iterator(test, batch_size=bs, device=device, train=False, shuffle=False, sort=False)\n",
        "  return [train_iter, valid_iter, test_iter]\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8maI4HcbPukL"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcTHSXSluKkt"
      },
      "source": [
        "#print(vars(train[0]))\n",
        "#print(vars(valid[0]))\n",
        "#print(vars(test[0]))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPypCNPuHaRz"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvy3rgrdHj_x"
      },
      "source": [
        "'''É preciso informar o número de labels '''\n",
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self, qtd_categories):\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        options_name = \"bert-base-portuguese-cased\"\n",
        "        self.encoder = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased',num_labels=qtd_categories )\n",
        "\n",
        "    def forward(self, text, label):\n",
        "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
        "\n",
        "        return loss, text_fea\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pYKbHICIus0"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJFwyl7DIoxQ"
      },
      "source": [
        "# Save and Load Functions\n",
        "\n",
        "def save_checkpoint(save_path, model, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(load_path, model):\n",
        "    \n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n",
        "\n",
        "def create_directory(path, directory):\n",
        "  try:\n",
        "    os.makedirs(path + '/' + directory)\n",
        "  except FileExistsError:\n",
        "    # directory already exists\n",
        "    pass"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiueExF5Jb8V"
      },
      "source": [
        "# Training Function\n",
        "'''criterion = nn.BCELoss() é BinaryCrossEntropy é a função de perda para targets binarios. Como o nosso alvo possui\n",
        "muitas classes troque a função de perda para nn.CrossEntropyLoss() '''\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          train_loader ,\n",
        "          valid_loader ,\n",
        "          eval_every ,\n",
        "          file_path ,\n",
        "          criterion = nn.CrossEntropyLoss(), #nn.BCELoss(),\n",
        "          num_epochs = 5,\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "    \n",
        "    \n",
        "    print('1 - inicializando variávies')\n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    # training loop\n",
        "    print('1 - inicializando treinamento')\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for (text, labels), _ in train_loader:\n",
        "            labels = labels.type(torch.LongTensor)           \n",
        "            #print('label size:', labels.size())\n",
        "            #print('label:', labels)\n",
        "            labels = labels.to(device)\n",
        "            text = text.type(torch.LongTensor) \n",
        "            #print('text:', text.size())\n",
        "            text = text.to(device)\n",
        "            #print('treina...')\n",
        "            output = model(text, labels)\n",
        "            #print('fim treino...')\n",
        "            loss, _ = output\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "\n",
        "                    # validation loop\n",
        "                    for (text,labels), _ in valid_loader:\n",
        "                        text = text.type(torch.LongTensor)  \n",
        "                        text = text.to(device)\n",
        "                        labels = labels.type(torch.LongTensor)           \n",
        "                        labels = labels.to(device)\n",
        "                        \n",
        "                        output = model(text, labels)\n",
        "                        loss, _ = output\n",
        "                        \n",
        "                        valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
        "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    \n",
        "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')\n",
        "    "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQcJQ5kaMPa4",
        "outputId": "7b564162-8ae2-40cb-86ae-e9200e96ca01"
      },
      "source": [
        "device"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        },
        "id": "RrkvwE2VL1qY",
        "outputId": "c2e72bbb-cd63-4605-b3aa-a0c959c5c483"
      },
      "source": [
        "%%time\n",
        "qtd_categories=[10]\n",
        "#token_train, token_valid, token_test = [],[],[]\n",
        "id_planilha = [9.4]\n",
        "dataset = [5]\n",
        "for idx, (id_, amostra) in enumerate(zip(id_planilha,dataset)):\n",
        "  create_directory(destination_folder,str(id_))\n",
        "  file_path = destination_folder + \"/\"+str(id_)\n",
        "  print(file_path)\n",
        "  model = BERT(qtd_categories=qtd_categories[idx]).to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  inicio = datetime.now()\n",
        "  print('Treinando amostra:', amostra, 'inicio:', inicio)\n",
        "  tokens = tokenizaAmostra(source_folder+str(amostra))\n",
        "  #token_train.append(tokens[0])\n",
        "  #token_valid.append(tokens[1])\n",
        "  #token_test.append(tokens[2])\n",
        "  train(model=model, \n",
        "      optimizer=optimizer,\n",
        "      num_epochs=4,\n",
        "      train_loader=tokens[0],\n",
        "      valid_loader=tokens[1],\n",
        "      eval_every=len(tokens[0]) // 2,\n",
        "      file_path=file_path)\n",
        "  print('Duracao:', datetime.now()-inicio)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./bertimbau_resp/9.4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando amostra: 5 inicio: 2021-09-01 15:27:35.880138\n",
            "1 - inicializando variávies\n",
            "1 - inicializando treinamento\n",
            "Epoch [1/4], Step [3849/30792], Train Loss: 0.6809, Valid Loss: 0.4634\n",
            "Model saved to ==> ./bertimbau_resp/9.4/model.pt\n",
            "Model saved to ==> ./bertimbau_resp/9.4/metrics.pt\n",
            "Epoch [1/4], Step [7698/30792], Train Loss: 0.3366, Valid Loss: 0.3577\n",
            "Model saved to ==> ./bertimbau_resp/9.4/model.pt\n",
            "Model saved to ==> ./bertimbau_resp/9.4/metrics.pt\n",
            "Epoch [2/4], Step [11547/30792], Train Loss: 0.2851, Valid Loss: 0.3530\n",
            "Model saved to ==> ./bertimbau_resp/9.4/model.pt\n",
            "Model saved to ==> ./bertimbau_resp/9.4/metrics.pt\n",
            "Epoch [2/4], Step [15396/30792], Train Loss: 0.2009, Valid Loss: 0.3342\n",
            "Model saved to ==> ./bertimbau_resp/9.4/model.pt\n",
            "Model saved to ==> ./bertimbau_resp/9.4/metrics.pt\n",
            "Epoch [3/4], Step [19245/30792], Train Loss: 0.1899, Valid Loss: 0.3877\n",
            "Epoch [3/4], Step [23094/30792], Train Loss: 0.1268, Valid Loss: 0.3565\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-f9398041d65c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'qtd_categories=[10]\\n#token_train, token_valid, token_test = [],[],[]\\nid_planilha = [9.4]\\ndataset = [5]\\nfor idx, (id_, amostra) in enumerate(zip(id_planilha,dataset)):\\n  create_directory(destination_folder,str(id_))\\n  file_path = destination_folder + \"/\"+str(id_)\\n  print(file_path)\\n  model = BERT(qtd_categories=qtd_categories[idx]).to(device)\\n  optimizer = optim.Adam(model.parameters(), lr=lr)\\n  inicio = datetime.now()\\n  print(\\'Treinando amostra:\\', amostra, \\'inicio:\\', inicio)\\n  tokens = tokenizaAmostra(source_folder+str(amostra))\\n  #token_train.append(tokens[0])\\n  #token_valid.append(tokens[1])\\n  #token_test.append(tokens[2])\\n  train(model=model, \\n      optimizer=optimizer,\\n      num_epochs=4,\\n      train_loader=tokens[0],\\n      valid_loader=tokens[1],\\n      eval_every=len(tokens[0]) // 2,\\n      file_path=file_path)\\n  print(\\'Duracao:\\', datetime.now()-inicio)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-127ed5c899a0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, valid_loader, eval_every, file_path, criterion, num_epochs, best_valid_loss)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# update running values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOCF9IBS-vXS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOLXjoQAhKWE"
      },
      "source": [
        "#!ls ./amostra_news_integrada/\n",
        "\n",
        "\n",
        "\n",
        "!du -sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbLMFDrarFbW"
      },
      "source": [
        "## Valor da função LOSS na descida do gradiente \n",
        "por id de experimento registrado na planilha de controle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHdfTUs0ALnZ"
      },
      "source": [
        "id_planilha = [9,9.1,9.2,9.3,9.4]\n",
        "for id_ in id_planilha:\n",
        "  file_path = destination_folder + \"/\"+str(id_)\n",
        "  train_loss_list, valid_loss_list, global_steps_list = load_metrics(file_path + '/metrics.pt')\n",
        "  plt.plot(global_steps_list, train_loss_list, label='Train')\n",
        "  plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
        "  plt.xlabel('Global Steps')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title = 'Resultado do Treino id ' + str(id_)\n",
        "  plt.legend()\n",
        "  plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAI0EpQOAgM1"
      },
      "source": [
        "### Avaliação\n",
        "* https://towardsdatascience.com/evaluating-categorical-models-e667e17987fd\n",
        "* https://towardsdatascience.com/evaluating-categorical-models-ii-sensitivity-and-specificity-e181e573cff8\n",
        "* https://towardsdatascience.com/metrics-for-imbalanced-classification-41c71549bbb5\n",
        "* https://towardsdatascience.com/multi-class-classification-extracting-performance-metrics-from-the-confusion-matrix-b379b427a872\n",
        "* https://towardsdatascience.com/matthews-correlation-coefficient-when-to-use-it-and-when-to-avoid-it-310b3c923f7e\n",
        "* https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2 \n",
        "*https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1\n",
        "* https://towardsdatascience.com/multi-class-metrics-made-simple-the-kappa-score-aka-cohens-kappa-coefficient-bdea137af09c\n",
        "* https://thedatascientist.com/performance-measures-cohens-kappa-statistic/\n",
        "* Material de aula do Professor André\n",
        "* Material do curso MicroSoft\n",
        "* Predictive Accuracy: A Misleading Performance Measure for Highly Imbalanced Data (sas_metrics.pdf) Paper 942-2017 (Josephine S Akosa, Oklahoma State University)\n",
        "\n",
        "\n",
        "VP = VERDADEIRO POSITIVO<br>\n",
        "VN = VERDADEIRO NEGATIVO<br>\n",
        "FP = FALSO POSITIVO<br>\n",
        "FN = FALSON NEGATIVO<br>\n",
        "TVP = TAXA DE VERDADEIRO POSITIVO<br>\n",
        "TVN = TAXA DE VERDADEIRO NEGATIVO<br>\n",
        "TFP = TAXA DE FALSO POSITIVO<br>\n",
        "TFN = TAXA DE FALSO NEGATIVO<br>\n",
        "\n",
        "Métricas utilizadas:<br>\n",
        "\n",
        "* **Acurácia:** proporção de classificações corretas feitas pelo modelo.<br>\n",
        "        Acurácia = (VP + VN)/(VP+VN+VP+VN)<br>\n",
        "\n",
        "* **Precisão:** proporção correta de classificação feita para uma determinada classe.<br>\n",
        "        Precisão = (VP)/(VP + FP)\n",
        "\n",
        "* **Revocação ou Sensibilidade:** proporção de classificações corretas de VP feitas para uma determinada classe considerando todas as instâncias do dataset de testes.<br>\n",
        "      Revocação = TVP = (VP)/(VP+FN)\n",
        "\n",
        "* **Especificidade:** proporção de classificações corretas de VN feitas para uma determinada classe considerando todas as instâncias do dataset de testes.\n",
        "      Especificidade = TVN = (VN)/(VN + FP)\n",
        "\n",
        "* **Suporte:** quantidade de instâncias de uma determinada classe no dataset de testes.\n",
        "\n",
        "**Para o DataSet 3 - dados desbalanceados**\n",
        "\n",
        "* **F1 é a média harmônica da precisão e da revocação.<br>\n",
        "\n",
        "      F1 = (2 * VP) / (2 * VP + FP + FN)\n",
        "* **MCC:** é o coeficiente de correlação Matthews que é utilizado para classes com tamanhos diferentes. Possui escala entre -1 e 1, sendo que 1 indica uma perfeita predição, 0 representa uma predição aleatória e -1 indica discordância total dos valores preditos e os valores verdadeiros.  \n",
        "\n",
        "      MCC = (VP*VN - FP*FN) / (SQRT(VP+FP)*(VP+FN)*(VN+FP)*(VN+FN)\n",
        "      \n",
        "\n",
        "* **Kappa Score - Cohen's Kappa Coefficient** informa quão melhor o classificador está se saindo em relação ao desempenho de um classificador que simplesmente adivinha aleatoriamente de acordo com a frequência de cada classe:\n",
        "\n",
        "      k = (Po - Pe)/(1-Pe) = 1 - (1 - Po)/(1 - Pe)\n",
        "\n",
        "Po é a concordancia observada e Pe é a concordância esperada. O Kappa de Cohen é sempre menor ou igual a 1, sendo que valores menores ou iguais a zero indicam que o classificador é inútil. Quanto mais próximo de 1, indica que o classificador gera uma concordância quase perfeita.\n",
        "\n",
        "* **Média Geométrica (G-mean)** é uma métrica que mede o equilíbrio entre o desempenho de classificação tanto na classe majoritária quanto na minoritária. Um índice baixo é uma indicação de mau desempenho na classificação dos casos positivos, mesmo que os casos negativos sejam corretamente classificados como tal. Ela é importante para prever o sobreajustamento da classe negativa e o subajustamento da classe positiva. Como nosso dataset não possui classes binárias, a fórmula de cálculo será como se segue (material de aula do Prof. André):\n",
        "\n",
        "      G-mean = $(\\pi_{i=1}^{c}Revocacao_{i})^{1/c}$ \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygXxE2fnYlY9"
      },
      "source": [
        "def calcula_especificidade(matriz_confusao):\n",
        "  FP = matriz_confusao.sum(axis=0) - np.diag(matriz_confusao) \n",
        "  FN = matriz_confusao.sum(axis=1) - np.diag(matriz_confusao)\n",
        "  VP = np.diag(matriz_confusao)\n",
        "  VN = matriz_confusao.sum() - (FP + FN + VP)\n",
        "  FP = FP.astype(float)\n",
        "  FN = FN.astype(float)\n",
        "  VP = VP.astype(float)\n",
        "  VN = VN.astype(float)\n",
        "  TVN = np.sum(VN)/(np.sum(VN) + np.sum(FP))\n",
        "  return TVN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTRdzPfNUmW4"
      },
      "source": [
        "def calcula_especificidade_porClasse(matriz_confusao):\n",
        "  FP = matriz_confusao.sum(axis=0) - np.diag(matriz_confusao) \n",
        "  FN = matriz_confusao.sum(axis=1) - np.diag(matriz_confusao)\n",
        "  VP = np.diag(matriz_confusao)\n",
        "  VN = matriz_confusao.sum() - (FP + FN + VP)\n",
        "  FP = FP.astype(float)\n",
        "  FN = FN.astype(float)\n",
        "  VP = VP.astype(float)\n",
        "  VN = VN.astype(float)\n",
        "  TVN = (VN)/(VN + FP)\n",
        "  return TVN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNfF5dTGibJe"
      },
      "source": [
        "def calcula_GMean_multiclass(revocacao):\n",
        "   revoc = np.array(revocacao)\n",
        "   GMean = revoc.prod()**(1.0/len(revoc))\n",
        "   return GMean  \n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZR9epC7bOAj"
      },
      "source": [
        "def elabora_relatorio_metricas(report, matriz_confusao):\n",
        "  espec = calcula_especificidade_porClasse(matriz_confusao) # calcula o valor da especificidade para cada classe\n",
        "  dfrep = pd.DataFrame(report).transpose() #transforma o conteúdo do classification_report em um dataframe pandas\n",
        "  dfrep_a = dfrep[:-3].copy() # separa as métricas de cada classe do valor da acurácia geral do modelo\n",
        "  dfrep_a['specificity'] = espec # inclui no dataframe o valor da especificidade\n",
        "  dfrep_b  = dfrep[dfrep.index=='accuracy'].copy() #obtem do dataframe somente o valor da acurácia\n",
        "  dfrep_b['specificity']=calcula_especificidade(matriz_confusao) # inclui o valor da especificidade geral de todas as classes\n",
        "  metricas = ['precision', 'recall', 'specificity', 'f1-score', 'support'] #organiza as métricas na ordem desejada\n",
        "  df = pd.concat([dfrep_a[metricas],dfrep_b[metricas]],sort=False) #concatena todos os valores em um único dataframe\n",
        "  df['support'] = df['support'].astype('int')\n",
        "  return df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sGfCMMB9ewJ"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "\n",
        "    #obtem os valores preditos e os valores de teste\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (text, labels), _ in test_loader:\n",
        "                labels = labels.type(torch.LongTensor)  #carrega as classes para uma estrutura pytorch         \n",
        "                labels = labels.to(device)  #carrega a estrutura pytorch para GPU (se houver, foi testado anteriormente)\n",
        "                text = text.type(torch.LongTensor)    #carrega o texto para uma estrutura pytorch\n",
        "                text = text.to(device)          #carrega a estrutura pytorch para GPU (se houver, foi testado anteriormente)\n",
        "                output = model(text, labels)   #submete o texto e a label da classe ao modelo\n",
        "\n",
        "                _, output = output\n",
        "                y_pred.extend(torch.argmax(output, 1).tolist())  #obtem do pytorch de saída do modelo o valor predito.\n",
        "                y_true.extend(labels.tolist()) #obtem do pytorch de teste  valor real.\n",
        "    \n",
        "    print('Classification Report:')\n",
        "    n_classe = np.max(y_true)+1 #obtem o número de classes\n",
        "    report = classification_report(y_true, y_pred, labels=np.arange(0,n_classe), digits=4, output_dict=True) #gera o relatório de métricas\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(0,n_classe)) #gera a matriz de confusao\n",
        "    report = elabora_relatorio_metricas(report, cm) #inclui no relatorio a especificidade\n",
        "    print(report)\n",
        "\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred,labels=np.arange(0,n_classe))\n",
        "    ccmatheus = matthews_corrcoef(y_true, y_pred)\n",
        "    \n",
        "    print(\"Acurácia:\",accuracy_score(y_true, y_pred))\n",
        "    print('Acurácia balanceada:',balanced_accuracy_score(y_true, y_pred))\n",
        "    print('GMean:', calcula_GMean_multiclass(report['recall']))\n",
        "    print('Cohen Kappa Score:', cohen_kappa)\n",
        "    print('Coef. Correlacao Matheus:', ccmatheus)\n",
        "    \n",
        "    \n",
        "    \n",
        "    #ax= plt.subplot()\n",
        "    #sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
        "    #ax.set_title('Confusion Matrix')\n",
        "    #ax.set_xlabel('Predicted Labels')\n",
        "    #ax.set_ylabel('True Labels')\n",
        "    return report\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pc0NIvYbLsm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5g9qV-7OZ5X"
      },
      "source": [
        "#%%time    \n",
        "qtd_categories=[10]\n",
        "id_planilha = [9.4]\n",
        "dataset = [5]\n",
        "for idx, (id_, amostra) in enumerate(zip(id_planilha,dataset)):\n",
        "      print('id_planilha:', id_)\n",
        "      tokens = tokenizaAmostra(source_folder+str(amostra))\n",
        "      best_model =BERT(qtd_categories=qtd_categories[idx]).to(device)\n",
        "      load_checkpoint(destination_folder+\"/\"+str(id_) + '/model.pt', best_model)\n",
        "      evaluate(best_model, tokens[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1koq2k7jrIj"
      },
      "source": [
        "#!rm ./bertimbau_all_categ/amostra_4/*.*\n",
        "#!du -sh\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJrH6W-UEXCK"
      },
      "source": [
        ""
      ]
    }
  ]
}