{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTimbau_Testes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HedersonSantos/Noticias/blob/main/BERT/BERTimbau_com_amostras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKMj5WG6lxGP",
        "outputId": "bc4678c9-9e54-46bd-ff79-0e010c8bf689"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIlZbdsGo_vR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d808ca36-64d9-405a-e5b9-97d5bfe709bf"
      },
      "source": [
        "\n",
        "!pip install torch==1.8.1 torchvision==0.9.1 torchaudio==0.8.\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.8.1\n",
            "  Using cached torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "Collecting torchvision==0.9.1\n",
            "  Using cached torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4 MB)\n",
            "Collecting torchaudio==0.8.\n",
            "  Using cached torchaudio-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1) (7.1.2)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install torch==1.8.1, torchaudio==0.8.0 and torchvision==0.9.1 because these package versions have conflicting dependencies.\u001b[0m\n",
            "\n",
            "The conflict is caused by:\n",
            "    The user requested torch==1.8.1\n",
            "    torchvision 0.9.1 depends on torch==1.8.1\n",
            "    torchaudio 0.8.0 depends on torch==1.8.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky7aWx6DyYU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1deefe6e-110a-490e-c4e3-a2ef020949f6"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Aug  1 18:32:26 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hts6ewoYvUYn"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from google.colab import files, drive\n",
        "\n",
        "# Preliminaries\n",
        "\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "# Models\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "from transformers import AutoTokenizer  # Or BertTokenizer\n",
        "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
        "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
        "\n",
        "# Training\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPOdGbGitPHS"
      },
      "source": [
        "### Carregando tokens e vocabulário do BERTimbau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLcgDJZSpmfD",
        "outputId": "06511dbe-cf14-4a12-cf73-947f5ddc7e71"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joJqFofvtKcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ae0e32-2697-416b-df39-9591e93f7866"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizerFast(name_or_path='neuralmind/bert-base-portuguese-cased', vocab_size=29794, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZOL2o_ltyzz"
      },
      "source": [
        "### Preparando DataSet\n",
        "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFtrA_dtunW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32efbd4a-cea7-4b33-8417-d31cb87ee0aa"
      },
      "source": [
        "#!rm *.csv\n",
        "#!!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/train.csv\n",
        "#!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/valid.csv\n",
        "#!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/test.csv\n",
        "#!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/miscelanea.csv\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/Colab\\ Notebooks\n",
        "path = '/gdrive/My Drive/Colab Notebooks'\n",
        "KEY='4/1AX4XfWiQe0_gzMbt_ESEYXu4XM6IA5-fzg63z5pT8Hgf8_lGAjxUXwlE5uU'"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFFldLWvV6Mg"
      },
      "source": [
        "#%cd amostra_news_integrada\n",
        "#!unzip drive-download-20210731T211249Z-001.zip\n",
        "#!ls -lh\n",
        "\n",
        "#!mkdir bertimbau_all_categ\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfj3koMmWsZ9"
      },
      "source": [
        "#!mkdir ./bertimbau_all_categ/amostra_4\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV8T4nMYXqdR",
        "outputId": "917ef1ef-e3a9-4e1a-8cd7-096b8ff5a77b"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sjrTvfBLGCL"
      },
      "source": [
        "source_folder = './amostra_news_integrada' #'/content'\n",
        "destination_folder = './bertimbau_all_categ' #'/content'\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90WJxOxcxgAQ",
        "outputId": "53c86bcf-f108-4e3c-d0c6-e5e498faf47f"
      },
      "source": [
        "!ls ./amostra_news_integrada"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test.csv  train.csv  valid.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyoK7ac8t4HF"
      },
      "source": [
        "#https://towardsdatascience.com/multi-class-text-classification-with-deep-learning-using-bert-b59ca2f5c613\n",
        "#Model hyper-parameter\n",
        "MAX_SEQ_LEN = 128 #limita os artigos em 128 tokens. Bert é limitado em 512 tokens por texto (checar isto e aplicar limpeza nos textos).\n",
        "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
        "bs = 16\n",
        "lr = 1e-5\n",
        "qtd_categories=11\n",
        "qtd_amostra=4\n",
        "\n",
        "# Fields - use_vocab=False  e tokenizer.encode permite que utilizemos os tokens do BERTimbau.\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
        "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
        "fields = [('text', text_field),('label', label_field)]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqRpEtW1OuqC"
      },
      "source": [
        "def tokenizaAmostra(source_folder, fields=fields):\n",
        "  # TabularDataset\n",
        "  train, valid, test = TabularDataset.splits(path=source_folder, train='train.csv', validation='valid.csv',\n",
        "                                            test='test.csv', format='CSV', fields=fields, skip_header=True)\n",
        "  # Iterators\n",
        "\n",
        "  train_iter = BucketIterator(train, batch_size=bs, sort_key=lambda x: len(x.text),\n",
        "                              device=device, train=True, sort=True, sort_within_batch=True)\n",
        "  valid_iter = BucketIterator(valid, batch_size=bs, sort_key=lambda x: len(x.text),\n",
        "                              device=device, train=True, sort=True, sort_within_batch=True)\n",
        "  test_iter = Iterator(test, batch_size=bs, device=device, train=False, shuffle=False, sort=False)\n",
        "  return [train_iter, valid_iter, test_iter]\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8maI4HcbPukL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcTHSXSluKkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d05730-a0bf-453a-9d0c-ae8bed8e29ff"
      },
      "source": [
        "print(vars(train[0]))\n",
        "print(vars(valid[0]))\n",
        "print(vars(test[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': [101, 761, 146, 100, 1247, 229, 10622, 171, 338, 350, 8551, 22287, 125, 511, 174, 171, 2088, 125, 2631, 10922, 117, 229, 3787, 117, 146, 1910, 5789, 5754, 11596, 3660, 6151, 118, 14258, 113, 1934, 114, 327, 5926, 229, 2310, 221, 259, 3956, 6955, 171, 1007, 125, 1455, 119, 2067, 180, 5926, 15669, 117, 5754, 117, 179, 2286, 123, 4428, 11838, 22307, 117, 1934, 2038, 117, 346, 12900, 353, 2171, 774, 180, 3109, 118, 820, 259, 1242, 1867, 8978, 176, 15709, 228, 119, 231, 1910, 262, 12439, 243, 170, 123, 5926, 229, 1007, 118, 4284, 2113, 146, 13355, 8905, 178, 19734, 117, 179, 770, 5491, 5926, 423, 2453, 298, 3956, 4782, 118, 13421, 22281, 125, 11719, 117, 122, 146, 3065, 21077, 16180, 7832, 735, 117, 5905, 171, 2236, 9920, 117, 5157, 420, 259, 1242, 1867, 119, 107, 3396, 346, 15212, 2798, 3724, 221, 4640, 119, 9009, 203, 785, 117, 785, 8540, 119, 1643, 2779, 770, 8544, 13685, 1950, 4029, 2646, 170, 146, 2453, 117, 240, 370, 1858, 576, 179, 6331, 230, 2341, 567, 117, 179, 253, 230, 2310, 13661, 117, 170, 325, 125, 3055, 7834, 117, 123, 8653, 3429, 119, 10751, 370, 244, 222, 622, 7821, 1203, 221, 9767, 149, 2291, 117, 221, 311, 12274, 119, 18153, 22281, 3955, 5863, 173, 2631, 10922, 506, 2980, 179, 173, 11719, 117, 848, 253, 4227, 179, 146, 10456, 262, 2160, 4863, 119, 10587, 3171, 12357, 246, 173, 3885, 3354, 107, 117, 5222, 119, 503, 681, 2171, 180, 3623, 117, 146, 1910, 1767, 173, 100, 1247, 117, 170, 11662, 22315, 119, 1934, 119, 5789, 5754, 770, 5998, 259, 3956, 6955, 125, 10507, 118, 4591, 117, 13706, 118, 3500, 122, 3250, 118, 3470, 119, 5027, 15783, 15929, 1968, 2631, 10922, 102], 'label': '3'}\n",
            "{'text': [101, 231, 1997, 125, 1007, 6277, 698, 558, 8269, 243, 221, 123, 4251, 125, 5517, 240, 2318, 180, 13086, 6489, 179, 11903, 123, 1855, 171, 7236, 119, 5264, 370, 356, 118, 14258, 113, 511, 114, 117, 146, 2187, 7236, 2080, 712, 542, 117, 1242, 2390, 117, 4513, 325, 125, 977, 119, 6324, 1101, 1950, 2042, 6839, 119, 15697, 297, 592, 12179, 122, 325, 125, 5011, 592, 1101, 506, 19706, 22281, 412, 13540, 365, 671, 119, 8599, 1136, 209, 4282, 122, 6184, 1376, 19115, 22281, 123, 10987, 1676, 8443, 11401, 119, 510, 864, 14607, 179, 13933, 125, 2831, 712, 682, 1649, 8112, 180, 651, 506, 20304, 240, 3659, 125, 3320, 119, 4505, 353, 13086, 117, 260, 13429, 193, 315, 5449, 4412, 150, 834, 5093, 2151, 548, 123, 4669, 7250, 118, 14258, 113, 888, 114, 122, 260, 6880, 506, 20083, 138, 529, 3577, 119, 231, 3261, 171, 7236, 117, 7165, 150, 13056, 113, 8690, 114, 117, 11283, 229, 6293, 1014, 370, 356, 117, 173, 4331, 353, 4471, 117, 179, 1007, 6277, 1820, 5121, 125, 4412, 834, 1991, 1652, 146, 2187, 9007, 542, 117, 3199, 2390, 119, 3393, 2113, 146, 2187, 7236, 2991, 123, 5783, 146, 1181, 125, 853, 1068, 180, 1310, 3160, 125, 3896, 125, 1991, 117, 2806, 240, 8226, 943, 9720, 110, 180, 1855, 119, 107, 530, 5334, 391, 146, 179, 698, 10155, 1790, 117, 1007, 6277, 1820, 146, 5121, 125, 4412, 834, 853, 1068, 119, 3074, 230, 13367, 125, 179, 6022, 3767, 712, 542, 117, 1934, 2390, 117, 122, 176, 1257, 9963, 706, 5110, 1921, 20310, 125, 1991, 107, 117, 11283, 146, 3261, 119, 102], 'label': '5'}\n",
            "{'text': [101, 1431, 1743, 2623, 8039, 22303, 146, 14566, 180, 1158, 715, 182, 125, 117, 3102, 14157, 113, 1242, 114, 117, 3859, 146, 15692, 21793, 117, 2740, 179, 659, 670, 171, 213, 465, 125, 5252, 15438, 2107, 557, 2465, 125, 6482, 1839, 171, 2363, 119, 177, 8729, 125, 2667, 9963, 22303, 229, 4768, 7161, 7929, 117, 3704, 171, 10452, 117, 1000, 2336, 22296, 119, 231, 4794, 171, 21098, 171, 2740, 262, 2160, 117, 995, 3565, 412, 7586, 171, 9091, 11513, 140, 488, 171, 9744, 202, 13479, 117, 240, 5523, 5243, 15134, 185, 5019, 122, 123, 1743, 2623, 117, 18765, 423, 7472, 7593, 1389, 6315, 247, 119, 107, 1602, 1257, 1203, 2080, 221, 123, 9349, 3185, 2097, 353, 2954, 117, 346, 3363, 596, 125, 7871, 367, 107, 117, 2275, 146, 6533, 4858, 125, 2165, 117, 503, 980, 11858, 14200, 22283, 119, 107, 192, 271, 770, 698, 16643, 170, 1719, 123, 2388, 5231, 3281, 221, 146, 2740, 117, 122, 1021, 123, 13874, 366, 1101, 117, 19821, 10682, 118, 1340, 229, 4768, 7161, 7929, 119, 107, 2195, 146, 6533, 117, 146, 5607, 10019, 202, 4794, 125, 21098, 1467, 123, 739, 3442, 125, 1101, 3106, 3263, 221, 146, 2740, 179, 117, 173, 3843, 117, 652, 622, 171, 15692, 21793, 229, 1158, 715, 182, 113, 368, 1366, 1065, 3693, 202, 14566, 3275, 1586, 13024, 7715, 508, 114, 117, 7493, 17856, 183, 229, 3302, 122, 1229, 326, 1068, 119, 107, 231, 179, 368, 164, 146, 7854, 403, 166, 7199, 262, 179, 202, 622, 3714, 117, 625, 3363, 222, 2740, 4327, 117, 3363, 222, 1189, 9983, 1430, 125, 1101, 122, 1257, 4654, 5990, 259, 18854, 107, 117, 1331, 503, 980, 119, 107, 3758, 12245, 5220, 5267, 793, 2535, 117, 1021, 230, 18976, 171, 1189, 125, 1101, 122, 123, 4128, 298, 15896, 346, 262, 2850, 202, 1946, 1238, 117, 221, 346, 176, 9928, 17856, 183, 119, 107, 231, 15692, 21793, 253, 6568, 125, 4353, 125, 10034, 298, 4954, 11272, 22281, 202, 1158, 715, 182, 1065, 146, 622, 3714, 117, 179, 10801, 4747, 22303, 118, 1340, 273, 16624, 1789, 22280, 119, 107, 2664, 1434, 210, 1257, 1858, 576, 253, 222, 417, 1017, 119, 989, 230, 17507, 125, 222, 1247, 15797, 117, 107, 2275, 146, 7131, 7221, 4061, 157, 2245, 117, 171, 8452, 125, 8285, 171, 14566, 180, 1158, 715, 182, 119, 959, 503, 980, 117, 202, 1325, 117, 146, 2740, 253, 230, 547, 125, 18699, 910, 171, 2363, 119, 107, 2501, 125, 5110, 222, 8122, 1788, 498, 1257, 117, 449, 7749, 698, 273, 16624, 1375, 214, 260, 1101, 179, 1376, 1369, 117, 423, 3811, 119, 107, 177, 15492, 346, 2286, 21695, 146, 7472, 7593, 1389, 6315, 247, 122, 5523, 5243, 15134, 185, 5019, 119, 102], 'label': '6'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdXa_EMroqXz",
        "outputId": "1425371d-1cce-4186-ce81-5c00f939167d"
      },
      "source": [
        "j=3\n",
        "for i in train_iter:\n",
        "  print(i)\n",
        "  j-=1\n",
        "  if j<0:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.LongTensor of size 16x128]\n",
            "\t[.label]:[torch.FloatTensor of size 16]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.LongTensor of size 16x128]\n",
            "\t[.label]:[torch.FloatTensor of size 16]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.LongTensor of size 16x128]\n",
            "\t[.label]:[torch.FloatTensor of size 16]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.LongTensor of size 16x128]\n",
            "\t[.label]:[torch.FloatTensor of size 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPypCNPuHaRz"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvy3rgrdHj_x"
      },
      "source": [
        "'''É preciso informar o número de labels, pois o BERTimbau foi treinado para 2 classes '''\n",
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        options_name = \"bert-base-portuguese-cased\"\n",
        "        self.encoder = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased',num_labels=qtd_categories )\n",
        "\n",
        "    def forward(self, text, label):\n",
        "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
        "\n",
        "        return loss, text_fea\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pYKbHICIus0"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJFwyl7DIoxQ"
      },
      "source": [
        "# Save and Load Functions\n",
        "\n",
        "def save_checkpoint(save_path, model, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(load_path, model):\n",
        "    \n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiueExF5Jb8V"
      },
      "source": [
        "# Training Function\n",
        "'''criterion = nn.BCELoss() é BinaryCrossEntropy é a função de perda para targets binarios. Como o nosso alvo possui\n",
        "muitas classes troque a função de perda para nn.CrossEntropyLoss() '''\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          train_loader ,\n",
        "          valid_loader ,\n",
        "          eval_every ,\n",
        "          file_path ,\n",
        "          criterion = nn.CrossEntropyLoss(), #nn.BCELoss(),\n",
        "          num_epochs = 5,\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "    \n",
        "    \n",
        "    print('1 - inicializando variávies')\n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    # training loop\n",
        "    print('1 - inicializando treinamento')\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for (text, labels), _ in train_loader:\n",
        "            labels = labels.type(torch.LongTensor)           \n",
        "            #print('label size:', labels.size())\n",
        "            #print('label:', labels)\n",
        "            labels = labels.to(device)\n",
        "            text = text.type(torch.LongTensor) \n",
        "            #print('text:', text.size())\n",
        "            text = text.to(device)\n",
        "            #print('treina...')\n",
        "            output = model(text, labels)\n",
        "            #print('fim treino...')\n",
        "            loss, _ = output\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "\n",
        "                    # validation loop\n",
        "                    for (text,labels), _ in valid_loader:\n",
        "                        text = text.type(torch.LongTensor)  \n",
        "                        text = text.to(device)\n",
        "                        labels = labels.type(torch.LongTensor)           \n",
        "                        labels = labels.to(device)\n",
        "                        \n",
        "                        output = model(text, labels)\n",
        "                        loss, _ = output\n",
        "                        \n",
        "                        valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
        "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    \n",
        "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')\n",
        "    "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQcJQ5kaMPa4",
        "outputId": "a87d13c1-351d-49e8-8cee-1a567f0435c3"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "RrkvwE2VL1qY",
        "outputId": "e2e2f67b-e136-4eb3-a5e1-600dd04a2c06"
      },
      "source": [
        "model = BERT().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "token_train, token_valid, token_test=[],[],[]\n",
        "for amostra in range(1,qtd_amostra+1):\n",
        "  tokens = tokenizaAmostra('./amostra_news_integrada/amostra_'+str(amostra))\n",
        "  token_train.append(tokens[0])\n",
        "  token_valid.append(tokens[1])\n",
        "  token_test.append(tokens[2])\n",
        "  train(model=model, \n",
        "        optimizer=optimizer,\n",
        "        train_loader=tokens[amostra-1],\n",
        "        valid_loader=token_valid[amostra-1],\n",
        "        eval_every=len(tokens[amostra-1]) // 2,\n",
        "        file_path='./bertimbau_all_categ/amostra_' + str(amostra))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-fc8993848125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtoken_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mamostra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqtd_amostra\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizaAmostra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./amostra_news_integrada/amostra_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamostra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mtoken_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mtoken_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tokenizaAmostra() missing 1 required positional argument: 'fields'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOLXjoQAhKWE"
      },
      "source": [
        "!ls ./bertimbau_all_categ/amostra_1"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHdfTUs0ALnZ"
      },
      "source": [
        "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
        "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
        "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
        "plt.xlabel('Global Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAI0EpQOAgM1"
      },
      "source": [
        "### Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sGfCMMB9ewJ"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (text, labels), _ in test_loader:\n",
        "                labels = labels.type(torch.LongTensor)           \n",
        "                labels = labels.to(device)\n",
        "                text = text.type(torch.LongTensor)  \n",
        "                text = text.to(device)\n",
        "                output = model(text, labels)\n",
        "\n",
        "                _, output = output\n",
        "                y_pred.extend(torch.argmax(output, 1).tolist())\n",
        "                y_true.extend(labels.tolist())\n",
        "    \n",
        "    print('Classification Report:')\n",
        "    n_classe = np.max(y_true)+1\n",
        "    print(classification_report(y_true, y_pred, labels=np.arange(0,n_classe), digits=4))\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(0,n_classe))\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
        "\n",
        "    ax.set_title('Confusion Matrix')\n",
        "\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "\n",
        "    \n",
        "    \n",
        "best_model = BERT().to(device)\n",
        "\n",
        "load_checkpoint(destination_folder + '/model.pt', best_model)\n",
        "\n",
        "evaluate(best_model, test_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIA9kWsxs7SQ"
      },
      "source": [
        "## Salvando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg_nC_Pyu7vn"
      },
      "source": [
        "from google.colab import files, drive\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/Colab\\ Notebooks/nlp_tcc\n",
        "path = '/gdrive/My Drive/Colab Notebooks/nlp_tcc'\n",
        "arquivo = '/bertimbau_classifier_pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuUSiZFswnGs"
      },
      "source": [
        "pickle.dump(best_model, open(path+arquivo,'wb'))\n",
        "pickle.dump(tokenizer, open(path+arquivo+'_token','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBuMztZ-gwyN"
      },
      "source": [
        "## Aplicando o modelo nas miscelâneas de notícias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBAsMn2o3rGc"
      },
      "source": [
        "#carregando o modelo e o tokenizador\n",
        "model = pickle.load(open(path+arquivo,'rb'))\n",
        "token = pickle.load(open(path+arquivo+'_token','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnFvPwrWgnX5"
      },
      "source": [
        "### Tokenizando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTxwEmKpjmjR"
      },
      "source": [
        "# TabularDataset\n",
        "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
        "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
        "fields = [('text', text_field)]\n",
        "\n",
        "miscelanea = TabularDataset.splits(path=source_folder, miscelanea='miscelanea.csv', \n",
        "                                  format='CSV',fields=fields, skip_header=True)\n",
        "\n",
        "# Iterators\n",
        "\n",
        "miscelanea_iter = BucketIterator(miscelanea, batch_size=bs, sort_key=lambda x: len(x.text),\n",
        "                            device=device, train=True, sort=True, sort_within_batch=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1koq2k7jrIj"
      },
      "source": [
        "tokenizer(\"Hoje é um belo dia de sábado, porém estou aqui estudando.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJrH6W-UEXCK"
      },
      "source": [
        ""
      ]
    }
  ]
}