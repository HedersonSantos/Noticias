{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTimbau_Testes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HedersonSantos/Noticias/blob/main/BERT/BERTimbau_com_amostras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKMj5WG6lxGP",
        "outputId": "8e35392c-9e49-4935-bc81-0ce7456ac202"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIlZbdsGo_vR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89d2a0f-d318-41e1-cff3-6d17ac3b62e8"
      },
      "source": [
        "\n",
        "!pip install torch==1.8.1 torchvision==0.9.1 torchaudio==0.8.\n"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.8.1\n",
            "  Using cached torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "Collecting torchvision==0.9.1\n",
            "  Using cached torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4 MB)\n",
            "Collecting torchaudio==0.8.\n",
            "  Using cached torchaudio-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1) (7.1.2)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install torch==1.8.1, torchaudio==0.8.0 and torchvision==0.9.1 because these package versions have conflicting dependencies.\u001b[0m\n",
            "\n",
            "The conflict is caused by:\n",
            "    The user requested torch==1.8.1\n",
            "    torchvision 0.9.1 depends on torch==1.8.1\n",
            "    torchaudio 0.8.0 depends on torch==1.8.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky7aWx6DyYU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394471f7-31f8-43b9-8972-e0fae1b5e63a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Aug  1 22:40:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    28W /  70W |   4632MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hts6ewoYvUYn"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from google.colab import files, drive\n",
        "\n",
        "# Preliminaries\n",
        "\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "# Models\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "from transformers import AutoTokenizer  # Or BertTokenizer\n",
        "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
        "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
        "\n",
        "# Training\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPOdGbGitPHS"
      },
      "source": [
        "### Carregando tokens e vocabulário do BERTimbau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLcgDJZSpmfD",
        "outputId": "0b51833b-4027-4a11-d90d-59c432a1d2d2"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joJqFofvtKcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43bbd42c-a053-4c3f-ffe9-e07967d1749f"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizerFast(name_or_path='neuralmind/bert-base-portuguese-cased', vocab_size=29794, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZOL2o_ltyzz"
      },
      "source": [
        "### Preparando DataSet\n",
        "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFtrA_dtunW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe4e922-23ac-4c1d-edf2-d7697b61b52c"
      },
      "source": [
        "#!rm *.csv\n",
        "#!!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/train.csv\n",
        "#!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/valid.csv\n",
        "#!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/test.csv\n",
        "#!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/miscelanea.csv\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/Colab\\ Notebooks\n",
        "path = '/gdrive/My Drive/Colab Notebooks'\n",
        "KEY='4/1AX4XfWiQe0_gzMbt_ESEYXu4XM6IA5-fzg63z5pT8Hgf8_lGAjxUXwlE5uU'"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFFldLWvV6Mg"
      },
      "source": [
        "#%cd amostra_news_integrada\n",
        "#!unzip drive-download-20210731T211249Z-001.zip\n",
        "#!ls -lh\n",
        "\n",
        "#!mkdir bertimbau_all_categ\n"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfj3koMmWsZ9"
      },
      "source": [
        "#!mkdir ./bertimbau_all_categ/amostra_4\n"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV8T4nMYXqdR",
        "outputId": "a8017745-f432-44eb-813a-8fc3a6ec60fd"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sjrTvfBLGCL"
      },
      "source": [
        "source_folder = './amostra_news_integrada/amostra_' #'/content'\n",
        "destination_folder = './bertimbau_all_categ/amostra_' #'/content'\n"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90WJxOxcxgAQ",
        "outputId": "fbaa4ada-79cf-4367-900a-b099eb9694bc"
      },
      "source": [
        "!ls ./amostra_news_integrada"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "amostra_1  amostra_2  amostra_3  amostra_4  test.csv  train.csv  valid.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyoK7ac8t4HF"
      },
      "source": [
        "#https://towardsdatascience.com/multi-class-text-classification-with-deep-learning-using-bert-b59ca2f5c613\n",
        "#Model hyper-parameter\n",
        "MAX_SEQ_LEN = 128 #limita os artigos em 128 tokens. Bert é limitado em 512 tokens por texto (checar isto e aplicar limpeza nos textos).\n",
        "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
        "bs = 32\n",
        "lr = 2e-5\n",
        "qtd_categories=11\n",
        "qtd_amostra=4\n",
        "\n",
        "# Fields - use_vocab=False  e tokenizer.encode permite que utilizemos os tokens do BERTimbau.\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
        "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
        "fields = [('text', text_field),('label', label_field)]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqRpEtW1OuqC"
      },
      "source": [
        "def tokenizaAmostra(source_folder, fields=fields):\n",
        "  # TabularDataset\n",
        "  train, valid, test = TabularDataset.splits(path=source_folder, train='train.csv', validation='valid.csv',\n",
        "                                            test='test.csv', format='CSV', fields=fields, skip_header=True)\n",
        "  # Iterators\n",
        "\n",
        "  train_iter = BucketIterator(train, batch_size=bs, sort_key=lambda x: len(x.text),\n",
        "                              device=device, train=True, sort=True, sort_within_batch=True)\n",
        "  valid_iter = BucketIterator(valid, batch_size=bs, sort_key=lambda x: len(x.text),\n",
        "                              device=device, train=True, sort=True, sort_within_batch=True)\n",
        "  test_iter = Iterator(test, batch_size=bs, device=device, train=False, shuffle=False, sort=False)\n",
        "  return [train_iter, valid_iter, test_iter]\n",
        "\n"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8maI4HcbPukL"
      },
      "source": [
        ""
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcTHSXSluKkt"
      },
      "source": [
        "#print(vars(train[0]))\n",
        "#print(vars(valid[0]))\n",
        "#print(vars(test[0]))"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPypCNPuHaRz"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvy3rgrdHj_x"
      },
      "source": [
        "'''É preciso informar o número de labels '''\n",
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        options_name = \"bert-base-portuguese-cased\"\n",
        "        self.encoder = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased',num_labels=qtd_categories )\n",
        "\n",
        "    def forward(self, text, label):\n",
        "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
        "\n",
        "        return loss, text_fea\n",
        "\n"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pYKbHICIus0"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJFwyl7DIoxQ"
      },
      "source": [
        "# Save and Load Functions\n",
        "\n",
        "def save_checkpoint(save_path, model, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(load_path, model):\n",
        "    \n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiueExF5Jb8V"
      },
      "source": [
        "# Training Function\n",
        "'''criterion = nn.BCELoss() é BinaryCrossEntropy é a função de perda para targets binarios. Como o nosso alvo possui\n",
        "muitas classes troque a função de perda para nn.CrossEntropyLoss() '''\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          train_loader ,\n",
        "          valid_loader ,\n",
        "          eval_every ,\n",
        "          file_path ,\n",
        "          criterion = nn.CrossEntropyLoss(), #nn.BCELoss(),\n",
        "          num_epochs = 5,\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "    \n",
        "    \n",
        "    print('1 - inicializando variávies')\n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    # training loop\n",
        "    print('1 - inicializando treinamento')\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for (text, labels), _ in train_loader:\n",
        "            labels = labels.type(torch.LongTensor)           \n",
        "            #print('label size:', labels.size())\n",
        "            #print('label:', labels)\n",
        "            labels = labels.to(device)\n",
        "            text = text.type(torch.LongTensor) \n",
        "            #print('text:', text.size())\n",
        "            text = text.to(device)\n",
        "            #print('treina...')\n",
        "            output = model(text, labels)\n",
        "            #print('fim treino...')\n",
        "            loss, _ = output\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "\n",
        "                    # validation loop\n",
        "                    for (text,labels), _ in valid_loader:\n",
        "                        text = text.type(torch.LongTensor)  \n",
        "                        text = text.to(device)\n",
        "                        labels = labels.type(torch.LongTensor)           \n",
        "                        labels = labels.to(device)\n",
        "                        \n",
        "                        output = model(text, labels)\n",
        "                        loss, _ = output\n",
        "                        \n",
        "                        valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
        "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    \n",
        "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')\n",
        "    "
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQcJQ5kaMPa4",
        "outputId": "e1921864-36e3-452e-8ac7-1fd2087b8156"
      },
      "source": [
        "device"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrkvwE2VL1qY",
        "outputId": "d86a0f93-55ae-443b-ccbb-ce88435b1c20"
      },
      "source": [
        "%%time\n",
        "\n",
        "token_train, token_valid, token_test=[],[],[]\n",
        "for amostra in range(1,qtd_amostra+1):\n",
        "  model = BERT().to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  inicio = datetime.now()\n",
        "  print('Treinando amostra:', amostra, 'inicio:', inicio)\n",
        "  tokens = tokenizaAmostra(source_folder+str(amostra))\n",
        "  token_train.append(tokens[0])\n",
        "  token_valid.append(tokens[1])\n",
        "  token_test.append(tokens[2])\n",
        "  train(model=model, \n",
        "      optimizer=optimizer,\n",
        "      train_loader=token_train[amostra-1],\n",
        "      valid_loader=token_valid[amostra-1],\n",
        "      eval_every=len(token_train[amostra-1]) // 2,\n",
        "      file_path=destination_folder + str(amostra))\n",
        "  print('Duracao:', datetime.now()-inicio)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Treinando amostra: 1 inicio: 2021-08-01 23:19:42.898432\n",
            "1 - inicializando variávies\n",
            "1 - inicializando treinamento\n",
            "Epoch [1/5], Step [137/1375], Train Loss: 1.5369, Valid Loss: 1.0424\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_1/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_1/metrics.pt\n",
            "Epoch [1/5], Step [274/1375], Train Loss: 0.7632, Valid Loss: 0.8386\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_1/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_1/metrics.pt\n",
            "Epoch [2/5], Step [411/1375], Train Loss: 0.6218, Valid Loss: 0.7095\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_1/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_1/metrics.pt\n",
            "Epoch [2/5], Step [548/1375], Train Loss: 0.4008, Valid Loss: 0.7224\n",
            "Epoch [3/5], Step [685/1375], Train Loss: 0.4559, Valid Loss: 0.6608\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_1/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_1/metrics.pt\n",
            "Epoch [3/5], Step [822/1375], Train Loss: 0.2202, Valid Loss: 0.8215\n",
            "Epoch [4/5], Step [959/1375], Train Loss: 0.3491, Valid Loss: 0.6567\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_1/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_1/metrics.pt\n",
            "Epoch [4/5], Step [1096/1375], Train Loss: 0.1382, Valid Loss: 0.7718\n",
            "Epoch [5/5], Step [1233/1375], Train Loss: 0.2600, Valid Loss: 0.7502\n",
            "Epoch [5/5], Step [1370/1375], Train Loss: 0.0957, Valid Loss: 0.7594\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_1/metrics.pt\n",
            "Finished Training!\n",
            "Duracao: 0:19:34.780402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Treinando amostra: 2 inicio: 2021-08-01 23:39:19.438052\n",
            "1 - inicializando variávies\n",
            "1 - inicializando treinamento\n",
            "Epoch [1/5], Step [137/1375], Train Loss: 1.5798, Valid Loss: 1.0588\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_2/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_2/metrics.pt\n",
            "Epoch [1/5], Step [274/1375], Train Loss: 0.7883, Valid Loss: 0.8665\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_2/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_2/metrics.pt\n",
            "Epoch [2/5], Step [411/1375], Train Loss: 0.6233, Valid Loss: 0.7176\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_2/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_2/metrics.pt\n",
            "Epoch [2/5], Step [548/1375], Train Loss: 0.4286, Valid Loss: 0.7055\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_2/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_2/metrics.pt\n",
            "Epoch [3/5], Step [685/1375], Train Loss: 0.4031, Valid Loss: 0.6326\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_2/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_2/metrics.pt\n",
            "Epoch [3/5], Step [822/1375], Train Loss: 0.2605, Valid Loss: 0.7631\n",
            "Epoch [4/5], Step [959/1375], Train Loss: 0.2717, Valid Loss: 0.6347\n",
            "Epoch [4/5], Step [1096/1375], Train Loss: 0.1663, Valid Loss: 0.6873\n",
            "Epoch [5/5], Step [1233/1375], Train Loss: 0.1783, Valid Loss: 0.7254\n",
            "Epoch [5/5], Step [1370/1375], Train Loss: 0.1193, Valid Loss: 0.6869\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_2/metrics.pt\n",
            "Finished Training!\n",
            "Duracao: 0:19:39.138746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Treinando amostra: 3 inicio: 2021-08-01 23:59:00.626561\n",
            "1 - inicializando variávies\n",
            "1 - inicializando treinamento\n",
            "Epoch [1/5], Step [137/1375], Train Loss: 1.5363, Valid Loss: 1.0975\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_3/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_3/metrics.pt\n",
            "Epoch [1/5], Step [274/1375], Train Loss: 0.8093, Valid Loss: 0.8142\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_3/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_3/metrics.pt\n",
            "Epoch [2/5], Step [411/1375], Train Loss: 0.5744, Valid Loss: 0.6569\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_3/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_3/metrics.pt\n",
            "Epoch [2/5], Step [548/1375], Train Loss: 0.4031, Valid Loss: 0.6626\n",
            "Epoch [3/5], Step [685/1375], Train Loss: 0.3944, Valid Loss: 0.5812\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_3/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_3/metrics.pt\n",
            "Epoch [3/5], Step [822/1375], Train Loss: 0.2387, Valid Loss: 0.6144\n",
            "Epoch [4/5], Step [959/1375], Train Loss: 0.2948, Valid Loss: 0.6091\n",
            "Epoch [4/5], Step [1096/1375], Train Loss: 0.1517, Valid Loss: 0.6016\n",
            "Epoch [5/5], Step [1233/1375], Train Loss: 0.1987, Valid Loss: 0.6688\n",
            "Epoch [5/5], Step [1370/1375], Train Loss: 0.1061, Valid Loss: 0.6865\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_3/metrics.pt\n",
            "Finished Training!\n",
            "Duracao: 0:19:39.537562\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Treinando amostra: 4 inicio: 2021-08-02 00:18:42.091953\n",
            "1 - inicializando variávies\n",
            "1 - inicializando treinamento\n",
            "Epoch [1/5], Step [137/1375], Train Loss: 1.7149, Valid Loss: 1.1090\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_4/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_4/metrics.pt\n",
            "Epoch [1/5], Step [274/1375], Train Loss: 0.7669, Valid Loss: 0.8825\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_4/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_4/metrics.pt\n",
            "Epoch [2/5], Step [411/1375], Train Loss: 0.6180, Valid Loss: 0.7153\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_4/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_4/metrics.pt\n",
            "Epoch [2/5], Step [548/1375], Train Loss: 0.4004, Valid Loss: 0.7277\n",
            "Epoch [3/5], Step [685/1375], Train Loss: 0.3981, Valid Loss: 0.6138\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_4/model.pt\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_4/metrics.pt\n",
            "Epoch [3/5], Step [822/1375], Train Loss: 0.2419, Valid Loss: 0.8522\n",
            "Epoch [4/5], Step [959/1375], Train Loss: 0.2620, Valid Loss: 0.6420\n",
            "Epoch [4/5], Step [1096/1375], Train Loss: 0.1503, Valid Loss: 0.7756\n",
            "Epoch [5/5], Step [1233/1375], Train Loss: 0.1765, Valid Loss: 0.7009\n",
            "Epoch [5/5], Step [1370/1375], Train Loss: 0.1041, Valid Loss: 0.7655\n",
            "Model saved to ==> ./bertimbau_all_categ/amostra_4/metrics.pt\n",
            "Finished Training!\n",
            "Duracao: 0:19:37.887512\n",
            "CPU times: user 1h 5min 38s, sys: 12min 26s, total: 1h 18min 4s\n",
            "Wall time: 1h 18min 39s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOLXjoQAhKWE"
      },
      "source": [
        "#!ls ./amostra_news_integrada/\n",
        "\n",
        "#!rm  ./bertimbau_all_categ/amostra_4/*.*\n",
        "\n",
        "!du -sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHdfTUs0ALnZ"
      },
      "source": [
        "for amostra in range(1,qtd_amostra+1):\n",
        "  train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder+str(amostra) + '/metrics.pt')\n",
        "  plt.plot(global_steps_list, train_loss_list, label='Train')\n",
        "  plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
        "  plt.xlabel('Global Steps')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title = 'Resultado do Treino da amostra ' + str(amostra)\n",
        "  plt.legend()\n",
        "  plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAI0EpQOAgM1"
      },
      "source": [
        "### Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sGfCMMB9ewJ"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (text, labels), _ in test_loader:\n",
        "                labels = labels.type(torch.LongTensor)           \n",
        "                labels = labels.to(device)\n",
        "                text = text.type(torch.LongTensor)  \n",
        "                text = text.to(device)\n",
        "                output = model(text, labels)\n",
        "\n",
        "                _, output = output\n",
        "                y_pred.extend(torch.argmax(output, 1).tolist())\n",
        "                y_true.extend(labels.tolist())\n",
        "    \n",
        "    print('Classification Report:')\n",
        "    n_classe = np.max(y_true)+1\n",
        "    print(classification_report(y_true, y_pred, labels=np.arange(0,n_classe), digits=4))\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(0,n_classe))\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
        "\n",
        "    ax.set_title('Confusion Matrix')\n",
        "\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "\n",
        "%%time    \n",
        "for amostra in range(1,qtd_amostra+1):    \n",
        "  best_model = BERT().to(device)\n",
        "\n",
        "  load_checkpoint(destination_folder+str(amostra) + '/model.pt', best_model)\n",
        "\n",
        "  evaluate(best_model, token_test[amostra-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1koq2k7jrIj"
      },
      "source": [
        "#!rm ./bertimbau_all_categ/amostra_4/*.*\n",
        "#!du -sh\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJrH6W-UEXCK"
      },
      "source": [
        ""
      ]
    }
  ]
}