{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clusterizacaoNoticias",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQDsHbFVY0pRMpfE1pdJmc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HedersonSantos/Noticias/blob/main/clusterizacaoNoticias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6ntCB_ZoYIR"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk, re\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATJaqnNUqr_L"
      },
      "source": [
        "# Obtendo dataset de noticias tratadas em preProcessamento_noticias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnmIRw0WowrK",
        "outputId": "8dd13eb5-ec15-4329-fd60-e662a079060d"
      },
      "source": [
        "!rm news.*\n",
        "!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/news.zip\n",
        "!unzip news.zip\n",
        "%ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'news.*': No such file or directory\n",
            "--2021-06-19 15:38:59--  https://raw.githubusercontent.com/HedersonSantos/Noticias/main/news.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11333903 (11M) [application/zip]\n",
            "Saving to: ‘news.zip’\n",
            "\n",
            "news.zip            100%[===================>]  10.81M  53.0MB/s    in 0.2s    \n",
            "\n",
            "2021-06-19 15:39:00 (53.0 MB/s) - ‘news.zip’ saved [11333903/11333903]\n",
            "\n",
            "Archive:  news.zip\n",
            "  inflating: news.csv                \n",
            "news.csv  news.zip  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWX1z5Ntq-Ec"
      },
      "source": [
        "# Funções para processamento de Linguagem Natural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmvLl_wTrFR9"
      },
      "source": [
        "def download_pt_stopWords():\n",
        "\n",
        "  '''download das stopwords '''\n",
        "  nltk.download('stopwords') #stopwords\n",
        "  nltk.download('rslp')  #stemming\n",
        "        \n",
        "def removeStopWords( texto, excluirWords:list=None):\n",
        "  '''remove as stopwords do texto. Novas stopwords podem ser adicionadas através da lista excluirWords'''\n",
        "  naoQueridas = nltk.corpus.stopwords.words('portuguese')\n",
        "  if not excluirWords==None:\n",
        "    naoQueridas.extend(excluirWords)\n",
        "  naoQueridas = list(set(naoQueridas))\n",
        "  palavras = [i for i in texto.split() if not i.lower() in naoQueridas]\n",
        "  return (\" \".join(palavras))\n",
        "\n",
        "def aplicaStemming( texto):\n",
        "  ''' obtém o radical das palavras do vocabulário'''\n",
        "  stemmer = nltk.stem.RSLPStemmer()\n",
        "  palavras = []\n",
        "  for w in texto.split():\n",
        "      palavras.append(stemmer.stem(w))\n",
        "  return (\" \".join(palavras))\n",
        "\n",
        "def removeCaracteresNaoDesejados(texto):\n",
        "  textoLimpo = re.sub(r\"http\\S+\", \"\", texto)\n",
        "  textoLimpo = re.sub(r\"www\\..+\\..+\", \"\", texto)\n",
        "  textoLimpo = re.sub(r\"[^a-zA-ZáÁéÉíÍóÓúÚãÃàÀôâÂêÊôÔçÇ!,:.; ]\", \"\", texto)\n",
        "  return textoLimpo\n",
        "\n",
        "def retornaVetorizacao(X,pct_min=1, pct_max=1, excluirSW:list=None):\n",
        "  ''' monta a matriz sparsa com o índice de vocabulário em cada texto. \n",
        "    Retorna a matriz sparsa e o vocabulário '''\n",
        "  count_vect = CountVectorizer(min_df=pct_min, max_df=pct_max, lowercase=True,stop_words=excluirSW) \n",
        "  matriz_sparsa = count_vect.fit_transform(X)\n",
        "  vocabulario = count_vect.fit(X)\n",
        "  return [matriz_sparsa,count_vect]\n",
        "\n",
        "def retornaMatriztfIdf( V):\n",
        "    ''' em cada documento, calcula o tf-idf de cada palavra\n",
        "        term frequency - inverse document frequency'''\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "    matriz_tfidf = tfidf_transformer.fit_transform(V)\n",
        "    return matriz_tfidf\n",
        "\n",
        "def normalizaEreduzDimensionalidadecomPCA( X, nro_dimensao,UT=None):\n",
        "    X_norm = StandardScaler(with_mean=False).fit_transform(X)\n",
        "    data_pca= TruncatedSVD(nro_dimensao)\n",
        "    if UT==None:\n",
        "        UT = data_pca.fit(X_norm)\n",
        "    X_pca =  UT.fit_transform(X_norm)\n",
        "      \n",
        "    return [UT,X_pca]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Sln_Vau32S"
      },
      "source": [
        "# Prepara dataset para clusterizacao"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "2StJlfb_vCMx",
        "outputId": "d4302586-f2a4-43dd-f397-c9860423e988"
      },
      "source": [
        "#abrir arquivo news.csv\n",
        "dfDados = pd.read_csv('news.csv')\n",
        "dfDados['TEXT_TRATADO'] = dfDados['TEXT_TRATADO'].astype('unicode')\n",
        "dfDados['TEXT_TRATADO'] = dfDados.loc[:,['TEXT_TRATADO']].apply(lambda x: removeCaracteresNaoDesejados(x['TEXT_TRATADO']),axis=1)\n",
        "download_pt_stopWords()\n",
        "dfDados['TEXT_TRATADO'] = dfDados.loc[:,['TEXT_TRATADO']].apply(lambda x: removeStopWords(x['TEXT_TRATADO']),axis=1)\n",
        "dfDados['TEXT_TRATADO'] = dfDados.loc[:,['TEXT_TRATADO']].apply(lambda x: aplicaStemming(x['TEXT_TRATADO']),axis=1)\n",
        "dfDados[['TEXT_TRATADO']].head(3)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT_TRATADO</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sext bbb tv glob broth receb com beb faz aquec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>program viv bbb tv globo, leifert convers empa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>continu dizendo, deu tir daquel cadeira. vai s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        TEXT_TRATADO\n",
              "0  sext bbb tv glob broth receb com beb faz aquec...\n",
              "1  program viv bbb tv globo, leifert convers empa...\n",
              "2  continu dizendo, deu tir daquel cadeira. vai s..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k3HMohJ3Dv9",
        "outputId": "55f3ff1d-36e0-45bd-afdb-e75ba4311550"
      },
      "source": [
        "X = dfDados['TEXT_TRATADO'].values\n",
        "min_fr=0.01; max_fr=0.7\n",
        "vetores = retornaVetorizacao(X,min_fr,max_fr) \n",
        "V = vetores[0]\n",
        "X_tfidf = retornaMatriztfIdf(V)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x3352 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 171 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7kB-m9x5GWS",
        "outputId": "8fc3a74e-7991-4bc1-bd4a-34c8b65cace8"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}