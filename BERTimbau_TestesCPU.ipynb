{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTimbau_Testes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKMj5WG6lxGP",
        "outputId": "46682c4b-522d-437a-9bb1-811bc130f9e4"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIlZbdsGo_vR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0b6a30-ca00-4c27-c670-1d9cae98f860"
      },
      "source": [
        "\n",
        "!pip install torch==1.8.1 torchvision==0.9.1 torchaudio==0.8.\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.9.1 in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: torchaudio==0.8. in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hts6ewoYvUYn"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Preliminaries\n",
        "\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "# Models\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AutoTokenizer  # Or BertTokenizer\n",
        "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
        "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
        "\n",
        "# Training\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPOdGbGitPHS"
      },
      "source": [
        "### Carregando tokens e vocabulário do BERTimbau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLcgDJZSpmfD",
        "outputId": "c244e29e-c497-4cc8-ff56-89d317d6f37e"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joJqFofvtKcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a94a02-8b09-4f02-c943-6b24852b06d3"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizerFast(name_or_path='neuralmind/bert-base-portuguese-cased', vocab_size=29794, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZOL2o_ltyzz"
      },
      "source": [
        "### Preparando DataSet\n",
        "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFtrA_dtunW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd7594f-82f2-4e5a-d866-7b2a78af678e"
      },
      "source": [
        "!rm train*.*\n",
        "!rm test*.*\n",
        "!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/train.csv\n",
        "!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/valid.csv\n",
        "!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/test.csv"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-05 14:06:28--  https://raw.githubusercontent.com/HedersonSantos/Noticias/main/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6105595 (5.8M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   5.82M  21.6MB/s    in 0.3s    \n",
            "\n",
            "2021-06-05 14:06:29 (21.6 MB/s) - ‘train.csv’ saved [6105595/6105595]\n",
            "\n",
            "--2021-06-05 14:06:29--  https://raw.githubusercontent.com/HedersonSantos/Noticias/main/valid.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 763946 (746K) [text/plain]\n",
            "Saving to: ‘valid.csv.2’\n",
            "\n",
            "valid.csv.2         100%[===================>] 746.04K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-06-05 14:06:29 (16.5 MB/s) - ‘valid.csv.2’ saved [763946/763946]\n",
            "\n",
            "--2021-06-05 14:06:29--  https://raw.githubusercontent.com/HedersonSantos/Noticias/main/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 720607 (704K) [text/plain]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>] 703.72K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-06-05 14:06:30 (14.3 MB/s) - ‘test.csv’ saved [720607/720607]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV8T4nMYXqdR",
        "outputId": "95cf752d-e7b5-4b7a-a72f-dc1d23a94bb2"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sjrTvfBLGCL"
      },
      "source": [
        "source_folder = '/content' #'/content'\n",
        "destination_folder = '/content' #'/content'\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90WJxOxcxgAQ",
        "outputId": "6052de6d-e035-41a4-f9e0-670698bb944c"
      },
      "source": [
        "!ls /content"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  test.csv  train.csv  valid.csv  valid.csv.1  valid.csv.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyoK7ac8t4HF"
      },
      "source": [
        "\n",
        "#Model parameter\n",
        "MAX_SEQ_LEN = 128 #limita os artigos em 128 tokens\n",
        "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
        "bs = 16\n",
        "\n",
        "# Fields - use_vocab=False  e tokenizer.encode permite que utilizemos os tokens do BERTimbau.\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
        "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
        "fields = [('text', text_field),('label', label_field)]\n",
        "\n",
        "# TabularDataset\n",
        "train, valid, test = TabularDataset.splits(path=source_folder, train='train.csv', validation='valid.csv',\n",
        "                                           test='test.csv', format='CSV', fields=fields, skip_header=True)\n",
        "\n",
        "# Iterators\n",
        "\n",
        "train_iter = BucketIterator(train, batch_size=bs, sort_key=lambda x: len(x.text),\n",
        "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
        "valid_iter = BucketIterator(valid, batch_size=bs, sort_key=lambda x: len(x.text),\n",
        "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
        "test_iter = Iterator(test, batch_size=bs, device=device, train=False, shuffle=False, sort=False)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcTHSXSluKkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a6d0b9-ee5b-4a3c-9597-654799a66e10"
      },
      "source": [
        "print(vars(train[0]))\n",
        "print(vars(valid[0]))\n",
        "print(vars(test[0]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': [101, 8298, 4089, 22360, 22327, 5234, 1379, 231, 1640, 21523, 20354, 716, 157, 1191, 222, 17611, 125, 18015, 240, 325, 125, 230, 5314, 229, 6293, 2166, 11388, 117, 3616, 366, 13914, 22281, 119, 231, 1640, 7493, 11621, 2406, 122, 117, 834, 12876, 117, 262, 7104, 2123, 11781, 207, 625, 4825, 320, 4976, 180, 21367, 1516, 285, 119, 8298, 4089, 22360, 22327, 5234, 1379, 231, 1640, 21523, 20354, 716, 157, 1191, 222, 17611, 125, 18015, 240, 325, 125, 230, 5314, 229, 6293, 2166, 11388, 117, 3616, 366, 13914, 22281, 119, 231, 1640, 7493, 11621, 2406, 122, 117, 834, 12876, 117, 262, 7104, 2123, 11781, 207, 625, 4825, 320, 4976, 180, 21367, 1516, 285, 119, 6312, 11781, 207, 117, 20354, 716, 157, 1996, 179, 146, 17611, 125, 18015, 11588, 176, 20206, 173, 1028, 2221, 117, 122, 19398, 710, 1033, 117, 1007, 125, 1455, 122, 6531, 7069, 119, 2195, 146, 1640, 117, 146, 5291, 262, 230, 13950, 125, 3165, 353, 3097, 1542, 119, 2010, 3758, 17611, 5863, 1790, 117, 170, 1719, 123, 11304, 11020, 8431, 117, 925, 5835, 221, 710, 1033, 117, 1007, 125, 1455, 122, 241, 22340, 113, 6531, 7069, 114, 119, 989, 230, 13950, 346, 1837, 117, 253, 230, 13950, 125, 3165, 353, 3097, 1542, 125, 944, 5022, 179, 16003, 4527, 117, 20885, 292, 122, 4676, 3364, 125, 2745, 2010, 4492, 146, 1640, 117, 179, 1021, 908, 5377, 412, 11121, 712, 9247, 382, 125, 1112, 6030, 22354, 122, 1112, 2779, 1368, 4815, 22354, 119, 231, 1640, 3539, 4227, 4820, 529, 6764, 3765, 179, 5357, 22303, 12950, 1030, 221, 8364, 180, 4162, 179, 368, 996, 230, 17507, 180, 5173, 122, 171, 2368, 125, 925, 122, 2477, 119, 409, 173, 2427, 170, 17448, 117, 4125, 22281, 122, 9564, 4846, 2528, 113, 13196, 22324, 114, 117, 240, 2318, 366, 5772, 125, 18976, 125, 10648, 182, 240, 2318, 180, 9196, 769, 151, 117, 20354, 716, 157, 770, 4492, 179, 1981, 12274, 222, 6636, 221, 8364, 180, 4162, 119, 3978, 16959, 214, 123, 9196, 769, 151, 117, 20354, 716, 157, 407, 2927, 123, 5961, 179, 146, 3310, 346, 5733, 1000, 5207, 221, 4314, 123, 1521, 173, 1105, 131, 2010, 7165, 2158, 128, 3350, 1497, 9748, 202, 622, 3714, 117, 3233, 179, 7749, 11690, 117, 123, 9196, 769, 151, 119, 1645, 117, 712, 3885, 117, 19821, 5805, 22280, 119, 11421, 370, 11304, 131, 271, 3795, 21688, 366, 9686, 14064, 117, 9871, 146, 7343, 2396, 529, 5207, 221, 2385, 22325, 118, 15887, 1839, 125, 1105, 119, 1683, 3530, 123, 3153, 1378, 13878, 15659, 16682, 113, 5374, 22327, 118, 250, 22324, 114, 117, 1640, 180, 4680, 125, 5173, 122, 4795, 180, 3159, 117, 20354, 716, 157, 123, 7010, 11527, 12294, 125, 1112, 123, 2196, 171, 7509, 15511, 1050, 22354, 117, 173, 3562, 353, 4562, 125, 173, 3870, 22278, 353, 5173, 113, 212, 5476, 114, 179, 16066, 146, 1700, 125, 8621, 379, 966, 10234, 221, 7940, 307, 146, 2453, 366, 3496, 119, 1379, 761, 1719, 11304, 5220, 3550, 823, 793, 1257, 202, 9503, 122, 4832, 128, 1141, 230, 2821, 125, 15511, 159, 259, 3114, 240, 5006, 180, 4451, 125, 2336, 119, 17264, 296, 1977, 5076, 117, 449, 229, 11304, 117, 122, 346, 229, 16322, 426, 180, 19972, 1379, 4492, 146, 1640, 119, 192, 14000, 131, 1379, 2542, 7105, 18689, 307, 1257, 117, 2113, 146, 7509, 253, 15198, 180, 10544, 122, 327, 13433, 1981, 333, 125, 3482, 125, 944, 122, 15511, 251, 117, 221, 179, 5034, 1977, 3983, 22281, 240, 15256, 7762, 684, 123, 8671, 202, 3753, 259, 1782, 22279, 119, 15659, 16682, 117, 179, 14009, 146, 16439, 179, 16146, 203, 240, 6191, 125, 18015, 3704, 171, 1640, 117, 253, 12504, 180, 212, 5476, 179, 18237, 146, 7509, 16085, 119, 231, 4054, 770, 1023, 123, 12238, 22281, 12569, 1758, 10999, 412, 17374, 22333, 744, 173, 13625, 119, 503, 2338, 2767, 117, 146, 1640, 180, 3159, 117, 8487, 10159, 124, 113, 15166, 118, 14215, 114, 117, 3127, 123, 2203, 125, 230, 6979, 1199, 221, 13958, 146, 1778, 119, 231, 3721, 117, 222, 1955, 1772, 131, 19687, 455, 240, 1839, 180, 5362, 171, 1955, 325, 20808, 171, 771, 533, 9314, 453, 125, 5296, 1758, 15615, 125, 532, 4761, 122, 346, 8734, 123, 6819, 2166, 3834, 119, 530, 16512, 3233, 179, 6430, 259, 3401, 125, 1700, 117, 5759, 1702, 22279, 119, 2502, 22278, 260, 14179, 325, 11845, 221, 4945, 146, 179, 253, 525, 18823, 342, 291, 11443, 100, 7314, 118, 14979, 22302, 119, 4859, 3015, 22121, 22281, 123, 9087, 3721, 200, 120, 177, 119, 1681, 3028, 346, 706, 333, 3565, 117, 9410, 240, 11934, 142, 18626, 117, 21141, 4396, 291, 184, 16364, 979, 1287, 834, 10910, 119, 102], 'label': '8'}\n",
            "{'text': [101, 21791, 183, 173, 179, 146, 1368, 2064, 122, 10591, 675, 5365, 122, 11085, 117, 123, 1018, 180, 6420, 125, 8446, 122, 2391, 119, 21791, 183, 173, 179, 146, 1368, 2064, 122, 10591, 675, 5365, 122, 11085, 117, 123, 1018, 180, 6420, 125, 8446, 122, 2391, 119, 16528, 326, 15167, 15226, 1351, 123, 1742, 173, 3500, 122, 3256, 170, 2200, 1065, 3190, 119, 2781, 12206, 22288, 2156, 117, 11470, 251, 117, 2694, 123, 13444, 171, 9627, 7221, 113, 20484, 2054, 22283, 2790, 114, 122, 2473, 4142, 125, 16463, 22280, 171, 622, 180, 1980, 22279, 7760, 173, 13625, 119, 1424, 19206, 8849, 171, 359, 7918, 2506, 120, 16720, 120, 14979, 22302, 997, 22296, 6773, 17820, 1088, 2530, 173, 2506, 120, 16720, 120, 14979, 22302, 997, 22296, 11800, 177, 18824, 113, 10385, 3597, 125, 3733, 114, 7716, 203, 1790, 113, 2506, 114, 259, 6543, 12568, 180, 2156, 171, 771, 119, 231, 3674, 262, 123, 20026, 399, 125, 260, 4102, 4242, 7201, 210, 2375, 123, 2375, 7293, 122, 7041, 117, 222, 298, 8900, 325, 2478, 171, 2200, 171, 7290, 119, 1629, 1659, 117, 146, 8283, 1023, 146, 3673, 159, 125, 333, 7716, 201, 221, 6331, 146, 5602, 18990, 7808, 18611, 117, 179, 706, 333, 2304, 222, 298, 11963, 325, 11456, 171, 3806, 245, 117, 3949, 660, 222, 298, 3885, 2166, 939, 179, 3623, 123, 3007, 125, 8062, 171, 19558, 119, 231, 8760, 2541, 18594, 22282, 146, 1553, 5137, 581, 122, 117, 2440, 125, 1719, 123, 3948, 180, 1478, 171, 5142, 117, 706, 333, 2304, 15192, 14972, 117, 1016, 271, 202, 1652, 171, 2771, 117, 179, 2541, 6331, 146, 5483, 119, 231, 8202, 1023, 123, 7716, 125, 333, 1074, 9783, 125, 230, 4036, 122, 2541, 6331, 146, 7723, 2868, 173, 222, 5885, 6570, 119, 231, 2995, 5710, 179, 5197, 171, 1477, 221, 5152, 117, 449, 117, 1839, 366, 12522, 117, 123, 4036, 548, 146, 5142, 221, 18594, 22282, 146, 15886, 18505, 185, 346, 253, 366, 325, 16535, 4079, 119, 710, 1033, 117, 9801, 122, 7802, 118, 13293, 4882, 18594, 22282, 11963, 325, 20843, 122, 407, 13888, 173, 2244, 271, 15192, 20561, 119, 177, 4162, 253, 179, 1061, 6331, 150, 230, 4036, 222, 1695, 325, 16535, 1383, 221, 259, 1960, 598, 678, 125, 1618, 113, 12375, 114, 117, 14854, 22318, 113, 14969, 114, 122, 16664, 113, 7980, 114, 117, 4309, 119, 335, 4932, 4204, 117, 146, 8838, 18594, 146, 7802, 118, 278, 22317, 173, 222, 9898, 179, 407, 346, 2541, 333, 298, 325, 3242, 9124, 119, 177, 1478, 125, 710, 1033, 376, 325, 3948, 202, 5391, 117, 449, 346, 2064, 2200, 146, 4974, 221, 333, 5727, 271, 14972, 3102, 8391, 119, 115, 115, 1681, 4054, 346, 14398, 117, 11223, 117, 123, 6819, 171, 359, 7918, 102], 'label': '6'}\n",
            "{'text': [101, 566, 5602, 341, 171, 2510, 2010, 1007, 125, 1455, 566, 5602, 341, 171, 2510, 2010, 1007, 125, 1455, 16956, 120, 18838, 120, 14979, 22302, 2506, 22296, 7375, 13103, 7370, 19148, 120, 18838, 120, 14979, 22302, 177, 1024, 155, 1309, 14236, 123, 6858, 4580, 4419, 180, 2156, 2094, 125, 14979, 22302, 229, 2954, 1014, 5044, 118, 14258, 119, 231, 1130, 125, 3903, 253, 771, 3043, 9596, 117, 202, 644, 1492, 125, 1592, 117, 1000, 542, 22296, 117, 202, 1470, 22300, 3272, 1471, 2034, 117, 173, 6191, 119, 1603, 117, 123, 5448, 11084, 8288, 122, 9346, 202, 17453, 22290, 897, 2995, 117, 202, 1007, 125, 1455, 117, 122, 4434, 123, 681, 2171, 598, 146, 14020, 117, 202, 14584, 117, 173, 14423, 119, 231, 17613, 2541, 3859, 820, 123, 774, 117, 202, 644, 1193, 125, 1618, 119, 177, 3007, 298, 2201, 122, 146, 4106, 180, 3109, 346, 506, 12225, 22281, 119, 177, 4389, 117, 125, 17601, 178, 9541, 22283, 117, 3035, 598, 146, 6703, 117, 202, 644, 1426, 117, 202, 17453, 22290, 897, 2995, 119, 1603, 659, 146, 5885, 170, 146, 7964, 117, 202, 1470, 22300, 3272, 1471, 2034, 117, 644, 542, 117, 7279, 173, 6191, 221, 14438, 146, 8538, 122, 4434, 123, 681, 2171, 598, 123, 12426, 117, 229, 11341, 14789, 9178, 117, 173, 20398, 119, 116, 354, 22309, 22333, 22301, 177, 267, 11880, 9008, 22301, 250, 22301, 6213, 5869, 9313, 22352, 21748, 4529, 20415, 3168, 22327, 22323, 22351, 11836, 22301, 1475, 5829, 180, 2156, 2094, 173, 2670, 2010, 783, 183, 131, 482, 15538, 120, 1024, 155, 1309, 1024, 155, 1309, 1331, 179, 5710, 107, 11561, 9894, 107, 122, 3196, 4777, 353, 2156, 2094, 125, 107, 10349, 8180, 1050, 107, 761, 6648, 171, 17613, 117, 179, 15913, 647, 820, 123, 3087, 117, 20398, 253, 123, 2496, 179, 2541, 3859, 1528, 1960, 117, 1685, 119, 231, 1470, 22300, 3272, 1471, 2034, 2541, 3859, 3003, 4814, 117, 122, 146, 14584, 117, 173, 14423, 117, 122, 146, 17453, 22290, 897, 2995, 117, 202, 1007, 125, 1455, 117, 14703, 150, 1118, 4814, 1078, 119, 530, 4412, 173, 652, 202, 347, 939, 117, 146, 771, 659, 260, 10020, 125, 774, 117, 10622, 122, 774, 202, 1007, 125, 1455, 117, 1652, 1938, 3440, 548, 123, 3087, 119, 21799, 259, 1960, 240, 2496, 131, 6191, 113, 1470, 22300, 3272, 1471, 2034, 114, 771, 3043, 9596, 117, 1492, 120, 18838, 117, 542, 22296, 4422, 3865, 7964, 117, 542, 120, 18838, 117, 2250, 22296, 4422, 3865, 3043, 8538, 117, 2250, 120, 18838, 117, 2250, 22296, 5224, 634, 3043, 8538, 117, 2235, 120, 18838, 117, 2250, 22296, 21701, 514, 9437, 747, 3043, 8288, 117, 2680, 120, 18838, 117, 542, 22296, 22313, 22301, 3043, 511, 22318, 117, 10020, 125, 774, 117, 19148, 120, 18506, 117, 275, 22296, 22328, 8510, 3043, 278, 11800, 117, 10622, 117, 18838, 120, 18506, 117, 2336, 22296, 4402, 16682, 150, 171, 2402, 1247, 117, 17791, 120, 18506, 117, 2250, 22296, 1007, 125, 1455, 113, 17453, 22290, 897, 2995, 114, 4389, 3043, 6703, 117, 1426, 120, 18838, 117, 542, 22296, 13751, 3043, 8288, 117, 1040, 120, 18838, 117, 2250, 22296, 21701, 514, 9437, 747, 3043, 14020, 117, 297, 120, 18838, 117, 542, 22296, 771, 3043, 9346, 117, 2506, 120, 18838, 117, 2250, 22296, 22341, 381, 4894, 3043, 8538, 117, 2593, 120, 18838, 117, 2250, 22296, 22302, 22318, 3043, 678, 22301, 117, 10020, 125, 774, 117, 16956, 120, 18506, 117, 2250, 22296, 22328, 9100, 3043, 278, 11058, 117, 10622, 117, 16899, 120, 18506, 117, 297, 22296, 20398, 113, 11341, 14789, 9178, 114, 115, 9346, 3043, 14020, 117, 1492, 120, 18838, 117, 297, 22296, 5224, 634, 3043, 12426, 117, 542, 120, 18838, 117, 1040, 22296, 22341, 381, 4894, 3043, 6703, 117, 2250, 120, 18838, 117, 1040, 22296, 7132, 19868, 619, 3043, 7964, 117, 2235, 120, 18838, 117, 1040, 22296, 7132, 19868, 619, 3043, 4389, 117, 2593, 120, 18838, 117, 297, 22296, 115, 3956, 202, 7356, 1238, 125, 20398, 14423, 113, 14584, 114, 8538, 3043, 12426, 117, 1426, 120, 18838, 117, 2250, 22296, 11882, 8961, 3043, 9596, 117, 1040, 120, 18838, 117, 542, 22296, 11882, 8961, 3043, 8288, 117, 297, 120, 18838, 117, 2250, 22296, 22309, 6513, 141, 3043, 8288, 117, 2506, 120, 18838, 117, 542, 22296, 13751, 3043, 14020, 117, 2680, 120, 18838, 117, 542, 22296, 22313, 22318, 3043, 511, 22301, 117, 10020, 125, 774, 117, 16956, 120, 18506, 117, 542, 22296, 22302, 22301, 3043, 678, 22318, 117, 10020, 125, 774, 117, 19148, 120, 18506, 117, 2336, 22296, 1007, 125, 1455, 113, 17613, 114, 5107, 117, 1193, 120, 18506, 117, 2250, 22296, 510, 6764, 7243, 171, 2510, 4188, 202, 347, 122, 118, 223, 215, 9172, 6943, 243, 106, 11976, 4364, 125, 176, 601, 18930, 229, 872, 3144, 4362, 367, 171, 2510, 119, 21799, 407, 102], 'label': '6'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPypCNPuHaRz"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvy3rgrdHj_x"
      },
      "source": [
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        options_name = \"bert-base-portuguese-cased\"\n",
        "        self.encoder = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "    def forward(self, text, label):\n",
        "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
        "\n",
        "        return loss, text_fea\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pYKbHICIus0"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJFwyl7DIoxQ"
      },
      "source": [
        "# Save and Load Functions\n",
        "\n",
        "def save_checkpoint(save_path, model, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(load_path, model):\n",
        "    \n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiueExF5Jb8V"
      },
      "source": [
        "# Training Function\n",
        "'''criterion = nn.BCELoss() é BinaryCrossEntropy é a função de perda para targets binarios. Como o nosso alvo possui\n",
        "muitas classes devemos rocar a função de perda. '''\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          criterion = nn.BCELoss(),\n",
        "          train_loader = train_iter,\n",
        "          valid_loader = valid_iter,\n",
        "          num_epochs = 5,\n",
        "          eval_every = len(train_iter) // 2,\n",
        "          file_path = destination_folder,\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "    \n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    # training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for (text, labels), _ in train_loader:\n",
        "            labels = labels.type(torch.LongTensor)           \n",
        "            print('label size:', labels.size())\n",
        "            print('label:', labels)\n",
        "            labels = labels.to(device)\n",
        "            text = text.type(torch.LongTensor) \n",
        "            print('text:', text.size())\n",
        "            text = text.to(device)\n",
        "            print('treina...')\n",
        "            output = model(text, labels)\n",
        "            print('fim treino...')\n",
        "            loss, _ = output\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "\n",
        "                    # validation loop\n",
        "                    for (text,labels), _ in valid_loader:\n",
        "                        text = text.type(torch.LongTensor)  \n",
        "                        text = text.to(device)\n",
        "                        labels = labels.type(torch.LongTensor)           \n",
        "                        labels = labels.to(device)\n",
        "                        \n",
        "                        output = model(titletext, labels)\n",
        "                        loss, _ = output\n",
        "                        \n",
        "                        valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
        "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    \n",
        "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQcJQ5kaMPa4",
        "outputId": "e2fb5cd2-1f74-4456-ffb3-9607892c4f8b"
      },
      "source": [
        "device"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "RrkvwE2VL1qY",
        "outputId": "5b60d11f-9bdc-4860-f2ef-cab32a0e2f3a"
      },
      "source": [
        "model = BERT().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "train(model=model, optimizer=optimizer)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "label size: torch.Size([16])\n",
            "label: tensor([1, 6, 1, 7, 7, 6, 0, 6, 2, 7, 7, 1, 7, 1, 8, 8])\n",
            "text: torch.Size([16, 128])\n",
            "treina...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e4474bff9c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-bd0cf4b1fcf0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_loader, valid_loader, num_epochs, eval_every, file_path, best_valid_loss)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'treina...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fim treino...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-8d7f357efb19>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, label)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_fea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_fea\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1535\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1048\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2386\u001b[0m         )\n\u001b[1;32m   2387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 6 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sGfCMMB9ewJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJrH6W-UEXCK"
      },
      "source": [
        ""
      ]
    }
  ]
}