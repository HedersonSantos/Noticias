{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTimbau_Testes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcC3GCvxdwyThJGV1FnYvB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HedersonSantos/Noticias/blob/main/BERTimbau_Testes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKMj5WG6lxGP",
        "outputId": "fc6664f7-f625-4b88-c9dd-006dbf18089c"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIlZbdsGo_vR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a8643c-695f-464a-c04d-6059654e6a92"
      },
      "source": [
        "\n",
        "!pip install torch==1.8.1 torchvision==0.9.1 torchaudio==0.8.\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.9.1 in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: torchaudio==0.8. in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hts6ewoYvUYn"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Preliminaries\n",
        "\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "# Models\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AutoTokenizer  # Or BertTokenizer\n",
        "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
        "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
        "\n",
        "# Training\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPOdGbGitPHS"
      },
      "source": [
        "### Carregando tokens e vocabulário do BERTimbau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLcgDJZSpmfD",
        "outputId": "d6d6aa23-a4ce-4c3e-f13f-6643610f7b8e"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joJqFofvtKcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7215bbea-1150-4a66-bd6a-d6544e06d6ff"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizerFast(name_or_path='neuralmind/bert-base-portuguese-cased', vocab_size=29794, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZOL2o_ltyzz"
      },
      "source": [
        "### Preparando DataSet\n",
        "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFtrA_dtunW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d32e0556-2429-4920-db3f-c4652bb7c0ad"
      },
      "source": [
        "!rm train*.*\n",
        "!rm test*.*\n",
        "!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/train.csv\n",
        "!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/test.csv"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-04 21:36:13--  https://raw.githubusercontent.com/HedersonSantos/Noticias/main/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5687298 (5.4M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   5.42M  12.6MB/s    in 0.4s    \n",
            "\n",
            "2021-06-04 21:36:14 (12.6 MB/s) - ‘train.csv’ saved [5687298/5687298]\n",
            "\n",
            "--2021-06-04 21:36:14--  https://raw.githubusercontent.com/HedersonSantos/Noticias/main/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 713944 (697K) [text/plain]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>] 697.21K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-06-04 21:36:14 (6.11 MB/s) - ‘test.csv’ saved [713944/713944]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV8T4nMYXqdR",
        "outputId": "013eca9d-76a2-4ddb-b689-d89a9dc63a72"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sjrTvfBLGCL"
      },
      "source": [
        "source_folder = '/content' #'/content/drive/My Drive/nlpPreProcessamento'\n",
        "destination_folder = '/content' #'/content/drive/My Drive/nlpPreProcessamento'\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90WJxOxcxgAQ",
        "outputId": "27a40c65-1bc0-4508-c033-ab8287d16561",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls /content"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  test.csv  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyoK7ac8t4HF"
      },
      "source": [
        "\n",
        "#Model parameter\n",
        "MAX_SEQ_LEN = 128\n",
        "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
        "\n",
        "# Fields\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
        "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
        "fields = [('text', text_field),('label', label_field)]\n",
        "\n",
        "# TabularDataset\n",
        "train,  test = TabularDataset.splits(path=source_folder, train='train.csv', \n",
        "                                           test='test.csv', format='CSV', fields=fields, skip_header=True)\n",
        "\n",
        "# Iterators\n",
        "\n",
        "train_iter = BucketIterator(train, batch_size=4, sort_key=lambda x: len(x.text),\n",
        "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
        "test_iter = Iterator(test, batch_size=4, device=device, train=False, shuffle=False, sort=False)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcTHSXSluKkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31065dd9-255d-47d1-fda5-e89d77d6b3e3"
      },
      "source": [
        "print(vars(train[0]))\n",
        "print(vars(test[0]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': [101, 13647, 240, 3426, 6071, 13647, 240, 3426, 6071, 16366, 789, 654, 259, 682, 1775, 173, 347, 6789, 6163, 203, 412, 100, 576, 320, 4855, 298, 3585, 14488, 3346, 303, 22279, 2939, 119, 472, 22282, 119, 14979, 22302, 113, 6151, 118, 14258, 114, 118, 2297, 22296, 2051, 231, 1640, 298, 1134, 1195, 117, 6991, 10912, 2346, 117, 2002, 222, 14130, 353, 3278, 122, 353, 3787, 117, 245, 125, 532, 2202, 11963, 117, 726, 146, 16506, 6789, 320, 4855, 3660, 100, 14258, 113, 2593, 119, 472, 22282, 119, 14979, 22302, 114, 119, 231, 6647, 154, 16735, 123, 3278, 125, 3995, 6901, 5201, 17579, 22279, 441, 122, 1996, 179, 260, 3680, 171, 1640, 6307, 117, 14247, 4055, 1245, 117, 14479, 10755, 119, 1112, 15964, 244, 785, 6054, 221, 146, 1640, 4055, 1245, 179, 117, 1853, 346, 2231, 20346, 128, 230, 17902, 117, 675, 3680, 1854, 10755, 22354, 117, 3415, 146, 6647, 154, 119, 177, 8030, 3539, 229, 860, 364, 180, 6529, 426, 125, 2903, 315, 9894, 298, 1134, 1195, 353, 3787, 700, 179, 222, 7399, 180, 8224, 298, 3585, 17636, 123, 15923, 9020, 529, 3496, 1410, 118, 11190, 125, 14979, 22307, 122, 4642, 13747, 604, 22285, 5051, 17547, 123, 9117, 119, 4055, 1245, 9396, 353, 3659, 11407, 348, 19678, 22281, 1410, 118, 6229, 119, 1112, 3396, 3989, 10773, 125, 547, 6750, 122, 16602, 353, 15923, 180, 3787, 173, 16594, 3496, 122, 4642, 13747, 604, 22285, 5051, 173, 7275, 1161, 122, 2786, 1379, 122, 1061, 4112, 260, 924, 4486, 122, 2779, 3989, 10773, 22354, 117, 1996, 10912, 2346, 177, 1712, 420, 259, 245, 5335, 376, 176, 1048, 155, 1859, 119, 335, 1570, 117, 6991, 10912, 2346, 1996, 179, 146, 461, 684, 3552, 253, 222, 1112, 12961, 22354, 119, 4055, 1245, 9396, 122, 11234, 179, 10912, 2346, 1112, 346, 3189, 7730, 260, 3454, 22354, 170, 123, 3787, 119, 3703, 3056, 117, 146, 1640, 1410, 118, 3725, 7276, 4275, 22288, 221, 146, 6307, 122, 9233, 230, 6085, 221, 14149, 1112, 1485, 260, 5100, 22354, 7389, 259, 1775, 119, 177, 3278, 407, 262, 6568, 125, 313, 7557, 1970, 591, 119, 6991, 10912, 2346, 3415, 179, 146, 806, 376, 6901, 5201, 17579, 22279, 441, 119, 1112, 533, 1134, 1195, 6331, 150, 6901, 5201, 18009, 4757, 22281, 179, 10144, 228, 259, 5684, 122, 260, 9132, 11190, 117, 271, 1237, 10724, 221, 2786, 21225, 122, 146, 18144, 125, 9041, 122, 3617, 9472, 11190, 22354, 119, 231, 6647, 154, 3415, 179, 1996, 320, 1640, 180, 3278, 117, 767, 22283, 15207, 19152, 117, 179, 259, 3585, 6209, 150, 259, 3015, 3773, 122, 123, 10544, 119, 1112, 177, 2094, 346, 2541, 20276, 125, 7275, 11798, 170, 259, 3015, 3773, 122, 260, 4676, 22281, 10171, 119, 19403, 1640, 1410, 118, 3725, 2806, 706, 9464, 173, 17158, 625, 259, 3015, 3773, 13508, 453, 14643, 442, 119, 1263, 1640, 376, 179, 7416, 123, 15198, 171, 7275, 806, 22354, 117, 1996, 119, 1112, 3396, 407, 1996, 320, 1640, 767, 22283, 179, 3247, 5835, 230, 2124, 2645, 2260, 202, 17408, 118, 9407, 117, 1016, 271, 4366, 128, 170, 123, 17534, 22285, 229, 2092, 1379, 346, 221, 9070, 146, 4367, 449, 221, 18196, 146, 4367, 22354, 117, 1996, 10912, 2346, 119, 11160, 154, 353, 14204, 22279, 522, 171, 6789, 320, 4855, 131, 14488, 3346, 303, 22279, 5602, 4086, 22278, 10508, 122, 118, 223, 215, 221, 14488, 3346, 303, 22279, 137, 5294, 352, 1573, 168, 4991, 303, 22279, 5602, 4086, 22278, 240, 6717, 1066, 9058, 11465, 22307, 240, 9058, 11465, 22307, 240, 21546, 3923, 22278, 240, 14488, 3346, 303, 22279, 240, 9058, 11465, 22307, 240, 9058, 11465, 22307, 240, 1033, 2967, 7846, 240, 21487, 8379, 240, 9058, 11465, 22307, 240, 15401, 8703, 22305, 240, 9058, 11465, 22307, 240, 9058, 11465, 22307, 240, 9058, 11465, 22307, 240, 14488, 3346, 303, 22279, 240, 6717, 1066, 9058, 11465, 22307, 240, 9058, 11465, 22307, 240, 9058, 11465, 22307, 240, 14488, 3346, 303, 22279, 2174, 2215, 215, 296, 13647, 240, 3426, 6071, 20022, 131, 195, 173, 18376, 22290, 2561, 8454, 22284, 197, 959, 10508, 221, 325, 125, 230, 2760, 117, 21333, 823, 259, 14441, 22281, 240, 1666, 22282, 225, 966, 119, 14979, 22302, 100, 4859, 259, 3015, 9058, 11465, 22307, 119, 102], 'label': '3'}\n",
            "{'text': [101, 257, 15749, 118, 22026, 358, 180, 6219, 2528, 122, 3218, 171, 3310, 10000, 3660, 370, 356, 118, 14258, 221, 123, 3167, 125, 2091, 314, 22283, 22332, 117, 202, 2699, 180, 2690, 3224, 2373, 912, 17809, 703, 8588, 117, 790, 4367, 15078, 420, 6742, 7814, 1044, 122, 6518, 119, 177, 4205, 262, 13470, 240, 4137, 8362, 591, 423, 278, 22055, 15114, 119, 231, 8391, 2789, 864, 16392, 4954, 122, 2139, 11337, 117, 420, 1061, 222, 10611, 10889, 229, 3827, 125, 5874, 2622, 117, 125, 1365, 170, 146, 1640, 171, 3101, 125, 6007, 3224, 2373, 912, 17809, 703, 8588, 122, 100, 117, 8378, 2330, 15866, 172, 17809, 703, 8588, 119, 177, 212, 22324, 6540, 5795, 221, 15076, 22282, 146, 1652, 117, 449, 346, 9202, 260, 9181, 548, 146, 2182, 119, 257, 15749, 118, 22026, 358, 180, 6219, 2528, 122, 3218, 171, 3310, 10000, 3660, 370, 356, 118, 14258, 221, 123, 3167, 125, 2091, 314, 22283, 22332, 117, 202, 2699, 180, 2690, 3224, 2373, 912, 17809, 703, 8588, 117, 790, 4367, 15078, 420, 6742, 7814, 1044, 122, 6518, 119, 177, 4205, 262, 13470, 240, 4137, 8362, 591, 423, 278, 22055, 15114, 119, 231, 8391, 2789, 864, 16392, 4954, 122, 2139, 11337, 117, 420, 1061, 222, 10611, 10889, 229, 3827, 125, 5874, 2622, 117, 125, 1365, 170, 146, 1640, 171, 3101, 125, 6007, 3224, 2373, 912, 17809, 703, 8588, 122, 100, 117, 8378, 2330, 15866, 172, 17809, 703, 8588, 119, 177, 212, 22324, 6540, 5795, 221, 15076, 22282, 146, 1652, 117, 449, 346, 9202, 260, 9181, 548, 146, 2182, 119, 5705, 7838, 15056, 22285, 324, 5496, 3027, 958, 720, 412, 5581, 1580, 171, 7429, 247, 113, 11513, 18376, 114, 4915, 150, 3218, 171, 3310, 122, 19616, 180, 212, 22324, 117, 1362, 1437, 125, 997, 7209, 117, 10000, 221, 146, 1238, 202, 1206, 180, 1373, 119, 1297, 8631, 22281, 131, 3272, 7814, 1044, 12374, 22287, 3041, 125, 112, 16593, 112, 598, 4759, 690, 148, 20691, 122, 14936, 173, 320, 1528, 3003, 6785, 4954, 231, 278, 22055, 15114, 305, 18417, 1982, 353, 212, 22324, 179, 222, 6742, 7814, 458, 262, 2533, 3870, 286, 954, 6518, 122, 6751, 412, 11513, 18376, 221, 123, 2296, 1017, 21245, 180, 6219, 2528, 744, 202, 644, 125, 3185, 2097, 221, 333, 12728, 122, 173, 2590, 262, 15111, 119, 231, 6742, 7814, 458, 3415, 179, 1011, 695, 557, 146, 2187, 8890, 1620, 303, 371, 170, 5831, 320, 6742, 7814, 22280, 625, 262, 5516, 243, 240, 6518, 117, 122, 173, 2590, 117, 1858, 15485, 4169, 170, 346, 6785, 122, 1351, 123, 20949, 119, 20039, 19166, 180, 11513, 18376, 21615, 228, 19993, 2191, 125, 20614, 117, 5153, 2859, 117, 16861, 130, 119, 297, 6898, 117, 8103, 22307, 12288, 122, 1117, 6898, 117, 9389, 5213, 202, 4367, 119, 177, 22317, 278, 22055, 15114, 117, 2330, 15866, 172, 3415, 179, 3851, 3185, 2097, 202, 1238, 171, 4367, 240, 1359, 366, 997, 22296, 6773, 122, 7539, 1982, 170, 736, 6518, 179, 864, 6742, 7814, 1044, 506, 4954, 122, 736, 1685, 506, 11337, 123, 12785, 122, 16240, 2034, 591, 119, 2010, 2664, 5063, 123, 4270, 229, 3167, 117, 1061, 506, 221, 2920, 221, 1434, 222, 15496, 119, 335, 997, 1564, 117, 253, 123, 2914, 576, 179, 6491, 146, 4367, 229, 3167, 2091, 2949, 22332, 119, 4407, 16682, 3558, 125, 3320, 5657, 11259, 221, 260, 2943, 122, 2459, 119, 787, 1376, 14987, 170, 19504, 125, 2260, 117, 170, 17266, 215, 117, 18595, 2895, 124, 119, 2195, 368, 117, 506, 2533, 3870, 474, 954, 4759, 690, 148, 20691, 179, 4366, 123, 15936, 125, 2499, 229, 3167, 682, 1896, 1653, 9913, 22281, 117, 732, 592, 18023, 125, 8437, 122, 1938, 3632, 17124, 22281, 298, 14750, 119, 2010, 2542, 7496, 2599, 128, 222, 3041, 123, 1569, 2182, 240, 12428, 119, 192, 944, 259, 4918, 2866, 1376, 2533, 3870, 474, 229, 15936, 17809, 703, 8588, 118, 2330, 15866, 172, 119, 14152, 272, 22280, 131, 17809, 703, 8588, 453, 13379, 22281, 240, 6742, 7814, 1044, 173, 20849, 231, 8391, 176, 2002, 625, 117, 240, 1359, 366, 1433, 22296, 3708, 1014, 1448, 118, 14258, 117, 1118, 11575, 125, 6742, 7814, 1044, 3106, 4298, 229, 3167, 122, 7320, 1206, 320, 3041, 598, 259, 6785, 119, 231, 278, 22055, 15114, 305, 18417, 179, 262, 6358, 3249, 123, 7693, 180, 1478, 125, 3231, 180, 2690, 3224, 2373, 912, 107, 221, 398, 10750, 22282, 123, 19032, 4810, 298, 13291, 107, 119, 107, 1292, 285, 123, 11547, 298, 8446, 122, 146, 9538, 20335, 125, 2432, 6617, 117, 346, 2810, 2199, 179, 123, 11513, 18376, 13880, 18982, 6017, 22279, 548, 123, 3167, 221, 13301, 140, 2202, 3476, 834, 179, 9966, 1279, 154, 366, 2376, 125, 3320, 3352, 107, 117, 1331, 16677, 8210, 412, 18082, 22278, 180, 10382, 125, 21126, 192, 6494, 3632, 282, 10359, 22290, 17809, 703, 8588, 180, 11513, 18376, 117, 1660, 18063, 6071, 15752, 178, 119, 14354, 285, 117, 123, 11513, 18376, 3415, 173, 4428, 179, 6082, 146, 1652, 1982, 1000, 2376, 8785, 122, 19316, 325, 3476, 119, 177, 3167, 125, 2091, 314, 22283, 22332, 1968, 123, 123, 17178, 125, 20849, 117, 1384, 125, 2633, 22307, 5289, 180, 1855, 7723, 10505, 119, 2502, 22278, 131, 4795, 2528, 10439, 112, 8196, 2227, 265, 22280, 11007, 112, 125, 16926, 598, 3161, 10611, 200, 1989, 8660, 524, 17376, 177, 3222, 226, 12869, 22278, 4310, 17809, 703, 8588, 770, 1021, 6147, 222, 16514, 202, 1957, 644, 1934, 125, 1614, 221, 259, 6625, 10621, 498, 123, 10517, 125, 7520, 21852, 22281, 420, 6518, 122, 6742, 7814, 1044, 202, 2091, 314, 22283, 22332, 117, 229, 16783, 171, 2187, 173, 2596, 353, 1037, 125, 4618, 130, 3054, 19241, 117, 202, 2187, 8890, 1620, 303, 371, 119, 13350, 6742, 7814, 1044, 506, 17865, 954, 6785, 790, 146, 7520, 21852, 119, 1645, 117, 4762, 146, 278, 22055, 15114, 305, 18417, 117, 123, 6790, 346, 4016, 4862, 119, 231, 3721, 117, 222, 1955, 1772, 131, 19687, 455, 240, 1839, 180, 5362, 171, 1955, 325, 20808, 171, 771, 533, 9314, 453, 125, 5296, 1758, 15615, 125, 532, 4761, 122, 346, 8734, 123, 6819, 2166, 3834, 119, 530, 16512, 3233, 179, 6430, 259, 3401, 125, 1700, 117, 5759, 1702, 22279, 119, 2502, 22278, 260, 14179, 325, 11845, 221, 4945, 146, 179, 253, 525, 18823, 342, 291, 11443, 100, 7314, 118, 14979, 22302, 119, 4859, 3015, 22121, 22281, 123, 9087, 3721, 200, 120, 177, 119, 1681, 3028, 346, 706, 333, 3565, 117, 9410, 240, 11934, 142, 18626, 117, 21141, 4396, 291, 184, 16364, 979, 1287, 834, 10910, 119, 102], 'label': '9'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPypCNPuHaRz"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvy3rgrdHj_x"
      },
      "source": [
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        options_name = \"bert-base-portuguese-cased\"\n",
        "        self.encoder = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "    def forward(self, text, label):\n",
        "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
        "\n",
        "        return loss, text_fea\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pYKbHICIus0"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJFwyl7DIoxQ"
      },
      "source": [
        "# Save and Load Functions\n",
        "\n",
        "def save_checkpoint(save_path, model, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(load_path, model):\n",
        "    \n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiueExF5Jb8V"
      },
      "source": [
        "# Training Function\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          criterion = nn.BCELoss(),\n",
        "          train_loader = train_iter,\n",
        "          valid_loader = test_iter,\n",
        "          num_epochs = 5,\n",
        "          eval_every = len(train_iter) // 2,\n",
        "          file_path = destination_folder,\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "    \n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    # training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for (text, labels), _ in train_loader:\n",
        "            labels = labels.type(torch.LongTensor)           \n",
        "            print('label:', labels.size())\n",
        "            labels = labels.to(device)\n",
        "            text = text.type(torch.LongTensor) \n",
        "            print('text:', text.size())\n",
        "            text = text.to(device)\n",
        "            output = model(text, labels)\n",
        "            #print('model:', output)\n",
        "            loss, _ = output\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "\n",
        "                    # validation loop\n",
        "                    for (text,labels), _ in valid_loader:\n",
        "                        text = text.type(torch.LongTensor)  \n",
        "                        text = text.to(device)\n",
        "                        labels = labels.type(torch.LongTensor)           \n",
        "                        labels = labels.to(device)\n",
        "                        \n",
        "                        output = model(titletext, labels)\n",
        "                        loss, _ = output\n",
        "                        \n",
        "                        valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
        "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    \n",
        "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQcJQ5kaMPa4",
        "outputId": "18aade4a-9d2c-4187-ad75-642827310ff0"
      },
      "source": [
        "device"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "RrkvwE2VL1qY",
        "outputId": "632d8f82-328a-4924-b60a-9e387781760c"
      },
      "source": [
        "model = BERT().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "train(model=model, optimizer=optimizer)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "label: torch.Size([4])\n",
            "text: torch.Size([4, 128])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e4474bff9c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-00f6ca9d3088>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_loader, valid_loader, num_epochs, eval_every, file_path, best_valid_loss)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sGfCMMB9ewJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJrH6W-UEXCK"
      },
      "source": [
        ""
      ]
    }
  ]
}