{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTimbau_Testes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5L/tC/N/xOZMul+/gcgDf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HedersonSantos/Noticias/blob/main/BERTimbau_Testes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKMj5WG6lxGP",
        "outputId": "bddcd50d-1487-4d5b-b6d7-ee977d08d8ca"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIlZbdsGo_vR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69d689a-0942-4977-fef0-7a136df76824"
      },
      "source": [
        "\n",
        "!pip install torch==1.8.1 torchvision==0.9.1 torchaudio==0.8.\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.9.1 in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: torchaudio==0.8. in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hts6ewoYvUYn"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Preliminaries\n",
        "\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "# Models\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AutoTokenizer  # Or BertTokenizer\n",
        "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
        "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
        "\n",
        "# Training\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPOdGbGitPHS"
      },
      "source": [
        "### Carregando tokens e vocabul√°rio do BERTimbau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLcgDJZSpmfD",
        "outputId": "4c779277-75b1-4dcd-a61b-ae11e6e8ab78"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joJqFofvtKcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144fee4d-fdef-4047-ff9b-246700e38e8e"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizerFast(name_or_path='neuralmind/bert-base-portuguese-cased', vocab_size=29794, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZOL2o_ltyzz"
      },
      "source": [
        "### Preparando DataSet\n",
        "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFtrA_dtunW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4492bfc-fbce-4918-8cea-bf60d642a2ff"
      },
      "source": [
        "!rm train*.*\n",
        "!rm test*.*\n",
        "!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/train.csv\n",
        "!wget https://raw.githubusercontent.com/HedersonSantos/Noticias/main/test.csv"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-04 17:22:24--  https://raw.githubusercontent.com/HedersonSantos/Noticias/main/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5655081 (5.4M) [text/plain]\n",
            "Saving to: ‚Äòtrain.csv‚Äô\n",
            "\n",
            "train.csv           100%[===================>]   5.39M  12.8MB/s    in 0.4s    \n",
            "\n",
            "2021-06-04 17:22:25 (12.8 MB/s) - ‚Äòtrain.csv‚Äô saved [5655081/5655081]\n",
            "\n",
            "--2021-06-04 17:22:25--  https://raw.githubusercontent.com/HedersonSantos/Noticias/main/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 705577 (689K) [text/plain]\n",
            "Saving to: ‚Äòtest.csv‚Äô\n",
            "\n",
            "test.csv            100%[===================>] 689.04K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-06-04 17:22:26 (6.00 MB/s) - ‚Äòtest.csv‚Äô saved [705577/705577]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV8T4nMYXqdR",
        "outputId": "a0b47db0-8326-40fa-9979-64fdbb52f7e8"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sjrTvfBLGCL"
      },
      "source": [
        "source_folder = '/content/drive/My Drive/nlpPreProcessamento'\n",
        "destination_folder = '/content/drive/My Drive/nlpPreProcessamento'\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyoK7ac8t4HF"
      },
      "source": [
        "\n",
        "#Model parameter\n",
        "MAX_SEQ_LEN = 128\n",
        "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
        "\n",
        "# Fields\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
        "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
        "fields = [('text', text_field),('label', label_field)]\n",
        "\n",
        "# TabularDataset\n",
        "train,  test = TabularDataset.splits(path=source_folder, train='train.csv', \n",
        "                                           test='test.csv', format='CSV', fields=fields, skip_header=True)\n",
        "\n",
        "# Iterators\n",
        "\n",
        "train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text),\n",
        "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
        "test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcTHSXSluKkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797abf33-10b5-41b4-ce2c-72b5181243a8"
      },
      "source": [
        "print(vars(train[0]))\n",
        "print(vars(test[0]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': [101, 566, 278, 22302, 566, 278, 22302, 1040, 120, 16720, 120, 14979, 22302, 1433, 22296, 9876, 13103, 7370, 1040, 120, 16720, 120, 14979, 22302, 7526, 10145, 1349, 117, 12222, 240, 325, 125, 5011, 481, 170, 10880, 117, 253, 123, 2338, 123, 3866, 202, 3952, 1286, 726, 146, 15036, 125, 1040, 125, 1614, 125, 14979, 22302, 2010, 783, 183, 131, 5378, 1133, 162, 120, 1094, 211, 120, 482, 1973, 424, 231, 15036, 171, 4523, 10880, 117, 179, 2931, 712, 16113, 481, 117, 4359, 3102, 14157, 113, 1040, 114, 173, 230, 7098, 11162, 240, 1284, 298, 14491, 22281, 180, 385, 1222, 118, 275, 119, 1305, 2055, 328, 262, 13460, 412, 4685, 10145, 1349, 122, 10588, 22281, 117, 3949, 146, 4523, 8373, 119, 231, 1831, 171, 7051, 125, 20887, 262, 6751, 221, 123, 786, 13759, 1752, 119, 4225, 117, 146, 1831, 171, 4523, 10880, 262, 20262, 243, 229, 9441, 125, 710, 3651, 117, 1839, 180, 3617, 1752, 171, 5463, 125, 14037, 15823, 117, 582, 368, 19575, 170, 123, 4685, 10145, 1349, 119, 6864, 22281, 3416, 122, 8373, 6082, 22287, 15036, 171, 9237, 117, 4523, 10880, 2010, 783, 183, 131, 15481, 12311, 22326, 120, 482, 1973, 424, 1240, 146, 1338, 180, 7098, 117, 123, 1288, 1752, 4825, 320, 5463, 125, 14037, 15823, 119, 4151, 2577, 315, 385, 1222, 566, 1284, 366, 11668, 6529, 470, 726, 123, 9196, 769, 151, 180, 385, 1222, 118, 275, 202, 2163, 2899, 117, 1307, 222, 6487, 125, 1101, 179, 1146, 4370, 125, 2189, 12422, 122, 20675, 441, 119, 3416, 122, 6333, 790, 146, 15036, 171, 9237, 117, 4523, 10880, 2010, 783, 183, 131, 482, 15538, 503, 7098, 2166, 14157, 117, 820, 1934, 1101, 117, 420, 2292, 117, 10588, 22281, 122, 736, 10034, 6543, 15037, 10850, 943, 353, 9441, 125, 710, 3651, 119, 7526, 10145, 1349, 176, 1636, 22278, 15480, 726, 146, 15036, 171, 4170, 117, 4523, 10880, 117, 229, 9441, 125, 710, 3651, 173, 1040, 125, 1614, 125, 14979, 22302, 2010, 783, 183, 131, 13690, 7464, 120, 1094, 211, 120, 482, 1973, 424, 1629, 180, 4685, 10145, 1349, 117, 179, 6366, 320, 1341, 125, 10880, 240, 325, 125, 5011, 481, 117, 1256, 298, 19031, 18671, 320, 4899, 8164, 180, 20433, 131, 6864, 5033, 113, 100, 114, 6864, 3416, 113, 100, 114, 6864, 8373, 113, 100, 114, 6864, 11762, 113, 100, 114, 231, 4523, 4404, 1379, 100, 229, 1999, 125, 8524, 1379, 117, 1417, 125, 3416, 117, 122, 532, 3788, 6062, 15654, 113, 100, 114, 122, 4523, 7016, 113, 100, 114, 346, 8164, 180, 7098, 240, 744, 2736, 2943, 113, 873, 524, 123, 20400, 18110, 5443, 4133, 114, 119, 13818, 4056, 20400, 5403, 180, 1288, 1752, 5777, 2010, 783, 183, 131, 5887, 278, 22302, 231, 4523, 8373, 117, 179, 9824, 538, 3585, 117, 2927, 320, 2163, 2899, 122, 1023, 179, 10241, 222, 1254, 125, 1717, 403, 324, 119, 3311, 253, 123, 681, 576, 179, 368, 1359, 320, 806, 700, 180, 873, 2951, 341, 180, 12227, 4331, 179, 368, 11530, 320, 1341, 180, 2772, 19963, 3148, 6589, 191, 353, 12089, 4865, 13234, 124, 22296, 119, 6864, 10880, 10415, 170, 146, 10588, 4523, 8373, 173, 3652, 834, 2788, 2010, 783, 183, 131, 8177, 3217, 120, 20077, 10304, 22290, 6589, 191, 117, 123, 19133, 125, 7772, 5679, 117, 346, 3851, 2981, 119, 1660, 1767, 538, 3585, 240, 16302, 315, 21930, 119, 177, 2772, 125, 8373, 698, 13772, 171, 100, 1417, 171, 4672, 119, 2195, 259, 14491, 22281, 2903, 6821, 117, 259, 8384, 3312, 179, 3247, 18175, 310, 726, 123, 7098, 122, 3407, 12876, 22281, 125, 5506, 119, 231, 4976, 125, 14435, 12582, 5695, 712, 6861, 179, 346, 2137, 22281, 6867, 176, 15625, 171, 5463, 125, 14037, 15823, 117, 2898, 123, 3250, 117, 221, 4365, 11621, 2406, 119, 8177, 1752, 6082, 15036, 171, 6864, 10880, 2010, 783, 183, 131, 482, 15538, 21209, 1220, 170, 2292, 122, 10588, 22281, 231, 11314, 22312, 150, 125, 10880, 262, 4636, 487, 171, 5463, 221, 123, 9441, 125, 710, 3651, 1379, 222, 19046, 125, 1384, 125, 10081, 22287, 1379, 173, 222, 3883, 13104, 673, 391, 16978, 119, 7526, 10145, 1349, 4147, 123, 9441, 125, 710, 3651, 221, 146, 15036, 171, 15211, 22280, 4523, 10880, 173, 1040, 125, 1614, 125, 14979, 22302, 2010, 783, 183, 131, 15481, 12311, 22326, 120, 1094, 211, 120, 482, 1973, 424, 231, 3883, 170, 146, 11314, 22312, 150, 125, 10880, 262, 8582, 123, 5309, 240, 1823, 180, 1288, 1752, 117, 17707, 320, 1341, 122, 3704, 171, 7594, 119, 177, 3007, 1538, 180, 18265, 5626, 171, 1341, 9657, 171, 11314, 22312, 150, 131, 11474, 13096, 117, 2267, 125, 10880, 11581, 3117, 7540, 117, 1417, 125, 10880, 192, 171, 1341, 2368, 131, 6864, 5033, 117, 1417, 125, 10880, 11581, 3117, 11762, 117, 1417, 125, 10880, 2315, 9690, 171, 11314, 22312, 150, 5157, 146, 4523, 8373, 353, 4573, 117, 4269, 16495, 9017, 1379, 1417, 180, 6062, 13096, 1379, 320, 1997, 117, 122, 4523, 3416, 353, 5065, 119, 2511, 3704, 117, 146, 4170, 125, 13096, 117, 2700, 118, 16461, 9052, 6852, 3356, 7905, 17561, 1710, 122, 3589, 19296, 118, 7464, 117, 100, 4917, 125, 18756, 7299, 117, 16325, 228, 146, 3952, 1286, 119, 177, 4685, 10145, 1349, 262, 20255, 221, 123, 9441, 125, 710, 3651, 119, 8177, 1752, 202, 15036, 171, 6864, 10880, 2010, 783, 183, 131, 482, 15538, 787, 262, 21617, 240, 1015, 3840, 180, 5825, 4012, 180, 4685, 117, 179, 346, 2365, 10910, 221, 12079, 123, 7098, 125, 1839, 180, 9441, 119, 1629, 2866, 117, 146, 7871, 1825, 125, 14037, 15823, 122, 146, 9507, 125, 11542, 140, 12724, 117, 3161, 180, 1912, 2299, 21599, 117, 4112, 230, 10428, 341, 1199, 119, 9839, 348, 117, 3769, 1193, 1101, 346, 4112, 670, 180, 13433, 1538, 125, 6135, 119, 1494, 123, 18265, 117, 3218, 12532, 4267, 9479, 173, 3902, 320, 7051, 125, 20887, 1379, 179, 1191, 670, 180, 6016, 5777, 119, 16591, 22281, 180, 5825, 1752, 17754, 22287, 146, 11314, 22312, 150, 170, 146, 1831, 171, 4523, 10880, 1839, 180, 9441, 125, 710, 3651, 173, 1040, 125, 1614, 125, 14979, 22302, 2010, 783, 183, 131, 7785, 22283, 895, 22331, 120, 1094, 211, 120, 482, 1973, 424, 335, 1364, 146, 9926, 117, 3840, 180, 6016, 3217, 117, 2054, 11112, 1044, 503, 5578, 117, 12285, 3217, 180, 7530, 117, 122, 180, 6319, 7977, 3217, 176, 12199, 221, 146, 3952, 1286, 119, 10460, 180, 5246, 117, 146, 11314, 22312, 150, 262, 14021, 170, 146, 418, 11647, 185, 171, 7051, 117, 230, 7386, 117, 146, 18429, 9324, 122, 327, 9531, 119, 628, 6546, 171, 6864, 10880, 253, 14021, 170, 347, 418, 11647, 185, 4012, 9618, 1708, 6645, 533, 8384, 15037, 3407, 6970, 6645, 122, 346, 20170, 3218, 1379, 449, 146, 1700, 125, 12876, 22281, 173, 2548, 19977, 262, 17620, 119, 6864, 22281, 3416, 122, 8373, 6082, 22287, 3952, 1286, 173, 15036, 171, 9237, 4523, 10880, 173, 1040, 125, 1614, 125, 14979, 22302, 2010, 783, 183, 131, 19071, 154, 307, 14742, 120, 1094, 211, 120, 482, 1973, 424, 177, 3087, 125, 179, 19004, 3218, 117, 5406, 221, 2984, 3840, 117, 346, 5327, 4478, 698, 660, 12110, 285, 271, 222, 10049, 300, 180, 4685, 221, 146, 10588, 117, 4523, 8373, 119, 11035, 259, 19004, 5327, 17620, 22281, 117, 8373, 122, 11762, 113, 1417, 180, 4685, 114, 4717, 259, 10711, 1823, 180, 1288, 1752, 179, 16330, 21616, 170, 370, 701, 119, 3393, 2113, 8373, 3763, 675, 11069, 2406, 320, 4314, 123, 1288, 1752, 119, 231, 14491, 2260, 8674, 179, 260, 4677, 143, 3840, 7944, 333, 12232, 591, 820, 170, 146, 1700, 366, 10909, 119, 2781, 11762, 262, 14426, 180, 1069, 3352, 790, 146, 15112, 12302, 240, 327, 8286, 170, 146, 6325, 3725, 8850, 6609, 6012, 7485, 244, 22285, 119, 540, 1365, 170, 123, 4471, 117, 123, 6016, 3217, 346, 2471, 908, 123, 2748, 125, 5003, 118, 1340, 18622, 146, 9470, 119, 21799, 325, 8244, 498, 123, 7098, 125, 20433, 171, 2439, 831, 22279, 10880, 231, 179, 4359, 1790, 117, 4188, 202, 347, 122, 118, 223, 215, 9172, 6943, 243, 106, 11976, 4364, 125, 176, 601, 18930, 229, 872, 3144, 4362, 367, 4151, 4387, 171, 644, 119, 21799, 407, 102], 'label': 'mundo'}\n",
            "{'text': [101, 12064, 3059, 373, 117, 12388, 4728, 181, 6071, 120, 11818, 6225, 12064, 3059, 373, 117, 12388, 4728, 181, 6071, 120, 11818, 6225, 7368, 22285, 2020, 12131, 7104, 2349, 766, 10225, 17110, 3672, 118, 7347, 171, 213, 5475, 2810, 9968, 22282, 180, 8250, 22323, 177, 4680, 7215, 2123, 125, 1308, 152, 6404, 113, 8250, 22323, 114, 180, 385, 1222, 2010, 179, 12802, 22303, 3680, 291, 17331, 3498, 125, 5258, 229, 9196, 769, 151, 2010, 1023, 1206, 3660, 370, 356, 118, 14258, 113, 2680, 120, 16720, 114, 170, 230, 4076, 221, 146, 1640, 21523, 20354, 716, 157, 119, 177, 1634, 298, 5938, 9916, 682, 14990, 7731, 221, 16537, 259, 2851, 122, 4787, 347, 7399, 774, 119, 20016, 5606, 281, 113, 16845, 118, 9313, 114, 117, 7347, 171, 8847, 117, 1477, 3596, 1181, 125, 3231, 10860, 22288, 202, 1206, 171, 622, 1372, 353, 11547, 180, 9196, 769, 151, 117, 262, 2074, 1640, 119, 959, 123, 2700, 118, 6810, 117, 262, 2074, 19779, 6040, 22279, 6071, 113, 3490, 118, 12127, 114, 335, 1365, 170, 736, 5938, 117, 5606, 281, 17861, 146, 7347, 7368, 22285, 2020, 12131, 113, 213, 5475, 118, 14215, 114, 221, 333, 146, 9968, 22282, 119, 8866, 1761, 22287, 6910, 170, 260, 2202, 21027, 22281, 202, 6225, 119, 177, 4451, 125, 5606, 281, 770, 495, 2521, 285, 117, 1502, 14990, 21698, 122, 7731, 6921, 22287, 1118, 298, 7981, 5621, 143, 180, 8250, 22323, 119, 177, 10013, 1767, 170, 146, 2347, 171, 7347, 21472, 14846, 113, 15166, 118, 212, 22323, 114, 117, 222, 298, 1256, 14990, 6689, 125, 20354, 716, 157, 179, 1761, 22287, 123, 6979, 119, 761, 1257, 117, 5606, 281, 1678, 3003, 3114, 122, 146, 4931, 4102, 743, 1620, 8849, 117, 7347, 4771, 6918, 150, 113, 19167, 118, 7931, 114, 2628, 864, 119, 231, 1640, 2074, 17400, 222, 107, 1223, 4022, 107, 179, 10467, 22303, 107, 123, 3295, 107, 119, 19687, 22287, 171, 18812, 407, 437, 3316, 107, 3311, 8250, 22323, 346, 706, 6202, 221, 176, 16695, 125, 21905, 7749, 119, 3311, 8250, 22323, 376, 179, 1434, 6722, 221, 6171, 125, 2818, 16160, 22281, 179, 1921, 9196, 769, 151, 698, 4513, 107, 117, 1996, 5606, 281, 119, 107, 19012, 225, 312, 125, 5220, 5863, 4914, 22303, 1434, 12935, 130, 117, 449, 7105, 2822, 222, 1410, 320, 3896, 122, 370, 222, 14491, 1772, 119, 17271, 2042, 22282, 4486, 179, 9502, 125, 333, 4330, 122, 240, 1977, 2789, 125, 1434, 117, 1547, 368, 5441, 117, 19975, 117, 3261, 291, 4125, 2811, 806, 107, 117, 14000, 119, 7215, 6560, 4102, 743, 1620, 8849, 22281, 10599, 6688, 22282, 123, 4990, 125, 2020, 12131, 117, 425, 10438, 125, 179, 368, 1467, 21358, 221, 9166, 146, 2466, 117, 770, 179, 222, 298, 7210, 180, 8250, 22323, 253, 12802, 146, 21417, 22279, 125, 3353, 10621, 712, 1134, 117, 122, 146, 7347, 253, 1568, 171, 3261, 125, 14969, 117, 7368, 22285, 4725, 119, 1431, 1743, 2623, 2080, 123, 333, 13712, 412, 681, 17231, 180, 4795, 2528, 125, 6191, 4535, 557, 123, 4990, 125, 2020, 12131, 117, 123, 4794, 180, 3153, 1378, 7108, 22278, 5977, 282, 7144, 113, 5374, 22327, 118, 9791, 114, 117, 449, 123, 3087, 262, 9610, 251, 423, 4846, 8071, 2528, 180, 100, 5574, 119, 5183, 9261, 9512, 771, 117, 146, 7347, 9396, 123, 3867, 4353, 229, 2767, 19023, 6897, 179, 222, 695, 118, 9968, 22282, 1981, 333, 7516, 271, 2806, 1652, 9966, 15076, 182, 498, 21417, 143, 10621, 123, 14969, 119, 231, 2898, 6793, 180, 8250, 22323, 2810, 123, 7344, 125, 222, 3204, 125, 1223, 117, 170, 20719, 221, 123, 16035, 180, 5795, 119, 533, 14990, 3921, 4849, 548, 1000, 1242, 22296, 125, 5044, 118, 14258, 113, 2593, 120, 16720, 114, 675, 4816, 4541, 123, 2020, 12131, 117, 179, 20088, 22303, 146, 3204, 119, 503, 6151, 118, 14258, 125, 6293, 113, 2939, 120, 16720, 114, 117, 1966, 10456, 2810, 15815, 117, 1016, 271, 259, 1867, 7854, 3820, 221, 19321, 125, 16414, 122, 14050, 125, 3476, 113, 240, 1416, 117, 146, 20502, 310, 125, 11864, 173, 15303, 173, 736, 6625, 117, 271, 146, 3890, 7638, 114, 119, 177, 13874, 253, 179, 259, 1867, 10897, 22281, 123, 13852, 4694, 259, 1256, 12058, 180, 6007, 125, 20354, 716, 157, 131, 4880, 3799, 15795, 17624, 117, 8474, 1208, 1361, 117, 8064, 2568, 326, 117, 122, 146, 2233, 117, 7593, 17858, 421, 119, 1986, 1981, 333, 1565, 2044, 202, 1206, 146, 1640, 180, 11818, 1580, 125, 11648, 13887, 2936, 5040, 113, 1575, 18418, 114, 117, 5523, 7708, 10314, 119, 12064, 3059, 373, 117, 257, 22309, 21557, 5054, 22308, 120, 359, 143, 7099, 11206, 517, 1643, 495, 5441, 117, 8064, 2568, 326, 1996, 179, 16246, 260, 6107, 125, 20354, 716, 157, 177, 6979, 376, 13367, 2206, 125, 3072, 22282, 5346, 1564, 117, 6620, 179, 706, 333, 15095, 1655, 119, 1629, 171, 5323, 125, 9928, 16535, 555, 221, 146, 1161, 320, 1639, 171, 347, 6927, 117, 123, 8250, 22323, 2810, 11271, 170, 123, 1468, 125, 222, 7399, 119, 3758, 6875, 706, 5178, 307, 123, 7344, 125, 2534, 4971, 423, 4855, 117, 123, 11935, 22281, 375, 320, 3890, 7638, 125, 675, 20087, 221, 2199, 5296, 980, 910, 3154, 122, 18939, 298, 2254, 3895, 117, 1016, 271, 6202, 125, 2807, 310, 221, 1160, 4794, 125, 6554, 19221, 598, 146, 1640, 119, 177, 3903, 125, 222, 1673, 221, 16083, 20354, 716, 157, 117, 1804, 117, 8793, 125, 3087, 6553, 171, 1640, 180, 3159, 117, 8487, 10159, 124, 113, 15166, 118, 14215, 114, 117, 179, 376, 176, 9573, 10663, 171, 17614, 119, 4725, 171, 1640, 117, 146, 7347, 13404, 20354, 716, 157, 113, 14389, 22281, 118, 19647, 114, 1996, 179, 107, 123, 8250, 22323, 2810, 222, 739, 5340, 1848, 221, 146, 1161, 4314, 2745, 1000, 18592, 107, 119, 787, 12459, 117, 1804, 117, 146, 2182, 5526, 221, 123, 6979, 119, 107, 807, 6474, 13348, 986, 1921, 8250, 22323, 2541, 13468, 202, 8363, 180, 1521, 136, 19403, 22278, 119, 231, 2182, 253, 6767, 4496, 16824, 107, 117, 3415, 117, 538, 13143, 179, 18431, 3730, 123, 4451, 171, 3118, 180, 8250, 22323, 119, 107, 11521, 265, 22280, 179, 1089, 14990, 179, 1429, 3407, 259, 11314, 22312, 270, 125, 1821, 7287, 592, 9181, 221, 1434, 1837, 11037, 154, 122, 5874, 7700, 598, 146, 1161, 4015, 107, 117, 1996, 744, 119, 8250, 22323, 2991, 170, 112, 388, 13168, 112, 19199, 598, 20354, 716, 157, 761, 146, 771, 2898, 125, 19263, 7287, 592, 9181, 412, 144, 1222, 118, 275, 117, 123, 8250, 22323, 12802, 22303, 123, 5115, 171, 1161, 125, 21523, 20354, 716, 157, 202, 20558, 265, 22280, 180, 4932, 171, 5553, 324, 22292, 4667, 249, 119, 5394, 7407, 180, 6979, 2810, 15076, 22282, 6809, 11443, 589, 202, 1700, 125, 3353, 21417, 308, 412, 2674, 221, 1134, 122, 4560, 6795, 210, 598, 123, 9196, 769, 151, 119, 177, 13874, 117, 1804, 117, 253, 179, 259, 2851, 9010, 15134, 22287, 3424, 123, 5115, 171, 1161, 4015, 117, 122, 770, 1307, 1450, 107, 11522, 832, 107, 6176, 221, 2736, 6266, 954, 14990, 598, 123, 4940, 20354, 716, 157, 119, 177, 8250, 22323, 8209, 6358, 931, 117, 240, 1416, 117, 320, 4846, 125, 20776, 125, 2674, 113, 267, 22304, 22341, 114, 122, 320, 3890, 7638, 2528, 113, 11164, 22324, 114, 146, 20502, 310, 125, 11864, 179, 770, 15076, 22287, 2199, 2128, 16327, 171, 1161, 202, 12228, 125, 12921, 122, 10671, 793, 221, 123, 2551, 3352, 117, 1016, 271, 123, 3174, 124, 173, 10194, 307, 353, 3207, 125, 12028, 13827, 173, 1543, 202, 8847, 119, 12064, 3059, 373, 117, 257, 22309, 21557, 5054, 22308, 120, 8298, 11964, 22317, 461, 9008, 22327, 22350, 19823, 183, 173, 12221, 598, 5115, 125, 20354, 716, 157, 229, 9196, 769, 151, 118, 4932, 202, 8847, 2810, 2254, 3281, 229, 8250, 22323, 1629, 125, 6358, 931, 5590, 113, 3949, 9137, 1808, 1409, 114, 123, 736, 6625, 117, 123, 6979, 407, 706, 7854, 140, 6642, 22281, 125, 9137, 1808, 22281, 13816, 117, 9284, 488, 122, 125, 2391, 117, 1016, 271, 4813, 934, 1101, 221, 13852, 119, 2781, 3033, 229, 2925, 125, 1421, 1965, 10897, 22281, 146, 294, 118, 6533, 125, 11266, 171, 1161, 20354, 716, 157, 117, 12261, 5493, 22314, 12674, 22282, 1476, 117, 179, 229, 2338, 2767, 1191, 3072, 22281, 4353, 320, 294, 118, 5441, 180, 6007, 4771, 8064, 2568, 326, 117, 173, 4331, 353, 2428, 21799, 119, 787, 1035, 9390, 146, 3376, 412, 3087, 171, 1161, 125, 346, 11640, 5011, 1529, 125, 13348, 986, 21061, 22281, 412, 212, 11429, 2964, 202, 622, 3714, 2010, 123, 5296, 1758, 171, 1161, 229, 3174, 124, 221, 13577, 2854, 123, 1521, 253, 222, 298, 1649, 160, 15855, 179, 123, 8250, 22323, 11179, 11199, 943, 119, 4859, 259, 7854, 3820, 12557, 22281, 229, 6979, 14479, 179, 333, 2807, 16209, 122, 3859, 146, 3548, 180, 1634, 298, 5938, 221, 925, 210, 14339, 119, 231, 1161, 117, 1804, 117, 1284, 170, 17919, 229, 8250, 22323, 117, 146, 179, 2976, 8234, 179, 14050, 19199, 22281, 221, 146, 1640, 4694, 9752, 22281, 119, 10643, 7981, 5938, 5621, 143, 180, 6979, 117, 1256, 453, 6689, 171, 4976, 171, 17614, 131, 21472, 14846, 113, 15166, 118, 212, 22323, 114, 117, 7221, 15539, 113, 10836, 22311, 118, 257, 22317, 114, 117, 724, 22282, 4602, 268, 11867, 113, 212, 22327, 118, 14614, 114, 122, 4771, 6918, 150, 113, 19167, 118, 7931, 114, 119, 13350, 14990, 453, 5641, 7731, 117, 449, 1854, 230, 12593, 3619, 498, 123, 16035, 171, 20558, 265, 22280, 180, 9196, 769, 151, 423, 1161, 131, 1166, 125, 7368, 22285, 2020, 12131, 113, 213, 5475, 118, 14215, 114, 122, 20016, 5606, 281, 113, 16845, 118, 9313, 114, 117, 1376, 3876, 939, 4771, 7808, 113, 213, 5475, 118, 9313, 114, 117, 14901, 19787, 113, 16845, 118, 241, 22301, 114, 117, 122, 1475, 22281, 293, 17653, 6501, 193, 113, 16798, 118, 7931, 114, 119, 533, 3232, 682, 453, 17712, 125, 4777, 123, 20354, 716, 157, 131, 17110, 3672, 113, 8690, 118, 212, 22309, 114, 122, 19779, 6040, 22279, 6071, 113, 3490, 118, 12127, 114, 119, 12064, 3059, 373, 117, 18471, 18532, 9008, 22317, 17715, 4790, 22322, 17807, 120, 177, 22328, 22420, 22320, 14298, 22301, 8298, 4089, 13292, 1240, 1434, 10278, 9498, 123, 8064, 2568, 326, 117, 15940, 247, 5493, 22314, 12674, 22282, 1476, 1981, 333, 10897, 412, 8250, 22323, 20298, 4813, 4851, 240, 8250, 22323, 229, 5061, 125, 17485, 453, 20616, 22281, 123, 10850, 943, 122, 4640, 123, 3295, 119, 2195, 146, 9878, 16494, 117, 4852, 307, 253, 2304, 5846, 117, 170, 7482, 12913, 125, 682, 123, 1256, 481, 125, 570, 20323, 117, 1166, 125, 20742, 119, 20252, 117, 1804, 117, 1854, 2368, 123, 346, 12304, 123, 14179, 179, 7944, 7766, 16581, 118, 10497, 4188, 119, 3393, 5229, 146, 4801, 125, 179, 7749, 253, 9773, 123, 4787, 2310, 598, 898, 653, 119, 566, 1966, 653, 5607, 117, 1101, 4813, 4851, 271, 2254, 3895, 1146, 8671, 4412, 1945, 308, 291, 2798, 10850, 943, 119, 231, 7347, 19779, 6040, 22279, 6071, 770, 1996, 179, 10381, 22303, 123, 19321, 125, 5493, 22314, 12674, 22282, 1476, 117, 122, 8971, 407, 179, 1547, 2850, 700, 230, 1334, 952, 182, 420, 368, 122, 8064, 2568, 326, 119, 3393, 21261, 151, 6542, 259, 682, 320, 653, 596, 221, 4489, 2443, 675, 4350, 119, 3074, 407, 123, 4890, 125, 179, 146, 5441, 180, 12313, 117, 1033, 773, 17284, 117, 1547, 1565, 119, 107, 3089, 1823, 113, 180, 8250, 22323, 114, 2535, 1376, 14480, 214, 179, 146, 5441, 1033, 773, 17284, 495, 598, 146, 9930, 6276, 5347, 119, 192, 11338, 179, 1397, 6042, 2166, 622, 346, 376, 3495, 221, 144, 1222, 117, 1203, 221, 13348, 986, 119, 5638, 117, 1226, 222, 2099, 10439, 175, 221, 6542, 146, 5441, 1033, 773, 17284, 291, 146, 3376, 117, 170, 11304, 6224, 5130, 107, 117, 1996, 353, 9261, 9512, 771, 146, 7347, 20016, 5606, 281, 117, 173, 4331, 229, 2767, 19023, 119, 2781, 20471, 712, 10811, 2432, 8244, 202, 11105, 136, 1983, 1128, 256, 118, 176, 202, 7275, 3112, 106, 100, 14979, 22302, 9261, 119, 177, 9261, 346, 176, 5296, 980, 2446, 423, 5015, 125, 12045, 15895, 119, 2502, 22278, 498, 7308, 1837, 173, 1712, 123, 1863, 3472, 15895, 119, 102], 'label': 'brasil'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPypCNPuHaRz"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvy3rgrdHj_x"
      },
      "source": [
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        options_name = \"bert-base-portuguese-cased\"\n",
        "        self.encoder = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "    def forward(self, text, label):\n",
        "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
        "\n",
        "        return loss, text_fea\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pYKbHICIus0"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJFwyl7DIoxQ"
      },
      "source": [
        "# Save and Load Functions\n",
        "\n",
        "def save_checkpoint(save_path, model, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(load_path, model):\n",
        "    \n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiueExF5Jb8V"
      },
      "source": [
        "# Training Function\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          criterion = nn.BCELoss(),\n",
        "          train_loader = train_iter,\n",
        "          valid_loader = test_iter,\n",
        "          num_epochs = 5,\n",
        "          eval_every = len(train_iter) // 2,\n",
        "          file_path = destination_folder,\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "    \n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    # training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for (labels, title, text, titletext), _ in train_loader:\n",
        "            labels = labels.type(torch.LongTensor)           \n",
        "            labels = labels.to(device)\n",
        "            titletext = titletext.type(torch.LongTensor)  \n",
        "            titletext = titletext.to(device)\n",
        "            output = model(titletext, labels)\n",
        "            loss, _ = output\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "\n",
        "                    # validation loop\n",
        "                    for (labels, title, text, titletext), _ in valid_loader:\n",
        "                        labels = labels.type(torch.LongTensor)           \n",
        "                        labels = labels.to(device)\n",
        "                        titletext = titletext.type(torch.LongTensor)  \n",
        "                        titletext = titletext.to(device)\n",
        "                        output = model(titletext, labels)\n",
        "                        loss, _ = output\n",
        "                        \n",
        "                        valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
        "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    \n",
        "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQcJQ5kaMPa4",
        "outputId": "44540670-f1a3-4fb5-de55-7d78d7f205e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrkvwE2VL1qY",
        "outputId": "02fe2a8b-641e-46ce-c649-ce73b37841e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = BERT().to(device)\n",
        "#optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "#train(model=model, optimizer=optimizer)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJrH6W-UEXCK"
      },
      "source": [
        ""
      ]
    }
  ]
}